{
    "version": 3,
    "terraform_version": "0.11.2",
    "serial": 37,
    "lineage": "4afb6907-a789-4997-a6e3-8dd8fa150aad",
    "modules": [
        {
            "path": [
                "root"
            ],
            "outputs": {
                "cassandra_server_public_ip": {
                    "sensitive": false,
                    "type": "string",
                    "value": "54.162.80.245,34.238.189.211,35.169.117.246"
                }
            },
            "resources": {
                "aws_instance.cassandra.0": {
                    "type": "aws_instance",
                    "depends_on": [
                        "aws_security_group.cassandra_sg",
                        "data.aws_ami.cassandra",
                        "data.aws_subnet_ids.subnets"
                    ],
                    "primary": {
                        "id": "i-0b6ff7d59cad9c8d7",
                        "attributes": {
                            "ami": "ami-3dcac447",
                            "associate_public_ip_address": "true",
                            "availability_zone": "us-east-1e",
                            "disable_api_termination": "false",
                            "ebs_block_device.#": "0",
                            "ebs_optimized": "false",
                            "ephemeral_block_device.#": "0",
                            "iam_instance_profile": "",
                            "id": "i-0b6ff7d59cad9c8d7",
                            "instance_state": "running",
                            "instance_type": "t2.medium",
                            "ipv6_addresses.#": "0",
                            "key_name": "Opsschool-1",
                            "monitoring": "false",
                            "network_interface.#": "0",
                            "network_interface_id": "eni-bf87fd04",
                            "placement_group": "",
                            "primary_network_interface_id": "eni-bf87fd04",
                            "private_dns": "ip-172-31-48-41.ec2.internal",
                            "private_ip": "172.31.48.41",
                            "public_dns": "ec2-54-162-80-245.compute-1.amazonaws.com",
                            "public_ip": "54.162.80.245",
                            "root_block_device.#": "1",
                            "root_block_device.0.delete_on_termination": "true",
                            "root_block_device.0.iops": "100",
                            "root_block_device.0.volume_id": "vol-0e29fc380a0941253",
                            "root_block_device.0.volume_size": "8",
                            "root_block_device.0.volume_type": "gp2",
                            "security_groups.#": "1",
                            "security_groups.2232598207": "cassandra_sg",
                            "source_dest_check": "true",
                            "subnet_id": "subnet-33f7930c",
                            "tags.%": "2",
                            "tags.Name": "Cassandra-0",
                            "tags.Owner": "Cassandra",
                            "tenancy": "default",
                            "volume_tags.%": "0",
                            "vpc_security_group_ids.#": "0"
                        },
                        "meta": {
                            "e2bfb730-ecaa-11e6-8f88-34363bc7c4c0": {
                                "create": 600000000000,
                                "delete": 600000000000,
                                "update": 600000000000
                            },
                            "schema_version": "1"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.aws"
                },
                "aws_instance.cassandra.1": {
                    "type": "aws_instance",
                    "depends_on": [
                        "aws_security_group.cassandra_sg",
                        "data.aws_ami.cassandra",
                        "data.aws_subnet_ids.subnets"
                    ],
                    "primary": {
                        "id": "i-0df458ae660bb50d6",
                        "attributes": {
                            "ami": "ami-3dcac447",
                            "associate_public_ip_address": "true",
                            "availability_zone": "us-east-1f",
                            "disable_api_termination": "false",
                            "ebs_block_device.#": "0",
                            "ebs_optimized": "false",
                            "ephemeral_block_device.#": "0",
                            "iam_instance_profile": "",
                            "id": "i-0df458ae660bb50d6",
                            "instance_state": "running",
                            "instance_type": "t2.medium",
                            "ipv6_addresses.#": "0",
                            "key_name": "Opsschool-1",
                            "monitoring": "false",
                            "network_interface.#": "0",
                            "network_interface_id": "eni-f2868350",
                            "placement_group": "",
                            "primary_network_interface_id": "eni-f2868350",
                            "private_dns": "ip-172-31-77-64.ec2.internal",
                            "private_ip": "172.31.77.64",
                            "public_dns": "ec2-34-238-189-211.compute-1.amazonaws.com",
                            "public_ip": "34.238.189.211",
                            "root_block_device.#": "1",
                            "root_block_device.0.delete_on_termination": "true",
                            "root_block_device.0.iops": "100",
                            "root_block_device.0.volume_id": "vol-0863ef6bebde1dd11",
                            "root_block_device.0.volume_size": "8",
                            "root_block_device.0.volume_type": "gp2",
                            "security_groups.#": "1",
                            "security_groups.2232598207": "cassandra_sg",
                            "source_dest_check": "true",
                            "subnet_id": "subnet-ff51bcf0",
                            "tags.%": "2",
                            "tags.Name": "Cassandra-1",
                            "tags.Owner": "Cassandra",
                            "tenancy": "default",
                            "volume_tags.%": "0",
                            "vpc_security_group_ids.#": "0"
                        },
                        "meta": {
                            "e2bfb730-ecaa-11e6-8f88-34363bc7c4c0": {
                                "create": 600000000000,
                                "delete": 600000000000,
                                "update": 600000000000
                            },
                            "schema_version": "1"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.aws"
                },
                "aws_instance.cassandra.2": {
                    "type": "aws_instance",
                    "depends_on": [
                        "aws_security_group.cassandra_sg",
                        "data.aws_ami.cassandra",
                        "data.aws_subnet_ids.subnets"
                    ],
                    "primary": {
                        "id": "i-01fbe781a4512834b",
                        "attributes": {
                            "ami": "ami-3dcac447",
                            "associate_public_ip_address": "true",
                            "availability_zone": "us-east-1c",
                            "disable_api_termination": "false",
                            "ebs_block_device.#": "0",
                            "ebs_optimized": "false",
                            "ephemeral_block_device.#": "0",
                            "iam_instance_profile": "",
                            "id": "i-01fbe781a4512834b",
                            "instance_state": "running",
                            "instance_type": "t2.medium",
                            "ipv6_addresses.#": "0",
                            "key_name": "Opsschool-1",
                            "monitoring": "false",
                            "network_interface.#": "0",
                            "network_interface_id": "eni-15990995",
                            "placement_group": "",
                            "primary_network_interface_id": "eni-15990995",
                            "private_dns": "ip-172-31-6-131.ec2.internal",
                            "private_ip": "172.31.6.131",
                            "public_dns": "ec2-35-169-117-246.compute-1.amazonaws.com",
                            "public_ip": "35.169.117.246",
                            "root_block_device.#": "1",
                            "root_block_device.0.delete_on_termination": "true",
                            "root_block_device.0.iops": "100",
                            "root_block_device.0.volume_id": "vol-0f33386154381c480",
                            "root_block_device.0.volume_size": "8",
                            "root_block_device.0.volume_type": "gp2",
                            "security_groups.#": "1",
                            "security_groups.2232598207": "cassandra_sg",
                            "source_dest_check": "true",
                            "subnet_id": "subnet-d2e1afb6",
                            "tags.%": "2",
                            "tags.Name": "Cassandra-2",
                            "tags.Owner": "Cassandra",
                            "tenancy": "default",
                            "volume_tags.%": "0",
                            "vpc_security_group_ids.#": "0"
                        },
                        "meta": {
                            "e2bfb730-ecaa-11e6-8f88-34363bc7c4c0": {
                                "create": 600000000000,
                                "delete": 600000000000,
                                "update": 600000000000
                            },
                            "schema_version": "1"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.aws"
                },
                "aws_security_group.cassandra_sg": {
                    "type": "aws_security_group",
                    "depends_on": [],
                    "primary": {
                        "id": "sg-8776f5f0",
                        "attributes": {
                            "description": "Security group for Cassandra",
                            "egress.#": "1",
                            "egress.482069346.cidr_blocks.#": "1",
                            "egress.482069346.cidr_blocks.0": "0.0.0.0/0",
                            "egress.482069346.description": "",
                            "egress.482069346.from_port": "0",
                            "egress.482069346.ipv6_cidr_blocks.#": "0",
                            "egress.482069346.prefix_list_ids.#": "0",
                            "egress.482069346.protocol": "-1",
                            "egress.482069346.security_groups.#": "0",
                            "egress.482069346.self": "false",
                            "egress.482069346.to_port": "0",
                            "id": "sg-8776f5f0",
                            "ingress.#": "8",
                            "ingress.189450684.cidr_blocks.#": "1",
                            "ingress.189450684.cidr_blocks.0": "0.0.0.0/0",
                            "ingress.189450684.description": "",
                            "ingress.189450684.from_port": "7199",
                            "ingress.189450684.ipv6_cidr_blocks.#": "0",
                            "ingress.189450684.protocol": "TCP",
                            "ingress.189450684.security_groups.#": "0",
                            "ingress.189450684.self": "false",
                            "ingress.189450684.to_port": "7199",
                            "ingress.2094876339.cidr_blocks.#": "1",
                            "ingress.2094876339.cidr_blocks.0": "0.0.0.0/0",
                            "ingress.2094876339.description": "",
                            "ingress.2094876339.from_port": "7000",
                            "ingress.2094876339.ipv6_cidr_blocks.#": "0",
                            "ingress.2094876339.protocol": "TCP",
                            "ingress.2094876339.security_groups.#": "0",
                            "ingress.2094876339.self": "false",
                            "ingress.2094876339.to_port": "7000",
                            "ingress.2147108726.cidr_blocks.#": "1",
                            "ingress.2147108726.cidr_blocks.0": "0.0.0.0/0",
                            "ingress.2147108726.description": "",
                            "ingress.2147108726.from_port": "9042",
                            "ingress.2147108726.ipv6_cidr_blocks.#": "0",
                            "ingress.2147108726.protocol": "TCP",
                            "ingress.2147108726.security_groups.#": "0",
                            "ingress.2147108726.self": "false",
                            "ingress.2147108726.to_port": "9042",
                            "ingress.2154638756.cidr_blocks.#": "1",
                            "ingress.2154638756.cidr_blocks.0": "0.0.0.0/0",
                            "ingress.2154638756.description": "",
                            "ingress.2154638756.from_port": "9160",
                            "ingress.2154638756.ipv6_cidr_blocks.#": "0",
                            "ingress.2154638756.protocol": "TCP",
                            "ingress.2154638756.security_groups.#": "0",
                            "ingress.2154638756.self": "false",
                            "ingress.2154638756.to_port": "9160",
                            "ingress.2541437006.cidr_blocks.#": "1",
                            "ingress.2541437006.cidr_blocks.0": "0.0.0.0/0",
                            "ingress.2541437006.description": "",
                            "ingress.2541437006.from_port": "22",
                            "ingress.2541437006.ipv6_cidr_blocks.#": "0",
                            "ingress.2541437006.protocol": "TCP",
                            "ingress.2541437006.security_groups.#": "0",
                            "ingress.2541437006.self": "false",
                            "ingress.2541437006.to_port": "22",
                            "ingress.2753027920.cidr_blocks.#": "1",
                            "ingress.2753027920.cidr_blocks.0": "0.0.0.0/0",
                            "ingress.2753027920.description": "",
                            "ingress.2753027920.from_port": "1",
                            "ingress.2753027920.ipv6_cidr_blocks.#": "0",
                            "ingress.2753027920.protocol": "icmp",
                            "ingress.2753027920.security_groups.#": "0",
                            "ingress.2753027920.self": "false",
                            "ingress.2753027920.to_port": "1",
                            "ingress.4111910901.cidr_blocks.#": "1",
                            "ingress.4111910901.cidr_blocks.0": "0.0.0.0/0",
                            "ingress.4111910901.description": "",
                            "ingress.4111910901.from_port": "9041",
                            "ingress.4111910901.ipv6_cidr_blocks.#": "0",
                            "ingress.4111910901.protocol": "TCP",
                            "ingress.4111910901.security_groups.#": "0",
                            "ingress.4111910901.self": "false",
                            "ingress.4111910901.to_port": "9041",
                            "ingress.91830578.cidr_blocks.#": "1",
                            "ingress.91830578.cidr_blocks.0": "0.0.0.0/0",
                            "ingress.91830578.description": "",
                            "ingress.91830578.from_port": "7001",
                            "ingress.91830578.ipv6_cidr_blocks.#": "0",
                            "ingress.91830578.protocol": "TCP",
                            "ingress.91830578.security_groups.#": "0",
                            "ingress.91830578.self": "false",
                            "ingress.91830578.to_port": "7001",
                            "name": "cassandra_sg",
                            "owner_id": "573468697385",
                            "revoke_rules_on_delete": "false",
                            "tags.%": "0",
                            "vpc_id": "vpc-584d6e20"
                        },
                        "meta": {
                            "schema_version": "1"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.aws"
                },
                "data.aws_ami.cassandra": {
                    "type": "aws_ami",
                    "depends_on": [],
                    "primary": {
                        "id": "ami-3dcac447",
                        "attributes": {
                            "architecture": "x86_64",
                            "block_device_mappings.#": "3",
                            "block_device_mappings.1634610537.device_name": "/dev/sdb",
                            "block_device_mappings.1634610537.ebs.%": "0",
                            "block_device_mappings.1634610537.no_device": "",
                            "block_device_mappings.1634610537.virtual_name": "ephemeral0",
                            "block_device_mappings.2547816212.device_name": "/dev/sda1",
                            "block_device_mappings.2547816212.ebs.%": "6",
                            "block_device_mappings.2547816212.ebs.delete_on_termination": "true",
                            "block_device_mappings.2547816212.ebs.encrypted": "false",
                            "block_device_mappings.2547816212.ebs.iops": "0",
                            "block_device_mappings.2547816212.ebs.snapshot_id": "snap-037c5f521daa29cd9",
                            "block_device_mappings.2547816212.ebs.volume_size": "8",
                            "block_device_mappings.2547816212.ebs.volume_type": "gp2",
                            "block_device_mappings.2547816212.no_device": "",
                            "block_device_mappings.2547816212.virtual_name": "",
                            "block_device_mappings.3850042718.device_name": "/dev/sdc",
                            "block_device_mappings.3850042718.ebs.%": "0",
                            "block_device_mappings.3850042718.no_device": "",
                            "block_device_mappings.3850042718.virtual_name": "ephemeral1",
                            "creation_date": "2018-02-06T13:12:57.000Z",
                            "description": "An Ubuntu AMI with cassandra installaed.",
                            "filter.#": "4",
                            "filter.1653228625.name": "name",
                            "filter.1653228625.values.#": "1",
                            "filter.1653228625.values.0": "cassandra-*",
                            "filter.338017418.name": "tag:Name",
                            "filter.338017418.values.#": "1",
                            "filter.338017418.values.0": "Cassandra",
                            "filter.3450777067.name": "tag:Owner",
                            "filter.3450777067.values.#": "1",
                            "filter.3450777067.values.0": "Opsschool",
                            "filter.490168357.name": "virtualization-type",
                            "filter.490168357.values.#": "1",
                            "filter.490168357.values.0": "hvm",
                            "hypervisor": "xen",
                            "id": "ami-3dcac447",
                            "image_id": "ami-3dcac447",
                            "image_location": "573468697385/cassandra-2018-02-06T13-05-41Z",
                            "image_type": "machine",
                            "most_recent": "true",
                            "name": "cassandra-2018-02-06T13-05-41Z",
                            "owner_id": "573468697385",
                            "product_codes.#": "0",
                            "public": "false",
                            "root_device_name": "/dev/sda1",
                            "root_device_type": "ebs",
                            "root_snapshot_id": "snap-037c5f521daa29cd9",
                            "sriov_net_support": "simple",
                            "state": "available",
                            "state_reason.%": "2",
                            "state_reason.code": "UNSET",
                            "state_reason.message": "UNSET",
                            "tags.%": "2",
                            "tags.Name": "Cassandra",
                            "tags.Owner": "Opsschool",
                            "virtualization_type": "hvm"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.aws"
                },
                "data.aws_subnet_ids.subnets": {
                    "type": "aws_subnet_ids",
                    "depends_on": [],
                    "primary": {
                        "id": "vpc-584d6e20",
                        "attributes": {
                            "id": "vpc-584d6e20",
                            "ids.#": "6",
                            "ids.1408978182": "subnet-33f7930c",
                            "ids.1421452738": "subnet-ff51bcf0",
                            "ids.1425631960": "subnet-d2e1afb6",
                            "ids.1697622550": "subnet-5a6cee07",
                            "ids.2226545938": "subnet-93dc55bc",
                            "ids.3328347078": "subnet-d5cc989e",
                            "vpc_id": "vpc-584d6e20"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.aws"
                },
                "data.template_file.cassandra_yaml.0": {
                    "type": "template_file",
                    "depends_on": [
                        "aws_instance.cassandra.*"
                    ],
                    "primary": {
                        "id": "6d9e085a836ce0a998acaeec5de4ede0def634ab86f185f7c01c627c98967737",
                        "attributes": {
                            "id": "6d9e085a836ce0a998acaeec5de4ede0def634ab86f185f7c01c627c98967737",
                            "rendered": "# Cassandra storage config YAML \n\n# NOTE:\n#   See http://wiki.apache.org/cassandra/StorageConfiguration for\n#   full explanations of configuration directives\n# /NOTE\n\n# The name of the cluster. This is mainly used to prevent machines in\n# one logical cluster from joining another.\ncluster_name: 'opsschool'\n\n# This defines the number of tokens randomly assigned to this node on the ring\n# The more tokens, relative to other nodes, the larger the proportion of data\n# that this node will store. You probably want all nodes to have the same number\n# of tokens assuming they have equal hardware capability.\n#\n# If you leave this unspecified, Cassandra will use the default of 1 token for legacy compatibility,\n# and will use the initial_token as described below.\n#\n# Specifying initial_token will override this setting on the node's initial start,\n# on subsequent starts, this setting will apply even if initial token is set.\n#\n# If you already have a cluster with 1 token per node, and wish to migrate to \n# multiple tokens per node, see http://wiki.apache.org/cassandra/Operations\nnum_tokens: 256\n\n# Triggers automatic allocation of num_tokens tokens for this node. The allocation\n# algorithm attempts to choose tokens in a way that optimizes replicated load over\n# the nodes in the datacenter for the replication strategy used by the specified\n# keyspace.\n#\n# The load assigned to each node will be close to proportional to its number of\n# vnodes.\n#\n# Only supported with the Murmur3Partitioner.\n# allocate_tokens_for_keyspace: KEYSPACE\n\n# initial_token allows you to specify tokens manually.  While you can use # it with\n# vnodes (num_tokens \u003e 1, above) -- in which case you should provide a \n# comma-separated list -- it's primarily used when adding nodes # to legacy clusters \n# that do not have vnodes enabled.\n# initial_token:\n\n# See http://wiki.apache.org/cassandra/HintedHandoff\n# May either be \"true\" or \"false\" to enable globally\nhinted_handoff_enabled: true\n# When hinted_handoff_enabled is true, a black list of data centers that will not\n# perform hinted handoff\n#hinted_handoff_disabled_datacenters:\n#    - DC1\n#    - DC2\n# this defines the maximum amount of time a dead host will have hints\n# generated.  After it has been dead this long, new hints for it will not be\n# created until it has been seen alive and gone down again.\nmax_hint_window_in_ms: 10800000 # 3 hours\n\n# Maximum throttle in KBs per second, per delivery thread.  This will be\n# reduced proportionally to the number of nodes in the cluster.  (If there\n# are two nodes in the cluster, each delivery thread will use the maximum\n# rate; if there are three, each will throttle to half of the maximum,\n# since we expect two nodes to be delivering hints simultaneously.)\nhinted_handoff_throttle_in_kb: 1024\n\n# Number of threads with which to deliver hints;\n# Consider increasing this number when you have multi-dc deployments, since\n# cross-dc handoff tends to be slower\nmax_hints_delivery_threads: 2\n\n# Directory where Cassandra should store hints.\n# If not set, the default directory is $CASSANDRA_HOME/data/hints.\n# hints_directory: /var/lib/cassandra/hints\n\n# How often hints should be flushed from the internal buffers to disk.\n# Will *not* trigger fsync.\nhints_flush_period_in_ms: 10000\n\n# Maximum size for a single hints file, in megabytes.\nmax_hints_file_size_in_mb: 128\n\n# Compression to apply to the hint files. If omitted, hints files\n# will be written uncompressed. LZ4, Snappy, and Deflate compressors\n# are supported.\n#hints_compression:\n#   - class_name: LZ4Compressor\n#     parameters:\n#         -\n\n# Maximum throttle in KBs per second, total. This will be\n# reduced proportionally to the number of nodes in the cluster.\nbatchlog_replay_throttle_in_kb: 1024\n\n# Authentication backend, implementing IAuthenticator; used to identify users\n# Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthenticator,\n# PasswordAuthenticator}.\n#\n# - AllowAllAuthenticator performs no checks - set it to disable authentication.\n# - PasswordAuthenticator relies on username/password pairs to authenticate\n#   users. It keeps usernames and hashed passwords in system_auth.credentials table.\n#   Please increase system_auth keyspace replication factor if you use this authenticator.\n#   If using PasswordAuthenticator, CassandraRoleManager must also be used (see below)\nauthenticator: AllowAllAuthenticator\n\n# Authorization backend, implementing IAuthorizer; used to limit access/provide permissions\n# Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthorizer,\n# CassandraAuthorizer}.\n#\n# - AllowAllAuthorizer allows any action to any user - set it to disable authorization.\n# - CassandraAuthorizer stores permissions in system_auth.permissions table. Please\n#   increase system_auth keyspace replication factor if you use this authorizer.\nauthorizer: AllowAllAuthorizer\n\n# Part of the Authentication \u0026 Authorization backend, implementing IRoleManager; used\n# to maintain grants and memberships between roles.\n# Out of the box, Cassandra provides org.apache.cassandra.auth.CassandraRoleManager,\n# which stores role information in the system_auth keyspace. Most functions of the\n# IRoleManager require an authenticated login, so unless the configured IAuthenticator\n# actually implements authentication, most of this functionality will be unavailable.\n#\n# - CassandraRoleManager stores role data in the system_auth keyspace. Please\n#   increase system_auth keyspace replication factor if you use this role manager.\nrole_manager: CassandraRoleManager\n\n# Validity period for roles cache (fetching granted roles can be an expensive\n# operation depending on the role manager, CassandraRoleManager is one example)\n# Granted roles are cached for authenticated sessions in AuthenticatedUser and\n# after the period specified here, become eligible for (async) reload.\n# Defaults to 2000, set to 0 to disable caching entirely.\n# Will be disabled automatically for AllowAllAuthenticator.\nroles_validity_in_ms: 2000\n\n# Refresh interval for roles cache (if enabled).\n# After this interval, cache entries become eligible for refresh. Upon next\n# access, an async reload is scheduled and the old value returned until it\n# completes. If roles_validity_in_ms is non-zero, then this must be\n# also.\n# Defaults to the same value as roles_validity_in_ms.\n# roles_update_interval_in_ms: 2000\n\n# Validity period for permissions cache (fetching permissions can be an\n# expensive operation depending on the authorizer, CassandraAuthorizer is\n# one example). Defaults to 2000, set to 0 to disable.\n# Will be disabled automatically for AllowAllAuthorizer.\npermissions_validity_in_ms: 2000\n\n# Refresh interval for permissions cache (if enabled).\n# After this interval, cache entries become eligible for refresh. Upon next\n# access, an async reload is scheduled and the old value returned until it\n# completes. If permissions_validity_in_ms is non-zero, then this must be\n# also.\n# Defaults to the same value as permissions_validity_in_ms.\n# permissions_update_interval_in_ms: 2000\n\n# Validity period for credentials cache. This cache is tightly coupled to\n# the provided PasswordAuthenticator implementation of IAuthenticator. If\n# another IAuthenticator implementation is configured, this cache will not\n# be automatically used and so the following settings will have no effect.\n# Please note, credentials are cached in their encrypted form, so while\n# activating this cache may reduce the number of queries made to the\n# underlying table, it may not  bring a significant reduction in the\n# latency of individual authentication attempts.\n# Defaults to 2000, set to 0 to disable credentials caching.\ncredentials_validity_in_ms: 2000\n\n# Refresh interval for credentials cache (if enabled).\n# After this interval, cache entries become eligible for refresh. Upon next\n# access, an async reload is scheduled and the old value returned until it\n# completes. If credentials_validity_in_ms is non-zero, then this must be\n# also.\n# Defaults to the same value as credentials_validity_in_ms.\n# credentials_update_interval_in_ms: 2000\n\n# The partitioner is responsible for distributing groups of rows (by\n# partition key) across nodes in the cluster.  You should leave this\n# alone for new clusters.  The partitioner can NOT be changed without\n# reloading all data, so when upgrading you should set this to the\n# same partitioner you were already using.\n#\n# Besides Murmur3Partitioner, partitioners included for backwards\n# compatibility include RandomPartitioner, ByteOrderedPartitioner, and\n# OrderPreservingPartitioner.\n#\npartitioner: org.apache.cassandra.dht.Murmur3Partitioner\n\n# Directories where Cassandra should store data on disk.  Cassandra\n# will spread data evenly across them, subject to the granularity of\n# the configured compaction strategy.\n# If not set, the default directory is $CASSANDRA_HOME/data/data.\ndata_file_directories:\n    - /var/lib/cassandra/data\n\n# commit log.  when running on magnetic HDD, this should be a\n# separate spindle than the data directories.\n# If not set, the default directory is $CASSANDRA_HOME/data/commitlog.\ncommitlog_directory: /var/lib/cassandra/commitlog\n\n# policy for data disk failures:\n# die: shut down gossip and client transports and kill the JVM for any fs errors or\n#      single-sstable errors, so the node can be replaced.\n# stop_paranoid: shut down gossip and client transports even for single-sstable errors,\n#                kill the JVM for errors during startup.\n# stop: shut down gossip and client transports, leaving the node effectively dead, but\n#       can still be inspected via JMX, kill the JVM for errors during startup.\n# best_effort: stop using the failed disk and respond to requests based on\n#              remaining available sstables.  This means you WILL see obsolete\n#              data at CL.ONE!\n# ignore: ignore fatal errors and let requests fail, as in pre-1.2 Cassandra\ndisk_failure_policy: stop\n\n# policy for commit disk failures:\n# die: shut down gossip and Thrift and kill the JVM, so the node can be replaced.\n# stop: shut down gossip and Thrift, leaving the node effectively dead, but\n#       can still be inspected via JMX.\n# stop_commit: shutdown the commit log, letting writes collect but\n#              continuing to service reads, as in pre-2.0.5 Cassandra\n# ignore: ignore fatal errors and let the batches fail\ncommit_failure_policy: stop\n\n# Maximum size of the native protocol prepared statement cache\n#\n# Valid values are either \"auto\" (omitting the value) or a value greater 0.\n#\n# Note that specifying a too large value will result in long running GCs and possbily\n# out-of-memory errors. Keep the value at a small fraction of the heap.\n#\n# If you constantly see \"prepared statements discarded in the last minute because\n# cache limit reached\" messages, the first step is to investigate the root cause\n# of these messages and check whether prepared statements are used correctly -\n# i.e. use bind markers for variable parts.\n#\n# Do only change the default value, if you really have more prepared statements than\n# fit in the cache. In most cases it is not neccessary to change this value.\n# Constantly re-preparing statements is a performance penalty.\n#\n# Default value (\"auto\") is 1/256th of the heap or 10MB, whichever is greater\nprepared_statements_cache_size_mb:\n\n# Maximum size of the Thrift prepared statement cache\n#\n# If you do not use Thrift at all, it is safe to leave this value at \"auto\".\n#\n# See description of 'prepared_statements_cache_size_mb' above for more information.\n#\n# Default value (\"auto\") is 1/256th of the heap or 10MB, whichever is greater\nthrift_prepared_statements_cache_size_mb:\n\n# Maximum size of the key cache in memory.\n#\n# Each key cache hit saves 1 seek and each row cache hit saves 2 seeks at the\n# minimum, sometimes more. The key cache is fairly tiny for the amount of\n# time it saves, so it's worthwhile to use it at large numbers.\n# The row cache saves even more time, but must contain the entire row,\n# so it is extremely space-intensive. It's best to only use the\n# row cache if you have hot rows or static rows.\n#\n# NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.\n#\n# Default value is empty to make it \"auto\" (min(5% of Heap (in MB), 100MB)). Set to 0 to disable key cache.\nkey_cache_size_in_mb:\n\n# Duration in seconds after which Cassandra should\n# save the key cache. Caches are saved to saved_caches_directory as\n# specified in this configuration file.\n#\n# Saved caches greatly improve cold-start speeds, and is relatively cheap in\n# terms of I/O for the key cache. Row cache saving is much more expensive and\n# has limited use.\n#\n# Default is 14400 or 4 hours.\nkey_cache_save_period: 14400\n\n# Number of keys from the key cache to save\n# Disabled by default, meaning all keys are going to be saved\n# key_cache_keys_to_save: 100\n\n# Row cache implementation class name.\n# Available implementations:\n#   org.apache.cassandra.cache.OHCProvider                Fully off-heap row cache implementation (default).\n#   org.apache.cassandra.cache.SerializingCacheProvider   This is the row cache implementation availabile\n#                                                         in previous releases of Cassandra.\n# row_cache_class_name: org.apache.cassandra.cache.OHCProvider\n\n# Maximum size of the row cache in memory.\n# Please note that OHC cache implementation requires some additional off-heap memory to manage\n# the map structures and some in-flight memory during operations before/after cache entries can be\n# accounted against the cache capacity. This overhead is usually small compared to the whole capacity.\n# Do not specify more memory that the system can afford in the worst usual situation and leave some\n# headroom for OS block level cache. Do never allow your system to swap.\n#\n# Default value is 0, to disable row caching.\nrow_cache_size_in_mb: 0\n\n# Duration in seconds after which Cassandra should save the row cache.\n# Caches are saved to saved_caches_directory as specified in this configuration file.\n#\n# Saved caches greatly improve cold-start speeds, and is relatively cheap in\n# terms of I/O for the key cache. Row cache saving is much more expensive and\n# has limited use.\n#\n# Default is 0 to disable saving the row cache.\nrow_cache_save_period: 0\n\n# Number of keys from the row cache to save.\n# Specify 0 (which is the default), meaning all keys are going to be saved\n# row_cache_keys_to_save: 100\n\n# Maximum size of the counter cache in memory.\n#\n# Counter cache helps to reduce counter locks' contention for hot counter cells.\n# In case of RF = 1 a counter cache hit will cause Cassandra to skip the read before\n# write entirely. With RF \u003e 1 a counter cache hit will still help to reduce the duration\n# of the lock hold, helping with hot counter cell updates, but will not allow skipping\n# the read entirely. Only the local (clock, count) tuple of a counter cell is kept\n# in memory, not the whole counter, so it's relatively cheap.\n#\n# NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.\n#\n# Default value is empty to make it \"auto\" (min(2.5% of Heap (in MB), 50MB)). Set to 0 to disable counter cache.\n# NOTE: if you perform counter deletes and rely on low gcgs, you should disable the counter cache.\ncounter_cache_size_in_mb:\n\n# Duration in seconds after which Cassandra should\n# save the counter cache (keys only). Caches are saved to saved_caches_directory as\n# specified in this configuration file.\n#\n# Default is 7200 or 2 hours.\ncounter_cache_save_period: 7200\n\n# Number of keys from the counter cache to save\n# Disabled by default, meaning all keys are going to be saved\n# counter_cache_keys_to_save: 100\n\n# saved caches\n# If not set, the default directory is $CASSANDRA_HOME/data/saved_caches.\nsaved_caches_directory: /var/lib/cassandra/saved_caches\n\n# commitlog_sync may be either \"periodic\" or \"batch.\" \n# \n# When in batch mode, Cassandra won't ack writes until the commit log\n# has been fsynced to disk.  It will wait\n# commitlog_sync_batch_window_in_ms milliseconds between fsyncs.\n# This window should be kept short because the writer threads will\n# be unable to do extra work while waiting.  (You may need to increase\n# concurrent_writes for the same reason.)\n#\n# commitlog_sync: batch\n# commitlog_sync_batch_window_in_ms: 2\n#\n# the other option is \"periodic\" where writes may be acked immediately\n# and the CommitLog is simply synced every commitlog_sync_period_in_ms\n# milliseconds. \ncommitlog_sync: periodic\ncommitlog_sync_period_in_ms: 10000\n\n# The size of the individual commitlog file segments.  A commitlog\n# segment may be archived, deleted, or recycled once all the data\n# in it (potentially from each columnfamily in the system) has been\n# flushed to sstables.\n#\n# The default size is 32, which is almost always fine, but if you are\n# archiving commitlog segments (see commitlog_archiving.properties),\n# then you probably want a finer granularity of archiving; 8 or 16 MB\n# is reasonable.\n# Max mutation size is also configurable via max_mutation_size_in_kb setting in\n# cassandra.yaml. The default is half the size commitlog_segment_size_in_mb * 1024.\n#\n# NOTE: If max_mutation_size_in_kb is set explicitly then commitlog_segment_size_in_mb must\n# be set to at least twice the size of max_mutation_size_in_kb / 1024\n#\ncommitlog_segment_size_in_mb: 32\n\n# Compression to apply to the commit log. If omitted, the commit log\n# will be written uncompressed.  LZ4, Snappy, and Deflate compressors\n# are supported.\n#commitlog_compression:\n#   - class_name: LZ4Compressor\n#     parameters:\n#         -\n\n# any class that implements the SeedProvider interface and has a\n# constructor that takes a Map\u003cString, String\u003e of parameters will do.\nseed_provider:\n    # Addresses of hosts that are deemed contact points. \n    # Cassandra nodes use this list of hosts to find each other and learn\n    # the topology of the ring.  You must change this if you are running\n    # multiple nodes!\n    - class_name: org.apache.cassandra.locator.SimpleSeedProvider\n      parameters:\n          # seeds is actually a comma-delimited list of addresses.\n          # Ex: \"\u003cip1\u003e,\u003cip2\u003e,\u003cip3\u003e\"\n          - seeds: \"172.31.48.41,172.31.77.64,172.31.6.131\"\n\n# For workloads with more data than can fit in memory, Cassandra's\n# bottleneck will be reads that need to fetch data from\n# disk. \"concurrent_reads\" should be set to (16 * number_of_drives) in\n# order to allow the operations to enqueue low enough in the stack\n# that the OS and drives can reorder them. Same applies to\n# \"concurrent_counter_writes\", since counter writes read the current\n# values before incrementing and writing them back.\n#\n# On the other hand, since writes are almost never IO bound, the ideal\n# number of \"concurrent_writes\" is dependent on the number of cores in\n# your system; (8 * number_of_cores) is a good rule of thumb.\nconcurrent_reads: 32\nconcurrent_writes: 32\nconcurrent_counter_writes: 32\n\n# For materialized view writes, as there is a read involved, so this should\n# be limited by the less of concurrent reads or concurrent writes.\nconcurrent_materialized_view_writes: 32\n\n# Maximum memory to use for sstable chunk cache and buffer pooling.\n# 32MB of this are reserved for pooling buffers, the rest is used as an\n# cache that holds uncompressed sstable chunks.\n# Defaults to the smaller of 1/4 of heap or 512MB. This pool is allocated off-heap,\n# so is in addition to the memory allocated for heap. The cache also has on-heap\n# overhead which is roughly 128 bytes per chunk (i.e. 0.2% of the reserved size\n# if the default 64k chunk size is used).\n# Memory is only allocated when needed.\n# file_cache_size_in_mb: 512\n\n# Flag indicating whether to allocate on or off heap when the sstable buffer\n# pool is exhausted, that is when it has exceeded the maximum memory\n# file_cache_size_in_mb, beyond which it will not cache buffers but allocate on request.\n\n# buffer_pool_use_heap_if_exhausted: true\n\n# The strategy for optimizing disk read\n# Possible values are:\n# ssd (for solid state disks, the default)\n# spinning (for spinning disks)\n# disk_optimization_strategy: ssd\n\n# Total permitted memory to use for memtables. Cassandra will stop\n# accepting writes when the limit is exceeded until a flush completes,\n# and will trigger a flush based on memtable_cleanup_threshold\n# If omitted, Cassandra will set both to 1/4 the size of the heap.\n# memtable_heap_space_in_mb: 2048\n# memtable_offheap_space_in_mb: 2048\n\n# Ratio of occupied non-flushing memtable size to total permitted size\n# that will trigger a flush of the largest memtable. Larger mct will\n# mean larger flushes and hence less compaction, but also less concurrent\n# flush activity which can make it difficult to keep your disks fed\n# under heavy write load.\n#\n# memtable_cleanup_threshold defaults to 1 / (memtable_flush_writers + 1)\n# memtable_cleanup_threshold: 0.11\n\n# Specify the way Cassandra allocates and manages memtable memory.\n# Options are:\n#   heap_buffers:    on heap nio buffers\n#   offheap_buffers: off heap (direct) nio buffers\n#   offheap_objects: off heap objects\nmemtable_allocation_type: heap_buffers\n\n# Total space to use for commit logs on disk.\n#\n# If space gets above this value, Cassandra will flush every dirty CF\n# in the oldest segment and remove it.  So a small total commitlog space\n# will tend to cause more flush activity on less-active columnfamilies.\n#\n# The default value is the smaller of 8192, and 1/4 of the total space\n# of the commitlog volume.\n#\n# commitlog_total_space_in_mb: 8192\n\n# This sets the amount of memtable flush writer threads.  These will\n# be blocked by disk io, and each one will hold a memtable in memory\n# while blocked.\n#\n# memtable_flush_writers defaults to one per data_file_directory.\n#\n# If your data directories are backed by SSD, you can increase this, but\n# avoid having memtable_flush_writers * data_file_directories \u003e number of cores\n#memtable_flush_writers: 1\n\n# A fixed memory pool size in MB for for SSTable index summaries. If left\n# empty, this will default to 5% of the heap size. If the memory usage of\n# all index summaries exceeds this limit, SSTables with low read rates will\n# shrink their index summaries in order to meet this limit.  However, this\n# is a best-effort process. In extreme conditions Cassandra may need to use\n# more than this amount of memory.\nindex_summary_capacity_in_mb:\n\n# How frequently index summaries should be resampled.  This is done\n# periodically to redistribute memory from the fixed-size pool to sstables\n# proportional their recent read rates.  Setting to -1 will disable this\n# process, leaving existing index summaries at their current sampling level.\nindex_summary_resize_interval_in_minutes: 60\n\n# Whether to, when doing sequential writing, fsync() at intervals in\n# order to force the operating system to flush the dirty\n# buffers. Enable this to avoid sudden dirty buffer flushing from\n# impacting read latencies. Almost always a good idea on SSDs; not\n# necessarily on platters.\ntrickle_fsync: false\ntrickle_fsync_interval_in_kb: 10240\n\n# TCP port, for commands and data\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\nstorage_port: 7000\n\n# SSL port, for encrypted communication.  Unused unless enabled in\n# encryption_options\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\nssl_storage_port: 7001\n\n# Address or interface to bind to and tell other Cassandra nodes to connect to.\n# You _must_ change this if you want multiple nodes to be able to communicate!\n#\n# Set listen_address OR listen_interface, not both. Interfaces must correspond\n# to a single address, IP aliasing is not supported.\n#\n# Leaving it blank leaves it up to InetAddress.getLocalHost(). This\n# will always do the Right Thing _if_ the node is properly configured\n# (hostname, name resolution, etc), and the Right Thing is to use the\n# address associated with the hostname (it might not be).\n#\n# Setting listen_address to 0.0.0.0 is always wrong.\n#\n# If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\n# you can specify which should be chosen using listen_interface_prefer_ipv6. If false the first ipv4\n# address will be used. If true the first ipv6 address will be used. Defaults to false preferring\n# ipv4. If there is only one address it will be selected regardless of ipv4/ipv6.\nlisten_address: 172.31.48.41\n# listen_interface: eth0\n# listen_interface_prefer_ipv6: false\n\n# Address to broadcast to other Cassandra nodes\n# Leaving this blank will set it to the same value as listen_address\n# broadcast_address: 1.2.3.4\n\n# When using multiple physical network interfaces, set this\n# to true to listen on broadcast_address in addition to\n# the listen_address, allowing nodes to communicate in both\n# interfaces.\n# Ignore this property if the network configuration automatically\n# routes  between the public and private networks such as EC2.\n# listen_on_broadcast_address: false\n\n# Internode authentication backend, implementing IInternodeAuthenticator;\n# used to allow/disallow connections from peer nodes.\n# internode_authenticator: org.apache.cassandra.auth.AllowAllInternodeAuthenticator\n\n# Whether to start the native transport server.\n# Please note that the address on which the native transport is bound is the\n# same as the rpc_address. The port however is different and specified below.\nstart_native_transport: true\n# port for the CQL native transport to listen for clients on\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\nnative_transport_port: 9042\n# Enabling native transport encryption in client_encryption_options allows you to either use\n# encryption for the standard port or to use a dedicated, additional port along with the unencrypted\n# standard native_transport_port.\n# Enabling client encryption and keeping native_transport_port_ssl disabled will use encryption\n# for native_transport_port. Setting native_transport_port_ssl to a different value\n# from native_transport_port will use encryption for native_transport_port_ssl while\n# keeping native_transport_port unencrypted.\n# native_transport_port_ssl: 9142\n# The maximum threads for handling requests when the native transport is used.\n# This is similar to rpc_max_threads though the default differs slightly (and\n# there is no native_transport_min_threads, idle threads will always be stopped\n# after 30 seconds).\n# native_transport_max_threads: 128\n#\n# The maximum size of allowed frame. Frame (requests) larger than this will\n# be rejected as invalid. The default is 256MB.\n# native_transport_max_frame_size_in_mb: 256\n\n# The maximum number of concurrent client connections.\n# The default is -1, which means unlimited.\n# native_transport_max_concurrent_connections: -1\n\n# The maximum number of concurrent client connections per source ip.\n# The default is -1, which means unlimited.\n# native_transport_max_concurrent_connections_per_ip: -1\n\n# Whether to start the thrift rpc server.\nstart_rpc: true\n\n# The address or interface to bind the Thrift RPC service and native transport\n# server to.\n#\n# Set rpc_address OR rpc_interface, not both. Interfaces must correspond\n# to a single address, IP aliasing is not supported.\n#\n# Leaving rpc_address blank has the same effect as on listen_address\n# (i.e. it will be based on the configured hostname of the node).\n#\n# Note that unlike listen_address, you can specify 0.0.0.0, but you must also\n# set broadcast_rpc_address to a value other than 0.0.0.0.\n#\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\n#\n# If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\n# you can specify which should be chosen using rpc_interface_prefer_ipv6. If false the first ipv4\n# address will be used. If true the first ipv6 address will be used. Defaults to false preferring\n# ipv4. If there is only one address it will be selected regardless of ipv4/ipv6.\nrpc_address: 172.31.48.41\n# rpc_interface: eth1\n# rpc_interface_prefer_ipv6: false\n\n# port for Thrift to listen for clients on\nrpc_port: 9160\n\n# RPC address to broadcast to drivers and other Cassandra nodes. This cannot\n# be set to 0.0.0.0. If left blank, this will be set to the value of\n# rpc_address. If rpc_address is set to 0.0.0.0, broadcast_rpc_address must\n# be set.\n#broadcast_rpc_address: \n\n# enable or disable keepalive on rpc/native connections\nrpc_keepalive: true\n\n# Cassandra provides two out-of-the-box options for the RPC Server:\n#\n# sync  -\u003e One thread per thrift connection. For a very large number of clients, memory\n#          will be your limiting factor. On a 64 bit JVM, 180KB is the minimum stack size\n#          per thread, and that will correspond to your use of virtual memory (but physical memory\n#          may be limited depending on use of stack space).\n#\n# hsha  -\u003e Stands for \"half synchronous, half asynchronous.\" All thrift clients are handled\n#          asynchronously using a small number of threads that does not vary with the amount\n#          of thrift clients (and thus scales well to many clients). The rpc requests are still\n#          synchronous (one thread per active request). If hsha is selected then it is essential\n#          that rpc_max_threads is changed from the default value of unlimited.\n#\n# The default is sync because on Windows hsha is about 30% slower.  On Linux,\n# sync/hsha performance is about the same, with hsha of course using less memory.\n#\n# Alternatively,  can provide your own RPC server by providing the fully-qualified class name\n# of an o.a.c.t.TServerFactory that can create an instance of it.\nrpc_server_type: sync\n\n# Uncomment rpc_min|max_thread to set request pool size limits.\n#\n# Regardless of your choice of RPC server (see above), the number of maximum requests in the\n# RPC thread pool dictates how many concurrent requests are possible (but if you are using the sync\n# RPC server, it also dictates the number of clients that can be connected at all).\n#\n# The default is unlimited and thus provides no protection against clients overwhelming the server. You are\n# encouraged to set a maximum that makes sense for you in production, but do keep in mind that\n# rpc_max_threads represents the maximum number of client requests this server may execute concurrently.\n#\n# rpc_min_threads: 16\n# rpc_max_threads: 2048\n\n# uncomment to set socket buffer sizes on rpc connections\n# rpc_send_buff_size_in_bytes:\n# rpc_recv_buff_size_in_bytes:\n\n# Uncomment to set socket buffer size for internode communication\n# Note that when setting this, the buffer size is limited by net.core.wmem_max\n# and when not setting it it is defined by net.ipv4.tcp_wmem\n# See:\n# /proc/sys/net/core/wmem_max\n# /proc/sys/net/core/rmem_max\n# /proc/sys/net/ipv4/tcp_wmem\n# /proc/sys/net/ipv4/tcp_wmem\n# and: man tcp\n# internode_send_buff_size_in_bytes:\n# internode_recv_buff_size_in_bytes:\n\n# Frame size for thrift (maximum message length).\nthrift_framed_transport_size_in_mb: 15\n\n# Set to true to have Cassandra create a hard link to each sstable\n# flushed or streamed locally in a backups/ subdirectory of the\n# keyspace data.  Removing these links is the operator's\n# responsibility.\nincremental_backups: false\n\n# Whether or not to take a snapshot before each compaction.  Be\n# careful using this option, since Cassandra won't clean up the\n# snapshots for you.  Mostly useful if you're paranoid when there\n# is a data format change.\nsnapshot_before_compaction: false\n\n# Whether or not a snapshot is taken of the data before keyspace truncation\n# or dropping of column families. The STRONGLY advised default of true \n# should be used to provide data safety. If you set this flag to false, you will\n# lose data on truncation or drop.\nauto_snapshot: true\n\n# Granularity of the collation index of rows within a partition.\n# Increase if your rows are large, or if you have a very large\n# number of rows per partition.  The competing goals are these:\n#   1) a smaller granularity means more index entries are generated\n#      and looking up rows withing the partition by collation column\n#      is faster\n#   2) but, Cassandra will keep the collation index in memory for hot\n#      rows (as part of the key cache), so a larger granularity means\n#      you can cache more hot rows\ncolumn_index_size_in_kb: 64\n# Per sstable indexed key cache entries (the collation index in memory\n# mentioned above) exceeding this size will not be held on heap.\n# This means that only partition information is held on heap and the\n# index entries are read from disk.\n#\n# Note that this size refers to the size of the\n# serialized index information and not the size of the partition.\ncolumn_index_cache_size_in_kb: 2\n\n# Number of simultaneous compactions to allow, NOT including\n# validation \"compactions\" for anti-entropy repair.  Simultaneous\n# compactions can help preserve read performance in a mixed read/write\n# workload, by mitigating the tendency of small sstables to accumulate\n# during a single long running compactions. The default is usually\n# fine and if you experience problems with compaction running too\n# slowly or too fast, you should look at\n# compaction_throughput_mb_per_sec first.\n#\n# concurrent_compactors defaults to the smaller of (number of disks,\n# number of cores), with a minimum of 2 and a maximum of 8.\n# \n# If your data directories are backed by SSD, you should increase this\n# to the number of cores.\n#concurrent_compactors: 1\n\n# Throttles compaction to the given total throughput across the entire\n# system. The faster you insert data, the faster you need to compact in\n# order to keep the sstable count down, but in general, setting this to\n# 16 to 32 times the rate you are inserting data is more than sufficient.\n# Setting this to 0 disables throttling. Note that this account for all types\n# of compaction, including validation compaction.\ncompaction_throughput_mb_per_sec: 16\n\n# When compacting, the replacement sstable(s) can be opened before they\n# are completely written, and used in place of the prior sstables for\n# any range that has been written. This helps to smoothly transfer reads \n# between the sstables, reducing page cache churn and keeping hot rows hot\nsstable_preemptive_open_interval_in_mb: 50\n\n# Throttles all outbound streaming file transfers on this node to the\n# given total throughput in Mbps. This is necessary because Cassandra does\n# mostly sequential IO when streaming data during bootstrap or repair, which\n# can lead to saturating the network connection and degrading rpc performance.\n# When unset, the default is 200 Mbps or 25 MB/s.\n# stream_throughput_outbound_megabits_per_sec: 200\n\n# Throttles all streaming file transfer between the datacenters,\n# this setting allows users to throttle inter dc stream throughput in addition\n# to throttling all network stream traffic as configured with\n# stream_throughput_outbound_megabits_per_sec\n# When unset, the default is 200 Mbps or 25 MB/s\n# inter_dc_stream_throughput_outbound_megabits_per_sec: 200\n\n# How long the coordinator should wait for read operations to complete\nread_request_timeout_in_ms: 5000\n# How long the coordinator should wait for seq or index scans to complete\nrange_request_timeout_in_ms: 10000\n# How long the coordinator should wait for writes to complete\nwrite_request_timeout_in_ms: 2000\n# How long the coordinator should wait for counter writes to complete\ncounter_write_request_timeout_in_ms: 5000\n# How long a coordinator should continue to retry a CAS operation\n# that contends with other proposals for the same row\ncas_contention_timeout_in_ms: 1000\n# How long the coordinator should wait for truncates to complete\n# (This can be much longer, because unless auto_snapshot is disabled\n# we need to flush first so we can snapshot before removing the data.)\ntruncate_request_timeout_in_ms: 60000\n# The default timeout for other, miscellaneous operations\nrequest_timeout_in_ms: 10000\n\n# Enable operation timeout information exchange between nodes to accurately\n# measure request timeouts.  If disabled, replicas will assume that requests\n# were forwarded to them instantly by the coordinator, which means that\n# under overload conditions we will waste that much extra time processing \n# already-timed-out requests.\n#\n# Warning: before enabling this property make sure to ntp is installed\n# and the times are synchronized between the nodes.\ncross_node_timeout: false\n\n# Set socket timeout for streaming operation.\n# The stream session is failed if no data is received by any of the\n# participants within that period.\n# Default value is 3600000, which means streams timeout after an hour.\n# streaming_socket_timeout_in_ms: 3600000\n\n# phi value that must be reached for a host to be marked down.\n# most users should never need to adjust this.\n# phi_convict_threshold: 8\n\n# endpoint_snitch -- Set this to a class that implements\n# IEndpointSnitch.  The snitch has two functions:\n# - it teaches Cassandra enough about your network topology to route\n#   requests efficiently\n# - it allows Cassandra to spread replicas around your cluster to avoid\n#   correlated failures. It does this by grouping machines into\n#   \"datacenters\" and \"racks.\"  Cassandra will do its best not to have\n#   more than one replica on the same \"rack\" (which may not actually\n#   be a physical location)\n#\n# IF YOU CHANGE THE SNITCH AFTER DATA IS INSERTED INTO THE CLUSTER,\n# YOU MUST RUN A FULL REPAIR, SINCE THE SNITCH AFFECTS WHERE REPLICAS\n# ARE PLACED.\n#\n# IF THE RACK A REPLICA IS PLACED IN CHANGES AFTER THE REPLICA HAS BEEN\n# ADDED TO A RING, THE NODE MUST BE DECOMMISSIONED AND REBOOTSTRAPPED.\n#\n# Out of the box, Cassandra provides\n#  - SimpleSnitch:\n#    Treats Strategy order as proximity. This can improve cache\n#    locality when disabling read repair.  Only appropriate for\n#    single-datacenter deployments.\n#  - GossipingPropertyFileSnitch\n#    This should be your go-to snitch for production use.  The rack\n#    and datacenter for the local node are defined in\n#    cassandra-rackdc.properties and propagated to other nodes via\n#    gossip.  If cassandra-topology.properties exists, it is used as a\n#    fallback, allowing migration from the PropertyFileSnitch.\n#  - PropertyFileSnitch:\n#    Proximity is determined by rack and data center, which are\n#    explicitly configured in cassandra-topology.properties.\n#  - Ec2Snitch:\n#    Appropriate for EC2 deployments in a single Region. Loads Region\n#    and Availability Zone information from the EC2 API. The Region is\n#    treated as the datacenter, and the Availability Zone as the rack.\n#    Only private IPs are used, so this will not work across multiple\n#    Regions.\n#  - Ec2MultiRegionSnitch:\n#    Uses public IPs as broadcast_address to allow cross-region\n#    connectivity.  (Thus, you should set seed addresses to the public\n#    IP as well.) You will need to open the storage_port or\n#    ssl_storage_port on the public IP firewall.  (For intra-Region\n#    traffic, Cassandra will switch to the private IP after\n#    establishing a connection.)\n#  - RackInferringSnitch:\n#    Proximity is determined by rack and data center, which are\n#    assumed to correspond to the 3rd and 2nd octet of each node's IP\n#    address, respectively.  Unless this happens to match your\n#    deployment conventions, this is best used as an example of\n#    writing a custom Snitch class and is provided in that spirit.\n#\n# You can use a custom Snitch by setting this to the full class name\n# of the snitch, which will be assumed to be on your classpath.\nendpoint_snitch: GossipingPropertyFileSnitch\n\n# controls how often to perform the more expensive part of host score\n# calculation\ndynamic_snitch_update_interval_in_ms: 100 \n# controls how often to reset all host scores, allowing a bad host to\n# possibly recover\ndynamic_snitch_reset_interval_in_ms: 600000\n# if set greater than zero and read_repair_chance is \u003c 1.0, this will allow\n# 'pinning' of replicas to hosts in order to increase cache capacity.\n# The badness threshold will control how much worse the pinned host has to be\n# before the dynamic snitch will prefer other replicas over it.  This is\n# expressed as a double which represents a percentage.  Thus, a value of\n# 0.2 means Cassandra would continue to prefer the static snitch values\n# until the pinned host was 20% worse than the fastest.\ndynamic_snitch_badness_threshold: 0.1\n\n# request_scheduler -- Set this to a class that implements\n# RequestScheduler, which will schedule incoming client requests\n# according to the specific policy. This is useful for multi-tenancy\n# with a single Cassandra cluster.\n# NOTE: This is specifically for requests from the client and does\n# not affect inter node communication.\n# org.apache.cassandra.scheduler.NoScheduler - No scheduling takes place\n# org.apache.cassandra.scheduler.RoundRobinScheduler - Round robin of\n# client requests to a node with a separate queue for each\n# request_scheduler_id. The scheduler is further customized by\n# request_scheduler_options as described below.\nrequest_scheduler: org.apache.cassandra.scheduler.NoScheduler\n\n# Scheduler Options vary based on the type of scheduler\n# NoScheduler - Has no options\n# RoundRobin\n#  - throttle_limit -- The throttle_limit is the number of in-flight\n#                      requests per client.  Requests beyond \n#                      that limit are queued up until\n#                      running requests can complete.\n#                      The value of 80 here is twice the number of\n#                      concurrent_reads + concurrent_writes.\n#  - default_weight -- default_weight is optional and allows for\n#                      overriding the default which is 1.\n#  - weights -- Weights are optional and will default to 1 or the\n#               overridden default_weight. The weight translates into how\n#               many requests are handled during each turn of the\n#               RoundRobin, based on the scheduler id.\n#\n# request_scheduler_options:\n#    throttle_limit: 80\n#    default_weight: 5\n#    weights:\n#      Keyspace1: 1\n#      Keyspace2: 5\n\n# request_scheduler_id -- An identifier based on which to perform\n# the request scheduling. Currently the only valid option is keyspace.\n# request_scheduler_id: keyspace\n\n# Enable or disable inter-node encryption\n# JVM defaults for supported SSL socket protocols and cipher suites can\n# be replaced using custom encryption options. This is not recommended\n# unless you have policies in place that dictate certain settings, or\n# need to disable vulnerable ciphers or protocols in case the JVM cannot\n# be updated.\n# FIPS compliant settings can be configured at JVM level and should not\n# involve changing encryption settings here:\n# https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/FIPS.html\n# NOTE: No custom encryption options are enabled at the moment\n# The available internode options are : all, none, dc, rack\n#\n# If set to dc cassandra will encrypt the traffic between the DCs\n# If set to rack cassandra will encrypt the traffic between the racks\n#\n# The passwords used in these options must match the passwords used when generating\n# the keystore and truststore.  For instructions on generating these files, see:\n# http://download.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html#CreateKeystore\n#\nserver_encryption_options:\n    internode_encryption: none\n    keystore: conf/.keystore\n    keystore_password: cassandra\n    truststore: conf/.truststore\n    truststore_password: cassandra\n    # More advanced defaults below:\n    # protocol: TLS\n    # algorithm: SunX509\n    # store_type: JKS\n    # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n    # require_client_auth: false\n    # require_endpoint_verification: false\n\n# enable or disable client/server encryption.\nclient_encryption_options:\n    enabled: false\n    # If enabled and optional is set to true encrypted and unencrypted connections are handled.\n    optional: false\n    keystore: conf/.keystore\n    keystore_password: cassandra\n    # require_client_auth: false\n    # Set trustore and truststore_password if require_client_auth is true\n    # truststore: conf/.truststore\n    # truststore_password: cassandra\n    # More advanced defaults below:\n    # protocol: TLS\n    # algorithm: SunX509\n    # store_type: JKS\n    # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n\n# internode_compression controls whether traffic between nodes is\n# compressed.\n# can be:  all  - all traffic is compressed\n#          dc   - traffic between different datacenters is compressed\n#          none - nothing is compressed.\ninternode_compression: dc\n\n# Enable or disable tcp_nodelay for inter-dc communication.\n# Disabling it will result in larger (but fewer) network packets being sent,\n# reducing overhead from the TCP protocol itself, at the cost of increasing\n# latency if you block for cross-datacenter responses.\ninter_dc_tcp_nodelay: false\n\n# TTL for different trace types used during logging of the repair process.\ntracetype_query_ttl: 86400\ntracetype_repair_ttl: 604800\n\n# UDFs (user defined functions) are disabled by default.\n# As of Cassandra 3.0 there is a sandbox in place that should prevent execution of evil code.\nenable_user_defined_functions: false\n\n# Enables scripted UDFs (JavaScript UDFs).\n# Java UDFs are always enabled, if enable_user_defined_functions is true.\n# Enable this option to be able to use UDFs with \"language javascript\" or any custom JSR-223 provider.\n# This option has no effect, if enable_user_defined_functions is false.\nenable_scripted_user_defined_functions: false\n\n# The default Windows kernel timer and scheduling resolution is 15.6ms for power conservation.\n# Lowering this value on Windows can provide much tighter latency and better throughput, however\n# some virtualized environments may see a negative performance impact from changing this setting\n# below their system default. The sysinternals 'clockres' tool can confirm your system's default\n# setting.\nwindows_timer_interval: 1\n\n\n# Enables encrypting data at-rest (on disk). Different key providers can be plugged in, but the default reads from\n# a JCE-style keystore. A single keystore can hold multiple keys, but the one referenced by\n# the \"key_alias\" is the only key that will be used for encrypt opertaions; previously used keys\n# can still (and should!) be in the keystore and will be used on decrypt operations\n# (to handle the case of key rotation).\n#\n# It is strongly recommended to download and install Java Cryptography Extension (JCE)\n# Unlimited Strength Jurisdiction Policy Files for your version of the JDK.\n# (current link: http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html)\n#\n# Currently, only the following file types are supported for transparent data encryption, although\n# more are coming in future cassandra releases: commitlog, hints\ntransparent_data_encryption_options:\n    enabled: false\n    chunk_length_kb: 64\n    cipher: AES/CBC/PKCS5Padding\n    key_alias: testing:1\n    # CBC IV length for AES needs to be 16 bytes (which is also the default size)\n    # iv_length: 16\n    key_provider: \n      - class_name: org.apache.cassandra.security.JKSKeyProvider\n        parameters: \n          - keystore: conf/.keystore\n            keystore_password: cassandra\n            store_type: JCEKS\n            key_password: cassandra\n\n\n#####################\n# SAFETY THRESHOLDS #\n#####################\n\n# When executing a scan, within or across a partition, we need to keep the\n# tombstones seen in memory so we can return them to the coordinator, which\n# will use them to make sure other replicas also know about the deleted rows.\n# With workloads that generate a lot of tombstones, this can cause performance\n# problems and even exaust the server heap.\n# (http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets)\n# Adjust the thresholds here if you understand the dangers and want to\n# scan more tombstones anyway.  These thresholds may also be adjusted at runtime\n# using the StorageService mbean.\ntombstone_warn_threshold: 1000\ntombstone_failure_threshold: 100000\n\n# Log WARN on any batch size exceeding this value. 5kb per batch by default.\n# Caution should be taken on increasing the size of this threshold as it can lead to node instability.\nbatch_size_warn_threshold_in_kb: 5\n\n# Fail any batch exceeding this value. 50kb (10x warn threshold) by default.\nbatch_size_fail_threshold_in_kb: 50\n\n# Log WARN on any batches not of type LOGGED than span across more partitions than this limit\nunlogged_batch_across_partitions_warn_threshold: 10\n\n# Log a warning when compacting partitions larger than this value\ncompaction_large_partition_warning_threshold_mb: 100\n\n# GC Pauses greater than gc_warn_threshold_in_ms will be logged at WARN level\n# Adjust the threshold based on your application throughput requirement\n# By default, Cassandra logs GC Pauses greater than 200 ms at INFO level\ngc_warn_threshold_in_ms: 1000\n\nauto_bootstrap: false\n",
                            "template": "# Cassandra storage config YAML \n\n# NOTE:\n#   See http://wiki.apache.org/cassandra/StorageConfiguration for\n#   full explanations of configuration directives\n# /NOTE\n\n# The name of the cluster. This is mainly used to prevent machines in\n# one logical cluster from joining another.\ncluster_name: '${cluster_name}'\n\n# This defines the number of tokens randomly assigned to this node on the ring\n# The more tokens, relative to other nodes, the larger the proportion of data\n# that this node will store. You probably want all nodes to have the same number\n# of tokens assuming they have equal hardware capability.\n#\n# If you leave this unspecified, Cassandra will use the default of 1 token for legacy compatibility,\n# and will use the initial_token as described below.\n#\n# Specifying initial_token will override this setting on the node's initial start,\n# on subsequent starts, this setting will apply even if initial token is set.\n#\n# If you already have a cluster with 1 token per node, and wish to migrate to \n# multiple tokens per node, see http://wiki.apache.org/cassandra/Operations\nnum_tokens: 256\n\n# Triggers automatic allocation of num_tokens tokens for this node. The allocation\n# algorithm attempts to choose tokens in a way that optimizes replicated load over\n# the nodes in the datacenter for the replication strategy used by the specified\n# keyspace.\n#\n# The load assigned to each node will be close to proportional to its number of\n# vnodes.\n#\n# Only supported with the Murmur3Partitioner.\n# allocate_tokens_for_keyspace: KEYSPACE\n\n# initial_token allows you to specify tokens manually.  While you can use # it with\n# vnodes (num_tokens \u003e 1, above) -- in which case you should provide a \n# comma-separated list -- it's primarily used when adding nodes # to legacy clusters \n# that do not have vnodes enabled.\n# initial_token:\n\n# See http://wiki.apache.org/cassandra/HintedHandoff\n# May either be \"true\" or \"false\" to enable globally\nhinted_handoff_enabled: true\n# When hinted_handoff_enabled is true, a black list of data centers that will not\n# perform hinted handoff\n#hinted_handoff_disabled_datacenters:\n#    - DC1\n#    - DC2\n# this defines the maximum amount of time a dead host will have hints\n# generated.  After it has been dead this long, new hints for it will not be\n# created until it has been seen alive and gone down again.\nmax_hint_window_in_ms: 10800000 # 3 hours\n\n# Maximum throttle in KBs per second, per delivery thread.  This will be\n# reduced proportionally to the number of nodes in the cluster.  (If there\n# are two nodes in the cluster, each delivery thread will use the maximum\n# rate; if there are three, each will throttle to half of the maximum,\n# since we expect two nodes to be delivering hints simultaneously.)\nhinted_handoff_throttle_in_kb: 1024\n\n# Number of threads with which to deliver hints;\n# Consider increasing this number when you have multi-dc deployments, since\n# cross-dc handoff tends to be slower\nmax_hints_delivery_threads: 2\n\n# Directory where Cassandra should store hints.\n# If not set, the default directory is $CASSANDRA_HOME/data/hints.\n# hints_directory: /var/lib/cassandra/hints\n\n# How often hints should be flushed from the internal buffers to disk.\n# Will *not* trigger fsync.\nhints_flush_period_in_ms: 10000\n\n# Maximum size for a single hints file, in megabytes.\nmax_hints_file_size_in_mb: 128\n\n# Compression to apply to the hint files. If omitted, hints files\n# will be written uncompressed. LZ4, Snappy, and Deflate compressors\n# are supported.\n#hints_compression:\n#   - class_name: LZ4Compressor\n#     parameters:\n#         -\n\n# Maximum throttle in KBs per second, total. This will be\n# reduced proportionally to the number of nodes in the cluster.\nbatchlog_replay_throttle_in_kb: 1024\n\n# Authentication backend, implementing IAuthenticator; used to identify users\n# Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthenticator,\n# PasswordAuthenticator}.\n#\n# - AllowAllAuthenticator performs no checks - set it to disable authentication.\n# - PasswordAuthenticator relies on username/password pairs to authenticate\n#   users. It keeps usernames and hashed passwords in system_auth.credentials table.\n#   Please increase system_auth keyspace replication factor if you use this authenticator.\n#   If using PasswordAuthenticator, CassandraRoleManager must also be used (see below)\nauthenticator: AllowAllAuthenticator\n\n# Authorization backend, implementing IAuthorizer; used to limit access/provide permissions\n# Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthorizer,\n# CassandraAuthorizer}.\n#\n# - AllowAllAuthorizer allows any action to any user - set it to disable authorization.\n# - CassandraAuthorizer stores permissions in system_auth.permissions table. Please\n#   increase system_auth keyspace replication factor if you use this authorizer.\nauthorizer: AllowAllAuthorizer\n\n# Part of the Authentication \u0026 Authorization backend, implementing IRoleManager; used\n# to maintain grants and memberships between roles.\n# Out of the box, Cassandra provides org.apache.cassandra.auth.CassandraRoleManager,\n# which stores role information in the system_auth keyspace. Most functions of the\n# IRoleManager require an authenticated login, so unless the configured IAuthenticator\n# actually implements authentication, most of this functionality will be unavailable.\n#\n# - CassandraRoleManager stores role data in the system_auth keyspace. Please\n#   increase system_auth keyspace replication factor if you use this role manager.\nrole_manager: CassandraRoleManager\n\n# Validity period for roles cache (fetching granted roles can be an expensive\n# operation depending on the role manager, CassandraRoleManager is one example)\n# Granted roles are cached for authenticated sessions in AuthenticatedUser and\n# after the period specified here, become eligible for (async) reload.\n# Defaults to 2000, set to 0 to disable caching entirely.\n# Will be disabled automatically for AllowAllAuthenticator.\nroles_validity_in_ms: 2000\n\n# Refresh interval for roles cache (if enabled).\n# After this interval, cache entries become eligible for refresh. Upon next\n# access, an async reload is scheduled and the old value returned until it\n# completes. If roles_validity_in_ms is non-zero, then this must be\n# also.\n# Defaults to the same value as roles_validity_in_ms.\n# roles_update_interval_in_ms: 2000\n\n# Validity period for permissions cache (fetching permissions can be an\n# expensive operation depending on the authorizer, CassandraAuthorizer is\n# one example). Defaults to 2000, set to 0 to disable.\n# Will be disabled automatically for AllowAllAuthorizer.\npermissions_validity_in_ms: 2000\n\n# Refresh interval for permissions cache (if enabled).\n# After this interval, cache entries become eligible for refresh. Upon next\n# access, an async reload is scheduled and the old value returned until it\n# completes. If permissions_validity_in_ms is non-zero, then this must be\n# also.\n# Defaults to the same value as permissions_validity_in_ms.\n# permissions_update_interval_in_ms: 2000\n\n# Validity period for credentials cache. This cache is tightly coupled to\n# the provided PasswordAuthenticator implementation of IAuthenticator. If\n# another IAuthenticator implementation is configured, this cache will not\n# be automatically used and so the following settings will have no effect.\n# Please note, credentials are cached in their encrypted form, so while\n# activating this cache may reduce the number of queries made to the\n# underlying table, it may not  bring a significant reduction in the\n# latency of individual authentication attempts.\n# Defaults to 2000, set to 0 to disable credentials caching.\ncredentials_validity_in_ms: 2000\n\n# Refresh interval for credentials cache (if enabled).\n# After this interval, cache entries become eligible for refresh. Upon next\n# access, an async reload is scheduled and the old value returned until it\n# completes. If credentials_validity_in_ms is non-zero, then this must be\n# also.\n# Defaults to the same value as credentials_validity_in_ms.\n# credentials_update_interval_in_ms: 2000\n\n# The partitioner is responsible for distributing groups of rows (by\n# partition key) across nodes in the cluster.  You should leave this\n# alone for new clusters.  The partitioner can NOT be changed without\n# reloading all data, so when upgrading you should set this to the\n# same partitioner you were already using.\n#\n# Besides Murmur3Partitioner, partitioners included for backwards\n# compatibility include RandomPartitioner, ByteOrderedPartitioner, and\n# OrderPreservingPartitioner.\n#\npartitioner: org.apache.cassandra.dht.Murmur3Partitioner\n\n# Directories where Cassandra should store data on disk.  Cassandra\n# will spread data evenly across them, subject to the granularity of\n# the configured compaction strategy.\n# If not set, the default directory is $CASSANDRA_HOME/data/data.\ndata_file_directories:\n    - /var/lib/cassandra/data\n\n# commit log.  when running on magnetic HDD, this should be a\n# separate spindle than the data directories.\n# If not set, the default directory is $CASSANDRA_HOME/data/commitlog.\ncommitlog_directory: /var/lib/cassandra/commitlog\n\n# policy for data disk failures:\n# die: shut down gossip and client transports and kill the JVM for any fs errors or\n#      single-sstable errors, so the node can be replaced.\n# stop_paranoid: shut down gossip and client transports even for single-sstable errors,\n#                kill the JVM for errors during startup.\n# stop: shut down gossip and client transports, leaving the node effectively dead, but\n#       can still be inspected via JMX, kill the JVM for errors during startup.\n# best_effort: stop using the failed disk and respond to requests based on\n#              remaining available sstables.  This means you WILL see obsolete\n#              data at CL.ONE!\n# ignore: ignore fatal errors and let requests fail, as in pre-1.2 Cassandra\ndisk_failure_policy: stop\n\n# policy for commit disk failures:\n# die: shut down gossip and Thrift and kill the JVM, so the node can be replaced.\n# stop: shut down gossip and Thrift, leaving the node effectively dead, but\n#       can still be inspected via JMX.\n# stop_commit: shutdown the commit log, letting writes collect but\n#              continuing to service reads, as in pre-2.0.5 Cassandra\n# ignore: ignore fatal errors and let the batches fail\ncommit_failure_policy: stop\n\n# Maximum size of the native protocol prepared statement cache\n#\n# Valid values are either \"auto\" (omitting the value) or a value greater 0.\n#\n# Note that specifying a too large value will result in long running GCs and possbily\n# out-of-memory errors. Keep the value at a small fraction of the heap.\n#\n# If you constantly see \"prepared statements discarded in the last minute because\n# cache limit reached\" messages, the first step is to investigate the root cause\n# of these messages and check whether prepared statements are used correctly -\n# i.e. use bind markers for variable parts.\n#\n# Do only change the default value, if you really have more prepared statements than\n# fit in the cache. In most cases it is not neccessary to change this value.\n# Constantly re-preparing statements is a performance penalty.\n#\n# Default value (\"auto\") is 1/256th of the heap or 10MB, whichever is greater\nprepared_statements_cache_size_mb:\n\n# Maximum size of the Thrift prepared statement cache\n#\n# If you do not use Thrift at all, it is safe to leave this value at \"auto\".\n#\n# See description of 'prepared_statements_cache_size_mb' above for more information.\n#\n# Default value (\"auto\") is 1/256th of the heap or 10MB, whichever is greater\nthrift_prepared_statements_cache_size_mb:\n\n# Maximum size of the key cache in memory.\n#\n# Each key cache hit saves 1 seek and each row cache hit saves 2 seeks at the\n# minimum, sometimes more. The key cache is fairly tiny for the amount of\n# time it saves, so it's worthwhile to use it at large numbers.\n# The row cache saves even more time, but must contain the entire row,\n# so it is extremely space-intensive. It's best to only use the\n# row cache if you have hot rows or static rows.\n#\n# NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.\n#\n# Default value is empty to make it \"auto\" (min(5% of Heap (in MB), 100MB)). Set to 0 to disable key cache.\nkey_cache_size_in_mb:\n\n# Duration in seconds after which Cassandra should\n# save the key cache. Caches are saved to saved_caches_directory as\n# specified in this configuration file.\n#\n# Saved caches greatly improve cold-start speeds, and is relatively cheap in\n# terms of I/O for the key cache. Row cache saving is much more expensive and\n# has limited use.\n#\n# Default is 14400 or 4 hours.\nkey_cache_save_period: 14400\n\n# Number of keys from the key cache to save\n# Disabled by default, meaning all keys are going to be saved\n# key_cache_keys_to_save: 100\n\n# Row cache implementation class name.\n# Available implementations:\n#   org.apache.cassandra.cache.OHCProvider                Fully off-heap row cache implementation (default).\n#   org.apache.cassandra.cache.SerializingCacheProvider   This is the row cache implementation availabile\n#                                                         in previous releases of Cassandra.\n# row_cache_class_name: org.apache.cassandra.cache.OHCProvider\n\n# Maximum size of the row cache in memory.\n# Please note that OHC cache implementation requires some additional off-heap memory to manage\n# the map structures and some in-flight memory during operations before/after cache entries can be\n# accounted against the cache capacity. This overhead is usually small compared to the whole capacity.\n# Do not specify more memory that the system can afford in the worst usual situation and leave some\n# headroom for OS block level cache. Do never allow your system to swap.\n#\n# Default value is 0, to disable row caching.\nrow_cache_size_in_mb: 0\n\n# Duration in seconds after which Cassandra should save the row cache.\n# Caches are saved to saved_caches_directory as specified in this configuration file.\n#\n# Saved caches greatly improve cold-start speeds, and is relatively cheap in\n# terms of I/O for the key cache. Row cache saving is much more expensive and\n# has limited use.\n#\n# Default is 0 to disable saving the row cache.\nrow_cache_save_period: 0\n\n# Number of keys from the row cache to save.\n# Specify 0 (which is the default), meaning all keys are going to be saved\n# row_cache_keys_to_save: 100\n\n# Maximum size of the counter cache in memory.\n#\n# Counter cache helps to reduce counter locks' contention for hot counter cells.\n# In case of RF = 1 a counter cache hit will cause Cassandra to skip the read before\n# write entirely. With RF \u003e 1 a counter cache hit will still help to reduce the duration\n# of the lock hold, helping with hot counter cell updates, but will not allow skipping\n# the read entirely. Only the local (clock, count) tuple of a counter cell is kept\n# in memory, not the whole counter, so it's relatively cheap.\n#\n# NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.\n#\n# Default value is empty to make it \"auto\" (min(2.5% of Heap (in MB), 50MB)). Set to 0 to disable counter cache.\n# NOTE: if you perform counter deletes and rely on low gcgs, you should disable the counter cache.\ncounter_cache_size_in_mb:\n\n# Duration in seconds after which Cassandra should\n# save the counter cache (keys only). Caches are saved to saved_caches_directory as\n# specified in this configuration file.\n#\n# Default is 7200 or 2 hours.\ncounter_cache_save_period: 7200\n\n# Number of keys from the counter cache to save\n# Disabled by default, meaning all keys are going to be saved\n# counter_cache_keys_to_save: 100\n\n# saved caches\n# If not set, the default directory is $CASSANDRA_HOME/data/saved_caches.\nsaved_caches_directory: /var/lib/cassandra/saved_caches\n\n# commitlog_sync may be either \"periodic\" or \"batch.\" \n# \n# When in batch mode, Cassandra won't ack writes until the commit log\n# has been fsynced to disk.  It will wait\n# commitlog_sync_batch_window_in_ms milliseconds between fsyncs.\n# This window should be kept short because the writer threads will\n# be unable to do extra work while waiting.  (You may need to increase\n# concurrent_writes for the same reason.)\n#\n# commitlog_sync: batch\n# commitlog_sync_batch_window_in_ms: 2\n#\n# the other option is \"periodic\" where writes may be acked immediately\n# and the CommitLog is simply synced every commitlog_sync_period_in_ms\n# milliseconds. \ncommitlog_sync: periodic\ncommitlog_sync_period_in_ms: 10000\n\n# The size of the individual commitlog file segments.  A commitlog\n# segment may be archived, deleted, or recycled once all the data\n# in it (potentially from each columnfamily in the system) has been\n# flushed to sstables.\n#\n# The default size is 32, which is almost always fine, but if you are\n# archiving commitlog segments (see commitlog_archiving.properties),\n# then you probably want a finer granularity of archiving; 8 or 16 MB\n# is reasonable.\n# Max mutation size is also configurable via max_mutation_size_in_kb setting in\n# cassandra.yaml. The default is half the size commitlog_segment_size_in_mb * 1024.\n#\n# NOTE: If max_mutation_size_in_kb is set explicitly then commitlog_segment_size_in_mb must\n# be set to at least twice the size of max_mutation_size_in_kb / 1024\n#\ncommitlog_segment_size_in_mb: 32\n\n# Compression to apply to the commit log. If omitted, the commit log\n# will be written uncompressed.  LZ4, Snappy, and Deflate compressors\n# are supported.\n#commitlog_compression:\n#   - class_name: LZ4Compressor\n#     parameters:\n#         -\n\n# any class that implements the SeedProvider interface and has a\n# constructor that takes a Map\u003cString, String\u003e of parameters will do.\nseed_provider:\n    # Addresses of hosts that are deemed contact points. \n    # Cassandra nodes use this list of hosts to find each other and learn\n    # the topology of the ring.  You must change this if you are running\n    # multiple nodes!\n    - class_name: org.apache.cassandra.locator.SimpleSeedProvider\n      parameters:\n          # seeds is actually a comma-delimited list of addresses.\n          # Ex: \"\u003cip1\u003e,\u003cip2\u003e,\u003cip3\u003e\"\n          - seeds: \"${seeds}\"\n\n# For workloads with more data than can fit in memory, Cassandra's\n# bottleneck will be reads that need to fetch data from\n# disk. \"concurrent_reads\" should be set to (16 * number_of_drives) in\n# order to allow the operations to enqueue low enough in the stack\n# that the OS and drives can reorder them. Same applies to\n# \"concurrent_counter_writes\", since counter writes read the current\n# values before incrementing and writing them back.\n#\n# On the other hand, since writes are almost never IO bound, the ideal\n# number of \"concurrent_writes\" is dependent on the number of cores in\n# your system; (8 * number_of_cores) is a good rule of thumb.\nconcurrent_reads: 32\nconcurrent_writes: 32\nconcurrent_counter_writes: 32\n\n# For materialized view writes, as there is a read involved, so this should\n# be limited by the less of concurrent reads or concurrent writes.\nconcurrent_materialized_view_writes: 32\n\n# Maximum memory to use for sstable chunk cache and buffer pooling.\n# 32MB of this are reserved for pooling buffers, the rest is used as an\n# cache that holds uncompressed sstable chunks.\n# Defaults to the smaller of 1/4 of heap or 512MB. This pool is allocated off-heap,\n# so is in addition to the memory allocated for heap. The cache also has on-heap\n# overhead which is roughly 128 bytes per chunk (i.e. 0.2% of the reserved size\n# if the default 64k chunk size is used).\n# Memory is only allocated when needed.\n# file_cache_size_in_mb: 512\n\n# Flag indicating whether to allocate on or off heap when the sstable buffer\n# pool is exhausted, that is when it has exceeded the maximum memory\n# file_cache_size_in_mb, beyond which it will not cache buffers but allocate on request.\n\n# buffer_pool_use_heap_if_exhausted: true\n\n# The strategy for optimizing disk read\n# Possible values are:\n# ssd (for solid state disks, the default)\n# spinning (for spinning disks)\n# disk_optimization_strategy: ssd\n\n# Total permitted memory to use for memtables. Cassandra will stop\n# accepting writes when the limit is exceeded until a flush completes,\n# and will trigger a flush based on memtable_cleanup_threshold\n# If omitted, Cassandra will set both to 1/4 the size of the heap.\n# memtable_heap_space_in_mb: 2048\n# memtable_offheap_space_in_mb: 2048\n\n# Ratio of occupied non-flushing memtable size to total permitted size\n# that will trigger a flush of the largest memtable. Larger mct will\n# mean larger flushes and hence less compaction, but also less concurrent\n# flush activity which can make it difficult to keep your disks fed\n# under heavy write load.\n#\n# memtable_cleanup_threshold defaults to 1 / (memtable_flush_writers + 1)\n# memtable_cleanup_threshold: 0.11\n\n# Specify the way Cassandra allocates and manages memtable memory.\n# Options are:\n#   heap_buffers:    on heap nio buffers\n#   offheap_buffers: off heap (direct) nio buffers\n#   offheap_objects: off heap objects\nmemtable_allocation_type: heap_buffers\n\n# Total space to use for commit logs on disk.\n#\n# If space gets above this value, Cassandra will flush every dirty CF\n# in the oldest segment and remove it.  So a small total commitlog space\n# will tend to cause more flush activity on less-active columnfamilies.\n#\n# The default value is the smaller of 8192, and 1/4 of the total space\n# of the commitlog volume.\n#\n# commitlog_total_space_in_mb: 8192\n\n# This sets the amount of memtable flush writer threads.  These will\n# be blocked by disk io, and each one will hold a memtable in memory\n# while blocked.\n#\n# memtable_flush_writers defaults to one per data_file_directory.\n#\n# If your data directories are backed by SSD, you can increase this, but\n# avoid having memtable_flush_writers * data_file_directories \u003e number of cores\n#memtable_flush_writers: 1\n\n# A fixed memory pool size in MB for for SSTable index summaries. If left\n# empty, this will default to 5% of the heap size. If the memory usage of\n# all index summaries exceeds this limit, SSTables with low read rates will\n# shrink their index summaries in order to meet this limit.  However, this\n# is a best-effort process. In extreme conditions Cassandra may need to use\n# more than this amount of memory.\nindex_summary_capacity_in_mb:\n\n# How frequently index summaries should be resampled.  This is done\n# periodically to redistribute memory from the fixed-size pool to sstables\n# proportional their recent read rates.  Setting to -1 will disable this\n# process, leaving existing index summaries at their current sampling level.\nindex_summary_resize_interval_in_minutes: 60\n\n# Whether to, when doing sequential writing, fsync() at intervals in\n# order to force the operating system to flush the dirty\n# buffers. Enable this to avoid sudden dirty buffer flushing from\n# impacting read latencies. Almost always a good idea on SSDs; not\n# necessarily on platters.\ntrickle_fsync: false\ntrickle_fsync_interval_in_kb: 10240\n\n# TCP port, for commands and data\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\nstorage_port: 7000\n\n# SSL port, for encrypted communication.  Unused unless enabled in\n# encryption_options\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\nssl_storage_port: 7001\n\n# Address or interface to bind to and tell other Cassandra nodes to connect to.\n# You _must_ change this if you want multiple nodes to be able to communicate!\n#\n# Set listen_address OR listen_interface, not both. Interfaces must correspond\n# to a single address, IP aliasing is not supported.\n#\n# Leaving it blank leaves it up to InetAddress.getLocalHost(). This\n# will always do the Right Thing _if_ the node is properly configured\n# (hostname, name resolution, etc), and the Right Thing is to use the\n# address associated with the hostname (it might not be).\n#\n# Setting listen_address to 0.0.0.0 is always wrong.\n#\n# If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\n# you can specify which should be chosen using listen_interface_prefer_ipv6. If false the first ipv4\n# address will be used. If true the first ipv6 address will be used. Defaults to false preferring\n# ipv4. If there is only one address it will be selected regardless of ipv4/ipv6.\nlisten_address: ${server_ip}\n# listen_interface: eth0\n# listen_interface_prefer_ipv6: false\n\n# Address to broadcast to other Cassandra nodes\n# Leaving this blank will set it to the same value as listen_address\n# broadcast_address: 1.2.3.4\n\n# When using multiple physical network interfaces, set this\n# to true to listen on broadcast_address in addition to\n# the listen_address, allowing nodes to communicate in both\n# interfaces.\n# Ignore this property if the network configuration automatically\n# routes  between the public and private networks such as EC2.\n# listen_on_broadcast_address: false\n\n# Internode authentication backend, implementing IInternodeAuthenticator;\n# used to allow/disallow connections from peer nodes.\n# internode_authenticator: org.apache.cassandra.auth.AllowAllInternodeAuthenticator\n\n# Whether to start the native transport server.\n# Please note that the address on which the native transport is bound is the\n# same as the rpc_address. The port however is different and specified below.\nstart_native_transport: true\n# port for the CQL native transport to listen for clients on\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\nnative_transport_port: 9042\n# Enabling native transport encryption in client_encryption_options allows you to either use\n# encryption for the standard port or to use a dedicated, additional port along with the unencrypted\n# standard native_transport_port.\n# Enabling client encryption and keeping native_transport_port_ssl disabled will use encryption\n# for native_transport_port. Setting native_transport_port_ssl to a different value\n# from native_transport_port will use encryption for native_transport_port_ssl while\n# keeping native_transport_port unencrypted.\n# native_transport_port_ssl: 9142\n# The maximum threads for handling requests when the native transport is used.\n# This is similar to rpc_max_threads though the default differs slightly (and\n# there is no native_transport_min_threads, idle threads will always be stopped\n# after 30 seconds).\n# native_transport_max_threads: 128\n#\n# The maximum size of allowed frame. Frame (requests) larger than this will\n# be rejected as invalid. The default is 256MB.\n# native_transport_max_frame_size_in_mb: 256\n\n# The maximum number of concurrent client connections.\n# The default is -1, which means unlimited.\n# native_transport_max_concurrent_connections: -1\n\n# The maximum number of concurrent client connections per source ip.\n# The default is -1, which means unlimited.\n# native_transport_max_concurrent_connections_per_ip: -1\n\n# Whether to start the thrift rpc server.\nstart_rpc: true\n\n# The address or interface to bind the Thrift RPC service and native transport\n# server to.\n#\n# Set rpc_address OR rpc_interface, not both. Interfaces must correspond\n# to a single address, IP aliasing is not supported.\n#\n# Leaving rpc_address blank has the same effect as on listen_address\n# (i.e. it will be based on the configured hostname of the node).\n#\n# Note that unlike listen_address, you can specify 0.0.0.0, but you must also\n# set broadcast_rpc_address to a value other than 0.0.0.0.\n#\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\n#\n# If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\n# you can specify which should be chosen using rpc_interface_prefer_ipv6. If false the first ipv4\n# address will be used. If true the first ipv6 address will be used. Defaults to false preferring\n# ipv4. If there is only one address it will be selected regardless of ipv4/ipv6.\nrpc_address: ${server_ip}\n# rpc_interface: eth1\n# rpc_interface_prefer_ipv6: false\n\n# port for Thrift to listen for clients on\nrpc_port: 9160\n\n# RPC address to broadcast to drivers and other Cassandra nodes. This cannot\n# be set to 0.0.0.0. If left blank, this will be set to the value of\n# rpc_address. If rpc_address is set to 0.0.0.0, broadcast_rpc_address must\n# be set.\n#broadcast_rpc_address: \n\n# enable or disable keepalive on rpc/native connections\nrpc_keepalive: true\n\n# Cassandra provides two out-of-the-box options for the RPC Server:\n#\n# sync  -\u003e One thread per thrift connection. For a very large number of clients, memory\n#          will be your limiting factor. On a 64 bit JVM, 180KB is the minimum stack size\n#          per thread, and that will correspond to your use of virtual memory (but physical memory\n#          may be limited depending on use of stack space).\n#\n# hsha  -\u003e Stands for \"half synchronous, half asynchronous.\" All thrift clients are handled\n#          asynchronously using a small number of threads that does not vary with the amount\n#          of thrift clients (and thus scales well to many clients). The rpc requests are still\n#          synchronous (one thread per active request). If hsha is selected then it is essential\n#          that rpc_max_threads is changed from the default value of unlimited.\n#\n# The default is sync because on Windows hsha is about 30% slower.  On Linux,\n# sync/hsha performance is about the same, with hsha of course using less memory.\n#\n# Alternatively,  can provide your own RPC server by providing the fully-qualified class name\n# of an o.a.c.t.TServerFactory that can create an instance of it.\nrpc_server_type: sync\n\n# Uncomment rpc_min|max_thread to set request pool size limits.\n#\n# Regardless of your choice of RPC server (see above), the number of maximum requests in the\n# RPC thread pool dictates how many concurrent requests are possible (but if you are using the sync\n# RPC server, it also dictates the number of clients that can be connected at all).\n#\n# The default is unlimited and thus provides no protection against clients overwhelming the server. You are\n# encouraged to set a maximum that makes sense for you in production, but do keep in mind that\n# rpc_max_threads represents the maximum number of client requests this server may execute concurrently.\n#\n# rpc_min_threads: 16\n# rpc_max_threads: 2048\n\n# uncomment to set socket buffer sizes on rpc connections\n# rpc_send_buff_size_in_bytes:\n# rpc_recv_buff_size_in_bytes:\n\n# Uncomment to set socket buffer size for internode communication\n# Note that when setting this, the buffer size is limited by net.core.wmem_max\n# and when not setting it it is defined by net.ipv4.tcp_wmem\n# See:\n# /proc/sys/net/core/wmem_max\n# /proc/sys/net/core/rmem_max\n# /proc/sys/net/ipv4/tcp_wmem\n# /proc/sys/net/ipv4/tcp_wmem\n# and: man tcp\n# internode_send_buff_size_in_bytes:\n# internode_recv_buff_size_in_bytes:\n\n# Frame size for thrift (maximum message length).\nthrift_framed_transport_size_in_mb: 15\n\n# Set to true to have Cassandra create a hard link to each sstable\n# flushed or streamed locally in a backups/ subdirectory of the\n# keyspace data.  Removing these links is the operator's\n# responsibility.\nincremental_backups: false\n\n# Whether or not to take a snapshot before each compaction.  Be\n# careful using this option, since Cassandra won't clean up the\n# snapshots for you.  Mostly useful if you're paranoid when there\n# is a data format change.\nsnapshot_before_compaction: false\n\n# Whether or not a snapshot is taken of the data before keyspace truncation\n# or dropping of column families. The STRONGLY advised default of true \n# should be used to provide data safety. If you set this flag to false, you will\n# lose data on truncation or drop.\nauto_snapshot: true\n\n# Granularity of the collation index of rows within a partition.\n# Increase if your rows are large, or if you have a very large\n# number of rows per partition.  The competing goals are these:\n#   1) a smaller granularity means more index entries are generated\n#      and looking up rows withing the partition by collation column\n#      is faster\n#   2) but, Cassandra will keep the collation index in memory for hot\n#      rows (as part of the key cache), so a larger granularity means\n#      you can cache more hot rows\ncolumn_index_size_in_kb: 64\n# Per sstable indexed key cache entries (the collation index in memory\n# mentioned above) exceeding this size will not be held on heap.\n# This means that only partition information is held on heap and the\n# index entries are read from disk.\n#\n# Note that this size refers to the size of the\n# serialized index information and not the size of the partition.\ncolumn_index_cache_size_in_kb: 2\n\n# Number of simultaneous compactions to allow, NOT including\n# validation \"compactions\" for anti-entropy repair.  Simultaneous\n# compactions can help preserve read performance in a mixed read/write\n# workload, by mitigating the tendency of small sstables to accumulate\n# during a single long running compactions. The default is usually\n# fine and if you experience problems with compaction running too\n# slowly or too fast, you should look at\n# compaction_throughput_mb_per_sec first.\n#\n# concurrent_compactors defaults to the smaller of (number of disks,\n# number of cores), with a minimum of 2 and a maximum of 8.\n# \n# If your data directories are backed by SSD, you should increase this\n# to the number of cores.\n#concurrent_compactors: 1\n\n# Throttles compaction to the given total throughput across the entire\n# system. The faster you insert data, the faster you need to compact in\n# order to keep the sstable count down, but in general, setting this to\n# 16 to 32 times the rate you are inserting data is more than sufficient.\n# Setting this to 0 disables throttling. Note that this account for all types\n# of compaction, including validation compaction.\ncompaction_throughput_mb_per_sec: 16\n\n# When compacting, the replacement sstable(s) can be opened before they\n# are completely written, and used in place of the prior sstables for\n# any range that has been written. This helps to smoothly transfer reads \n# between the sstables, reducing page cache churn and keeping hot rows hot\nsstable_preemptive_open_interval_in_mb: 50\n\n# Throttles all outbound streaming file transfers on this node to the\n# given total throughput in Mbps. This is necessary because Cassandra does\n# mostly sequential IO when streaming data during bootstrap or repair, which\n# can lead to saturating the network connection and degrading rpc performance.\n# When unset, the default is 200 Mbps or 25 MB/s.\n# stream_throughput_outbound_megabits_per_sec: 200\n\n# Throttles all streaming file transfer between the datacenters,\n# this setting allows users to throttle inter dc stream throughput in addition\n# to throttling all network stream traffic as configured with\n# stream_throughput_outbound_megabits_per_sec\n# When unset, the default is 200 Mbps or 25 MB/s\n# inter_dc_stream_throughput_outbound_megabits_per_sec: 200\n\n# How long the coordinator should wait for read operations to complete\nread_request_timeout_in_ms: 5000\n# How long the coordinator should wait for seq or index scans to complete\nrange_request_timeout_in_ms: 10000\n# How long the coordinator should wait for writes to complete\nwrite_request_timeout_in_ms: 2000\n# How long the coordinator should wait for counter writes to complete\ncounter_write_request_timeout_in_ms: 5000\n# How long a coordinator should continue to retry a CAS operation\n# that contends with other proposals for the same row\ncas_contention_timeout_in_ms: 1000\n# How long the coordinator should wait for truncates to complete\n# (This can be much longer, because unless auto_snapshot is disabled\n# we need to flush first so we can snapshot before removing the data.)\ntruncate_request_timeout_in_ms: 60000\n# The default timeout for other, miscellaneous operations\nrequest_timeout_in_ms: 10000\n\n# Enable operation timeout information exchange between nodes to accurately\n# measure request timeouts.  If disabled, replicas will assume that requests\n# were forwarded to them instantly by the coordinator, which means that\n# under overload conditions we will waste that much extra time processing \n# already-timed-out requests.\n#\n# Warning: before enabling this property make sure to ntp is installed\n# and the times are synchronized between the nodes.\ncross_node_timeout: false\n\n# Set socket timeout for streaming operation.\n# The stream session is failed if no data is received by any of the\n# participants within that period.\n# Default value is 3600000, which means streams timeout after an hour.\n# streaming_socket_timeout_in_ms: 3600000\n\n# phi value that must be reached for a host to be marked down.\n# most users should never need to adjust this.\n# phi_convict_threshold: 8\n\n# endpoint_snitch -- Set this to a class that implements\n# IEndpointSnitch.  The snitch has two functions:\n# - it teaches Cassandra enough about your network topology to route\n#   requests efficiently\n# - it allows Cassandra to spread replicas around your cluster to avoid\n#   correlated failures. It does this by grouping machines into\n#   \"datacenters\" and \"racks.\"  Cassandra will do its best not to have\n#   more than one replica on the same \"rack\" (which may not actually\n#   be a physical location)\n#\n# IF YOU CHANGE THE SNITCH AFTER DATA IS INSERTED INTO THE CLUSTER,\n# YOU MUST RUN A FULL REPAIR, SINCE THE SNITCH AFFECTS WHERE REPLICAS\n# ARE PLACED.\n#\n# IF THE RACK A REPLICA IS PLACED IN CHANGES AFTER THE REPLICA HAS BEEN\n# ADDED TO A RING, THE NODE MUST BE DECOMMISSIONED AND REBOOTSTRAPPED.\n#\n# Out of the box, Cassandra provides\n#  - SimpleSnitch:\n#    Treats Strategy order as proximity. This can improve cache\n#    locality when disabling read repair.  Only appropriate for\n#    single-datacenter deployments.\n#  - GossipingPropertyFileSnitch\n#    This should be your go-to snitch for production use.  The rack\n#    and datacenter for the local node are defined in\n#    cassandra-rackdc.properties and propagated to other nodes via\n#    gossip.  If cassandra-topology.properties exists, it is used as a\n#    fallback, allowing migration from the PropertyFileSnitch.\n#  - PropertyFileSnitch:\n#    Proximity is determined by rack and data center, which are\n#    explicitly configured in cassandra-topology.properties.\n#  - Ec2Snitch:\n#    Appropriate for EC2 deployments in a single Region. Loads Region\n#    and Availability Zone information from the EC2 API. The Region is\n#    treated as the datacenter, and the Availability Zone as the rack.\n#    Only private IPs are used, so this will not work across multiple\n#    Regions.\n#  - Ec2MultiRegionSnitch:\n#    Uses public IPs as broadcast_address to allow cross-region\n#    connectivity.  (Thus, you should set seed addresses to the public\n#    IP as well.) You will need to open the storage_port or\n#    ssl_storage_port on the public IP firewall.  (For intra-Region\n#    traffic, Cassandra will switch to the private IP after\n#    establishing a connection.)\n#  - RackInferringSnitch:\n#    Proximity is determined by rack and data center, which are\n#    assumed to correspond to the 3rd and 2nd octet of each node's IP\n#    address, respectively.  Unless this happens to match your\n#    deployment conventions, this is best used as an example of\n#    writing a custom Snitch class and is provided in that spirit.\n#\n# You can use a custom Snitch by setting this to the full class name\n# of the snitch, which will be assumed to be on your classpath.\nendpoint_snitch: GossipingPropertyFileSnitch\n\n# controls how often to perform the more expensive part of host score\n# calculation\ndynamic_snitch_update_interval_in_ms: 100 \n# controls how often to reset all host scores, allowing a bad host to\n# possibly recover\ndynamic_snitch_reset_interval_in_ms: 600000\n# if set greater than zero and read_repair_chance is \u003c 1.0, this will allow\n# 'pinning' of replicas to hosts in order to increase cache capacity.\n# The badness threshold will control how much worse the pinned host has to be\n# before the dynamic snitch will prefer other replicas over it.  This is\n# expressed as a double which represents a percentage.  Thus, a value of\n# 0.2 means Cassandra would continue to prefer the static snitch values\n# until the pinned host was 20% worse than the fastest.\ndynamic_snitch_badness_threshold: 0.1\n\n# request_scheduler -- Set this to a class that implements\n# RequestScheduler, which will schedule incoming client requests\n# according to the specific policy. This is useful for multi-tenancy\n# with a single Cassandra cluster.\n# NOTE: This is specifically for requests from the client and does\n# not affect inter node communication.\n# org.apache.cassandra.scheduler.NoScheduler - No scheduling takes place\n# org.apache.cassandra.scheduler.RoundRobinScheduler - Round robin of\n# client requests to a node with a separate queue for each\n# request_scheduler_id. The scheduler is further customized by\n# request_scheduler_options as described below.\nrequest_scheduler: org.apache.cassandra.scheduler.NoScheduler\n\n# Scheduler Options vary based on the type of scheduler\n# NoScheduler - Has no options\n# RoundRobin\n#  - throttle_limit -- The throttle_limit is the number of in-flight\n#                      requests per client.  Requests beyond \n#                      that limit are queued up until\n#                      running requests can complete.\n#                      The value of 80 here is twice the number of\n#                      concurrent_reads + concurrent_writes.\n#  - default_weight -- default_weight is optional and allows for\n#                      overriding the default which is 1.\n#  - weights -- Weights are optional and will default to 1 or the\n#               overridden default_weight. The weight translates into how\n#               many requests are handled during each turn of the\n#               RoundRobin, based on the scheduler id.\n#\n# request_scheduler_options:\n#    throttle_limit: 80\n#    default_weight: 5\n#    weights:\n#      Keyspace1: 1\n#      Keyspace2: 5\n\n# request_scheduler_id -- An identifier based on which to perform\n# the request scheduling. Currently the only valid option is keyspace.\n# request_scheduler_id: keyspace\n\n# Enable or disable inter-node encryption\n# JVM defaults for supported SSL socket protocols and cipher suites can\n# be replaced using custom encryption options. This is not recommended\n# unless you have policies in place that dictate certain settings, or\n# need to disable vulnerable ciphers or protocols in case the JVM cannot\n# be updated.\n# FIPS compliant settings can be configured at JVM level and should not\n# involve changing encryption settings here:\n# https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/FIPS.html\n# NOTE: No custom encryption options are enabled at the moment\n# The available internode options are : all, none, dc, rack\n#\n# If set to dc cassandra will encrypt the traffic between the DCs\n# If set to rack cassandra will encrypt the traffic between the racks\n#\n# The passwords used in these options must match the passwords used when generating\n# the keystore and truststore.  For instructions on generating these files, see:\n# http://download.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html#CreateKeystore\n#\nserver_encryption_options:\n    internode_encryption: none\n    keystore: conf/.keystore\n    keystore_password: cassandra\n    truststore: conf/.truststore\n    truststore_password: cassandra\n    # More advanced defaults below:\n    # protocol: TLS\n    # algorithm: SunX509\n    # store_type: JKS\n    # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n    # require_client_auth: false\n    # require_endpoint_verification: false\n\n# enable or disable client/server encryption.\nclient_encryption_options:\n    enabled: false\n    # If enabled and optional is set to true encrypted and unencrypted connections are handled.\n    optional: false\n    keystore: conf/.keystore\n    keystore_password: cassandra\n    # require_client_auth: false\n    # Set trustore and truststore_password if require_client_auth is true\n    # truststore: conf/.truststore\n    # truststore_password: cassandra\n    # More advanced defaults below:\n    # protocol: TLS\n    # algorithm: SunX509\n    # store_type: JKS\n    # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n\n# internode_compression controls whether traffic between nodes is\n# compressed.\n# can be:  all  - all traffic is compressed\n#          dc   - traffic between different datacenters is compressed\n#          none - nothing is compressed.\ninternode_compression: dc\n\n# Enable or disable tcp_nodelay for inter-dc communication.\n# Disabling it will result in larger (but fewer) network packets being sent,\n# reducing overhead from the TCP protocol itself, at the cost of increasing\n# latency if you block for cross-datacenter responses.\ninter_dc_tcp_nodelay: false\n\n# TTL for different trace types used during logging of the repair process.\ntracetype_query_ttl: 86400\ntracetype_repair_ttl: 604800\n\n# UDFs (user defined functions) are disabled by default.\n# As of Cassandra 3.0 there is a sandbox in place that should prevent execution of evil code.\nenable_user_defined_functions: false\n\n# Enables scripted UDFs (JavaScript UDFs).\n# Java UDFs are always enabled, if enable_user_defined_functions is true.\n# Enable this option to be able to use UDFs with \"language javascript\" or any custom JSR-223 provider.\n# This option has no effect, if enable_user_defined_functions is false.\nenable_scripted_user_defined_functions: false\n\n# The default Windows kernel timer and scheduling resolution is 15.6ms for power conservation.\n# Lowering this value on Windows can provide much tighter latency and better throughput, however\n# some virtualized environments may see a negative performance impact from changing this setting\n# below their system default. The sysinternals 'clockres' tool can confirm your system's default\n# setting.\nwindows_timer_interval: 1\n\n\n# Enables encrypting data at-rest (on disk). Different key providers can be plugged in, but the default reads from\n# a JCE-style keystore. A single keystore can hold multiple keys, but the one referenced by\n# the \"key_alias\" is the only key that will be used for encrypt opertaions; previously used keys\n# can still (and should!) be in the keystore and will be used on decrypt operations\n# (to handle the case of key rotation).\n#\n# It is strongly recommended to download and install Java Cryptography Extension (JCE)\n# Unlimited Strength Jurisdiction Policy Files for your version of the JDK.\n# (current link: http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html)\n#\n# Currently, only the following file types are supported for transparent data encryption, although\n# more are coming in future cassandra releases: commitlog, hints\ntransparent_data_encryption_options:\n    enabled: false\n    chunk_length_kb: 64\n    cipher: AES/CBC/PKCS5Padding\n    key_alias: testing:1\n    # CBC IV length for AES needs to be 16 bytes (which is also the default size)\n    # iv_length: 16\n    key_provider: \n      - class_name: org.apache.cassandra.security.JKSKeyProvider\n        parameters: \n          - keystore: conf/.keystore\n            keystore_password: cassandra\n            store_type: JCEKS\n            key_password: cassandra\n\n\n#####################\n# SAFETY THRESHOLDS #\n#####################\n\n# When executing a scan, within or across a partition, we need to keep the\n# tombstones seen in memory so we can return them to the coordinator, which\n# will use them to make sure other replicas also know about the deleted rows.\n# With workloads that generate a lot of tombstones, this can cause performance\n# problems and even exaust the server heap.\n# (http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets)\n# Adjust the thresholds here if you understand the dangers and want to\n# scan more tombstones anyway.  These thresholds may also be adjusted at runtime\n# using the StorageService mbean.\ntombstone_warn_threshold: 1000\ntombstone_failure_threshold: 100000\n\n# Log WARN on any batch size exceeding this value. 5kb per batch by default.\n# Caution should be taken on increasing the size of this threshold as it can lead to node instability.\nbatch_size_warn_threshold_in_kb: 5\n\n# Fail any batch exceeding this value. 50kb (10x warn threshold) by default.\nbatch_size_fail_threshold_in_kb: 50\n\n# Log WARN on any batches not of type LOGGED than span across more partitions than this limit\nunlogged_batch_across_partitions_warn_threshold: 10\n\n# Log a warning when compacting partitions larger than this value\ncompaction_large_partition_warning_threshold_mb: 100\n\n# GC Pauses greater than gc_warn_threshold_in_ms will be logged at WARN level\n# Adjust the threshold based on your application throughput requirement\n# By default, Cassandra logs GC Pauses greater than 200 ms at INFO level\ngc_warn_threshold_in_ms: 1000\n\nauto_bootstrap: false\n",
                            "vars.%": "3",
                            "vars.cluster_name": "opsschool",
                            "vars.seeds": "172.31.48.41,172.31.77.64,172.31.6.131",
                            "vars.server_ip": "172.31.48.41"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.template"
                },
                "data.template_file.cassandra_yaml.1": {
                    "type": "template_file",
                    "depends_on": [
                        "aws_instance.cassandra.*"
                    ],
                    "primary": {
                        "id": "5cc5d58c6fa668e449e37fff74cfb699794c8973d4e44c98a11c88f78871f632",
                        "attributes": {
                            "id": "5cc5d58c6fa668e449e37fff74cfb699794c8973d4e44c98a11c88f78871f632",
                            "rendered": "# Cassandra storage config YAML \n\n# NOTE:\n#   See http://wiki.apache.org/cassandra/StorageConfiguration for\n#   full explanations of configuration directives\n# /NOTE\n\n# The name of the cluster. This is mainly used to prevent machines in\n# one logical cluster from joining another.\ncluster_name: 'opsschool'\n\n# This defines the number of tokens randomly assigned to this node on the ring\n# The more tokens, relative to other nodes, the larger the proportion of data\n# that this node will store. You probably want all nodes to have the same number\n# of tokens assuming they have equal hardware capability.\n#\n# If you leave this unspecified, Cassandra will use the default of 1 token for legacy compatibility,\n# and will use the initial_token as described below.\n#\n# Specifying initial_token will override this setting on the node's initial start,\n# on subsequent starts, this setting will apply even if initial token is set.\n#\n# If you already have a cluster with 1 token per node, and wish to migrate to \n# multiple tokens per node, see http://wiki.apache.org/cassandra/Operations\nnum_tokens: 256\n\n# Triggers automatic allocation of num_tokens tokens for this node. The allocation\n# algorithm attempts to choose tokens in a way that optimizes replicated load over\n# the nodes in the datacenter for the replication strategy used by the specified\n# keyspace.\n#\n# The load assigned to each node will be close to proportional to its number of\n# vnodes.\n#\n# Only supported with the Murmur3Partitioner.\n# allocate_tokens_for_keyspace: KEYSPACE\n\n# initial_token allows you to specify tokens manually.  While you can use # it with\n# vnodes (num_tokens \u003e 1, above) -- in which case you should provide a \n# comma-separated list -- it's primarily used when adding nodes # to legacy clusters \n# that do not have vnodes enabled.\n# initial_token:\n\n# See http://wiki.apache.org/cassandra/HintedHandoff\n# May either be \"true\" or \"false\" to enable globally\nhinted_handoff_enabled: true\n# When hinted_handoff_enabled is true, a black list of data centers that will not\n# perform hinted handoff\n#hinted_handoff_disabled_datacenters:\n#    - DC1\n#    - DC2\n# this defines the maximum amount of time a dead host will have hints\n# generated.  After it has been dead this long, new hints for it will not be\n# created until it has been seen alive and gone down again.\nmax_hint_window_in_ms: 10800000 # 3 hours\n\n# Maximum throttle in KBs per second, per delivery thread.  This will be\n# reduced proportionally to the number of nodes in the cluster.  (If there\n# are two nodes in the cluster, each delivery thread will use the maximum\n# rate; if there are three, each will throttle to half of the maximum,\n# since we expect two nodes to be delivering hints simultaneously.)\nhinted_handoff_throttle_in_kb: 1024\n\n# Number of threads with which to deliver hints;\n# Consider increasing this number when you have multi-dc deployments, since\n# cross-dc handoff tends to be slower\nmax_hints_delivery_threads: 2\n\n# Directory where Cassandra should store hints.\n# If not set, the default directory is $CASSANDRA_HOME/data/hints.\n# hints_directory: /var/lib/cassandra/hints\n\n# How often hints should be flushed from the internal buffers to disk.\n# Will *not* trigger fsync.\nhints_flush_period_in_ms: 10000\n\n# Maximum size for a single hints file, in megabytes.\nmax_hints_file_size_in_mb: 128\n\n# Compression to apply to the hint files. If omitted, hints files\n# will be written uncompressed. LZ4, Snappy, and Deflate compressors\n# are supported.\n#hints_compression:\n#   - class_name: LZ4Compressor\n#     parameters:\n#         -\n\n# Maximum throttle in KBs per second, total. This will be\n# reduced proportionally to the number of nodes in the cluster.\nbatchlog_replay_throttle_in_kb: 1024\n\n# Authentication backend, implementing IAuthenticator; used to identify users\n# Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthenticator,\n# PasswordAuthenticator}.\n#\n# - AllowAllAuthenticator performs no checks - set it to disable authentication.\n# - PasswordAuthenticator relies on username/password pairs to authenticate\n#   users. It keeps usernames and hashed passwords in system_auth.credentials table.\n#   Please increase system_auth keyspace replication factor if you use this authenticator.\n#   If using PasswordAuthenticator, CassandraRoleManager must also be used (see below)\nauthenticator: AllowAllAuthenticator\n\n# Authorization backend, implementing IAuthorizer; used to limit access/provide permissions\n# Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthorizer,\n# CassandraAuthorizer}.\n#\n# - AllowAllAuthorizer allows any action to any user - set it to disable authorization.\n# - CassandraAuthorizer stores permissions in system_auth.permissions table. Please\n#   increase system_auth keyspace replication factor if you use this authorizer.\nauthorizer: AllowAllAuthorizer\n\n# Part of the Authentication \u0026 Authorization backend, implementing IRoleManager; used\n# to maintain grants and memberships between roles.\n# Out of the box, Cassandra provides org.apache.cassandra.auth.CassandraRoleManager,\n# which stores role information in the system_auth keyspace. Most functions of the\n# IRoleManager require an authenticated login, so unless the configured IAuthenticator\n# actually implements authentication, most of this functionality will be unavailable.\n#\n# - CassandraRoleManager stores role data in the system_auth keyspace. Please\n#   increase system_auth keyspace replication factor if you use this role manager.\nrole_manager: CassandraRoleManager\n\n# Validity period for roles cache (fetching granted roles can be an expensive\n# operation depending on the role manager, CassandraRoleManager is one example)\n# Granted roles are cached for authenticated sessions in AuthenticatedUser and\n# after the period specified here, become eligible for (async) reload.\n# Defaults to 2000, set to 0 to disable caching entirely.\n# Will be disabled automatically for AllowAllAuthenticator.\nroles_validity_in_ms: 2000\n\n# Refresh interval for roles cache (if enabled).\n# After this interval, cache entries become eligible for refresh. Upon next\n# access, an async reload is scheduled and the old value returned until it\n# completes. If roles_validity_in_ms is non-zero, then this must be\n# also.\n# Defaults to the same value as roles_validity_in_ms.\n# roles_update_interval_in_ms: 2000\n\n# Validity period for permissions cache (fetching permissions can be an\n# expensive operation depending on the authorizer, CassandraAuthorizer is\n# one example). Defaults to 2000, set to 0 to disable.\n# Will be disabled automatically for AllowAllAuthorizer.\npermissions_validity_in_ms: 2000\n\n# Refresh interval for permissions cache (if enabled).\n# After this interval, cache entries become eligible for refresh. Upon next\n# access, an async reload is scheduled and the old value returned until it\n# completes. If permissions_validity_in_ms is non-zero, then this must be\n# also.\n# Defaults to the same value as permissions_validity_in_ms.\n# permissions_update_interval_in_ms: 2000\n\n# Validity period for credentials cache. This cache is tightly coupled to\n# the provided PasswordAuthenticator implementation of IAuthenticator. If\n# another IAuthenticator implementation is configured, this cache will not\n# be automatically used and so the following settings will have no effect.\n# Please note, credentials are cached in their encrypted form, so while\n# activating this cache may reduce the number of queries made to the\n# underlying table, it may not  bring a significant reduction in the\n# latency of individual authentication attempts.\n# Defaults to 2000, set to 0 to disable credentials caching.\ncredentials_validity_in_ms: 2000\n\n# Refresh interval for credentials cache (if enabled).\n# After this interval, cache entries become eligible for refresh. Upon next\n# access, an async reload is scheduled and the old value returned until it\n# completes. If credentials_validity_in_ms is non-zero, then this must be\n# also.\n# Defaults to the same value as credentials_validity_in_ms.\n# credentials_update_interval_in_ms: 2000\n\n# The partitioner is responsible for distributing groups of rows (by\n# partition key) across nodes in the cluster.  You should leave this\n# alone for new clusters.  The partitioner can NOT be changed without\n# reloading all data, so when upgrading you should set this to the\n# same partitioner you were already using.\n#\n# Besides Murmur3Partitioner, partitioners included for backwards\n# compatibility include RandomPartitioner, ByteOrderedPartitioner, and\n# OrderPreservingPartitioner.\n#\npartitioner: org.apache.cassandra.dht.Murmur3Partitioner\n\n# Directories where Cassandra should store data on disk.  Cassandra\n# will spread data evenly across them, subject to the granularity of\n# the configured compaction strategy.\n# If not set, the default directory is $CASSANDRA_HOME/data/data.\ndata_file_directories:\n    - /var/lib/cassandra/data\n\n# commit log.  when running on magnetic HDD, this should be a\n# separate spindle than the data directories.\n# If not set, the default directory is $CASSANDRA_HOME/data/commitlog.\ncommitlog_directory: /var/lib/cassandra/commitlog\n\n# policy for data disk failures:\n# die: shut down gossip and client transports and kill the JVM for any fs errors or\n#      single-sstable errors, so the node can be replaced.\n# stop_paranoid: shut down gossip and client transports even for single-sstable errors,\n#                kill the JVM for errors during startup.\n# stop: shut down gossip and client transports, leaving the node effectively dead, but\n#       can still be inspected via JMX, kill the JVM for errors during startup.\n# best_effort: stop using the failed disk and respond to requests based on\n#              remaining available sstables.  This means you WILL see obsolete\n#              data at CL.ONE!\n# ignore: ignore fatal errors and let requests fail, as in pre-1.2 Cassandra\ndisk_failure_policy: stop\n\n# policy for commit disk failures:\n# die: shut down gossip and Thrift and kill the JVM, so the node can be replaced.\n# stop: shut down gossip and Thrift, leaving the node effectively dead, but\n#       can still be inspected via JMX.\n# stop_commit: shutdown the commit log, letting writes collect but\n#              continuing to service reads, as in pre-2.0.5 Cassandra\n# ignore: ignore fatal errors and let the batches fail\ncommit_failure_policy: stop\n\n# Maximum size of the native protocol prepared statement cache\n#\n# Valid values are either \"auto\" (omitting the value) or a value greater 0.\n#\n# Note that specifying a too large value will result in long running GCs and possbily\n# out-of-memory errors. Keep the value at a small fraction of the heap.\n#\n# If you constantly see \"prepared statements discarded in the last minute because\n# cache limit reached\" messages, the first step is to investigate the root cause\n# of these messages and check whether prepared statements are used correctly -\n# i.e. use bind markers for variable parts.\n#\n# Do only change the default value, if you really have more prepared statements than\n# fit in the cache. In most cases it is not neccessary to change this value.\n# Constantly re-preparing statements is a performance penalty.\n#\n# Default value (\"auto\") is 1/256th of the heap or 10MB, whichever is greater\nprepared_statements_cache_size_mb:\n\n# Maximum size of the Thrift prepared statement cache\n#\n# If you do not use Thrift at all, it is safe to leave this value at \"auto\".\n#\n# See description of 'prepared_statements_cache_size_mb' above for more information.\n#\n# Default value (\"auto\") is 1/256th of the heap or 10MB, whichever is greater\nthrift_prepared_statements_cache_size_mb:\n\n# Maximum size of the key cache in memory.\n#\n# Each key cache hit saves 1 seek and each row cache hit saves 2 seeks at the\n# minimum, sometimes more. The key cache is fairly tiny for the amount of\n# time it saves, so it's worthwhile to use it at large numbers.\n# The row cache saves even more time, but must contain the entire row,\n# so it is extremely space-intensive. It's best to only use the\n# row cache if you have hot rows or static rows.\n#\n# NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.\n#\n# Default value is empty to make it \"auto\" (min(5% of Heap (in MB), 100MB)). Set to 0 to disable key cache.\nkey_cache_size_in_mb:\n\n# Duration in seconds after which Cassandra should\n# save the key cache. Caches are saved to saved_caches_directory as\n# specified in this configuration file.\n#\n# Saved caches greatly improve cold-start speeds, and is relatively cheap in\n# terms of I/O for the key cache. Row cache saving is much more expensive and\n# has limited use.\n#\n# Default is 14400 or 4 hours.\nkey_cache_save_period: 14400\n\n# Number of keys from the key cache to save\n# Disabled by default, meaning all keys are going to be saved\n# key_cache_keys_to_save: 100\n\n# Row cache implementation class name.\n# Available implementations:\n#   org.apache.cassandra.cache.OHCProvider                Fully off-heap row cache implementation (default).\n#   org.apache.cassandra.cache.SerializingCacheProvider   This is the row cache implementation availabile\n#                                                         in previous releases of Cassandra.\n# row_cache_class_name: org.apache.cassandra.cache.OHCProvider\n\n# Maximum size of the row cache in memory.\n# Please note that OHC cache implementation requires some additional off-heap memory to manage\n# the map structures and some in-flight memory during operations before/after cache entries can be\n# accounted against the cache capacity. This overhead is usually small compared to the whole capacity.\n# Do not specify more memory that the system can afford in the worst usual situation and leave some\n# headroom for OS block level cache. Do never allow your system to swap.\n#\n# Default value is 0, to disable row caching.\nrow_cache_size_in_mb: 0\n\n# Duration in seconds after which Cassandra should save the row cache.\n# Caches are saved to saved_caches_directory as specified in this configuration file.\n#\n# Saved caches greatly improve cold-start speeds, and is relatively cheap in\n# terms of I/O for the key cache. Row cache saving is much more expensive and\n# has limited use.\n#\n# Default is 0 to disable saving the row cache.\nrow_cache_save_period: 0\n\n# Number of keys from the row cache to save.\n# Specify 0 (which is the default), meaning all keys are going to be saved\n# row_cache_keys_to_save: 100\n\n# Maximum size of the counter cache in memory.\n#\n# Counter cache helps to reduce counter locks' contention for hot counter cells.\n# In case of RF = 1 a counter cache hit will cause Cassandra to skip the read before\n# write entirely. With RF \u003e 1 a counter cache hit will still help to reduce the duration\n# of the lock hold, helping with hot counter cell updates, but will not allow skipping\n# the read entirely. Only the local (clock, count) tuple of a counter cell is kept\n# in memory, not the whole counter, so it's relatively cheap.\n#\n# NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.\n#\n# Default value is empty to make it \"auto\" (min(2.5% of Heap (in MB), 50MB)). Set to 0 to disable counter cache.\n# NOTE: if you perform counter deletes and rely on low gcgs, you should disable the counter cache.\ncounter_cache_size_in_mb:\n\n# Duration in seconds after which Cassandra should\n# save the counter cache (keys only). Caches are saved to saved_caches_directory as\n# specified in this configuration file.\n#\n# Default is 7200 or 2 hours.\ncounter_cache_save_period: 7200\n\n# Number of keys from the counter cache to save\n# Disabled by default, meaning all keys are going to be saved\n# counter_cache_keys_to_save: 100\n\n# saved caches\n# If not set, the default directory is $CASSANDRA_HOME/data/saved_caches.\nsaved_caches_directory: /var/lib/cassandra/saved_caches\n\n# commitlog_sync may be either \"periodic\" or \"batch.\" \n# \n# When in batch mode, Cassandra won't ack writes until the commit log\n# has been fsynced to disk.  It will wait\n# commitlog_sync_batch_window_in_ms milliseconds between fsyncs.\n# This window should be kept short because the writer threads will\n# be unable to do extra work while waiting.  (You may need to increase\n# concurrent_writes for the same reason.)\n#\n# commitlog_sync: batch\n# commitlog_sync_batch_window_in_ms: 2\n#\n# the other option is \"periodic\" where writes may be acked immediately\n# and the CommitLog is simply synced every commitlog_sync_period_in_ms\n# milliseconds. \ncommitlog_sync: periodic\ncommitlog_sync_period_in_ms: 10000\n\n# The size of the individual commitlog file segments.  A commitlog\n# segment may be archived, deleted, or recycled once all the data\n# in it (potentially from each columnfamily in the system) has been\n# flushed to sstables.\n#\n# The default size is 32, which is almost always fine, but if you are\n# archiving commitlog segments (see commitlog_archiving.properties),\n# then you probably want a finer granularity of archiving; 8 or 16 MB\n# is reasonable.\n# Max mutation size is also configurable via max_mutation_size_in_kb setting in\n# cassandra.yaml. The default is half the size commitlog_segment_size_in_mb * 1024.\n#\n# NOTE: If max_mutation_size_in_kb is set explicitly then commitlog_segment_size_in_mb must\n# be set to at least twice the size of max_mutation_size_in_kb / 1024\n#\ncommitlog_segment_size_in_mb: 32\n\n# Compression to apply to the commit log. If omitted, the commit log\n# will be written uncompressed.  LZ4, Snappy, and Deflate compressors\n# are supported.\n#commitlog_compression:\n#   - class_name: LZ4Compressor\n#     parameters:\n#         -\n\n# any class that implements the SeedProvider interface and has a\n# constructor that takes a Map\u003cString, String\u003e of parameters will do.\nseed_provider:\n    # Addresses of hosts that are deemed contact points. \n    # Cassandra nodes use this list of hosts to find each other and learn\n    # the topology of the ring.  You must change this if you are running\n    # multiple nodes!\n    - class_name: org.apache.cassandra.locator.SimpleSeedProvider\n      parameters:\n          # seeds is actually a comma-delimited list of addresses.\n          # Ex: \"\u003cip1\u003e,\u003cip2\u003e,\u003cip3\u003e\"\n          - seeds: \"172.31.48.41,172.31.77.64,172.31.6.131\"\n\n# For workloads with more data than can fit in memory, Cassandra's\n# bottleneck will be reads that need to fetch data from\n# disk. \"concurrent_reads\" should be set to (16 * number_of_drives) in\n# order to allow the operations to enqueue low enough in the stack\n# that the OS and drives can reorder them. Same applies to\n# \"concurrent_counter_writes\", since counter writes read the current\n# values before incrementing and writing them back.\n#\n# On the other hand, since writes are almost never IO bound, the ideal\n# number of \"concurrent_writes\" is dependent on the number of cores in\n# your system; (8 * number_of_cores) is a good rule of thumb.\nconcurrent_reads: 32\nconcurrent_writes: 32\nconcurrent_counter_writes: 32\n\n# For materialized view writes, as there is a read involved, so this should\n# be limited by the less of concurrent reads or concurrent writes.\nconcurrent_materialized_view_writes: 32\n\n# Maximum memory to use for sstable chunk cache and buffer pooling.\n# 32MB of this are reserved for pooling buffers, the rest is used as an\n# cache that holds uncompressed sstable chunks.\n# Defaults to the smaller of 1/4 of heap or 512MB. This pool is allocated off-heap,\n# so is in addition to the memory allocated for heap. The cache also has on-heap\n# overhead which is roughly 128 bytes per chunk (i.e. 0.2% of the reserved size\n# if the default 64k chunk size is used).\n# Memory is only allocated when needed.\n# file_cache_size_in_mb: 512\n\n# Flag indicating whether to allocate on or off heap when the sstable buffer\n# pool is exhausted, that is when it has exceeded the maximum memory\n# file_cache_size_in_mb, beyond which it will not cache buffers but allocate on request.\n\n# buffer_pool_use_heap_if_exhausted: true\n\n# The strategy for optimizing disk read\n# Possible values are:\n# ssd (for solid state disks, the default)\n# spinning (for spinning disks)\n# disk_optimization_strategy: ssd\n\n# Total permitted memory to use for memtables. Cassandra will stop\n# accepting writes when the limit is exceeded until a flush completes,\n# and will trigger a flush based on memtable_cleanup_threshold\n# If omitted, Cassandra will set both to 1/4 the size of the heap.\n# memtable_heap_space_in_mb: 2048\n# memtable_offheap_space_in_mb: 2048\n\n# Ratio of occupied non-flushing memtable size to total permitted size\n# that will trigger a flush of the largest memtable. Larger mct will\n# mean larger flushes and hence less compaction, but also less concurrent\n# flush activity which can make it difficult to keep your disks fed\n# under heavy write load.\n#\n# memtable_cleanup_threshold defaults to 1 / (memtable_flush_writers + 1)\n# memtable_cleanup_threshold: 0.11\n\n# Specify the way Cassandra allocates and manages memtable memory.\n# Options are:\n#   heap_buffers:    on heap nio buffers\n#   offheap_buffers: off heap (direct) nio buffers\n#   offheap_objects: off heap objects\nmemtable_allocation_type: heap_buffers\n\n# Total space to use for commit logs on disk.\n#\n# If space gets above this value, Cassandra will flush every dirty CF\n# in the oldest segment and remove it.  So a small total commitlog space\n# will tend to cause more flush activity on less-active columnfamilies.\n#\n# The default value is the smaller of 8192, and 1/4 of the total space\n# of the commitlog volume.\n#\n# commitlog_total_space_in_mb: 8192\n\n# This sets the amount of memtable flush writer threads.  These will\n# be blocked by disk io, and each one will hold a memtable in memory\n# while blocked.\n#\n# memtable_flush_writers defaults to one per data_file_directory.\n#\n# If your data directories are backed by SSD, you can increase this, but\n# avoid having memtable_flush_writers * data_file_directories \u003e number of cores\n#memtable_flush_writers: 1\n\n# A fixed memory pool size in MB for for SSTable index summaries. If left\n# empty, this will default to 5% of the heap size. If the memory usage of\n# all index summaries exceeds this limit, SSTables with low read rates will\n# shrink their index summaries in order to meet this limit.  However, this\n# is a best-effort process. In extreme conditions Cassandra may need to use\n# more than this amount of memory.\nindex_summary_capacity_in_mb:\n\n# How frequently index summaries should be resampled.  This is done\n# periodically to redistribute memory from the fixed-size pool to sstables\n# proportional their recent read rates.  Setting to -1 will disable this\n# process, leaving existing index summaries at their current sampling level.\nindex_summary_resize_interval_in_minutes: 60\n\n# Whether to, when doing sequential writing, fsync() at intervals in\n# order to force the operating system to flush the dirty\n# buffers. Enable this to avoid sudden dirty buffer flushing from\n# impacting read latencies. Almost always a good idea on SSDs; not\n# necessarily on platters.\ntrickle_fsync: false\ntrickle_fsync_interval_in_kb: 10240\n\n# TCP port, for commands and data\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\nstorage_port: 7000\n\n# SSL port, for encrypted communication.  Unused unless enabled in\n# encryption_options\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\nssl_storage_port: 7001\n\n# Address or interface to bind to and tell other Cassandra nodes to connect to.\n# You _must_ change this if you want multiple nodes to be able to communicate!\n#\n# Set listen_address OR listen_interface, not both. Interfaces must correspond\n# to a single address, IP aliasing is not supported.\n#\n# Leaving it blank leaves it up to InetAddress.getLocalHost(). This\n# will always do the Right Thing _if_ the node is properly configured\n# (hostname, name resolution, etc), and the Right Thing is to use the\n# address associated with the hostname (it might not be).\n#\n# Setting listen_address to 0.0.0.0 is always wrong.\n#\n# If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\n# you can specify which should be chosen using listen_interface_prefer_ipv6. If false the first ipv4\n# address will be used. If true the first ipv6 address will be used. Defaults to false preferring\n# ipv4. If there is only one address it will be selected regardless of ipv4/ipv6.\nlisten_address: 172.31.77.64\n# listen_interface: eth0\n# listen_interface_prefer_ipv6: false\n\n# Address to broadcast to other Cassandra nodes\n# Leaving this blank will set it to the same value as listen_address\n# broadcast_address: 1.2.3.4\n\n# When using multiple physical network interfaces, set this\n# to true to listen on broadcast_address in addition to\n# the listen_address, allowing nodes to communicate in both\n# interfaces.\n# Ignore this property if the network configuration automatically\n# routes  between the public and private networks such as EC2.\n# listen_on_broadcast_address: false\n\n# Internode authentication backend, implementing IInternodeAuthenticator;\n# used to allow/disallow connections from peer nodes.\n# internode_authenticator: org.apache.cassandra.auth.AllowAllInternodeAuthenticator\n\n# Whether to start the native transport server.\n# Please note that the address on which the native transport is bound is the\n# same as the rpc_address. The port however is different and specified below.\nstart_native_transport: true\n# port for the CQL native transport to listen for clients on\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\nnative_transport_port: 9042\n# Enabling native transport encryption in client_encryption_options allows you to either use\n# encryption for the standard port or to use a dedicated, additional port along with the unencrypted\n# standard native_transport_port.\n# Enabling client encryption and keeping native_transport_port_ssl disabled will use encryption\n# for native_transport_port. Setting native_transport_port_ssl to a different value\n# from native_transport_port will use encryption for native_transport_port_ssl while\n# keeping native_transport_port unencrypted.\n# native_transport_port_ssl: 9142\n# The maximum threads for handling requests when the native transport is used.\n# This is similar to rpc_max_threads though the default differs slightly (and\n# there is no native_transport_min_threads, idle threads will always be stopped\n# after 30 seconds).\n# native_transport_max_threads: 128\n#\n# The maximum size of allowed frame. Frame (requests) larger than this will\n# be rejected as invalid. The default is 256MB.\n# native_transport_max_frame_size_in_mb: 256\n\n# The maximum number of concurrent client connections.\n# The default is -1, which means unlimited.\n# native_transport_max_concurrent_connections: -1\n\n# The maximum number of concurrent client connections per source ip.\n# The default is -1, which means unlimited.\n# native_transport_max_concurrent_connections_per_ip: -1\n\n# Whether to start the thrift rpc server.\nstart_rpc: true\n\n# The address or interface to bind the Thrift RPC service and native transport\n# server to.\n#\n# Set rpc_address OR rpc_interface, not both. Interfaces must correspond\n# to a single address, IP aliasing is not supported.\n#\n# Leaving rpc_address blank has the same effect as on listen_address\n# (i.e. it will be based on the configured hostname of the node).\n#\n# Note that unlike listen_address, you can specify 0.0.0.0, but you must also\n# set broadcast_rpc_address to a value other than 0.0.0.0.\n#\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\n#\n# If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\n# you can specify which should be chosen using rpc_interface_prefer_ipv6. If false the first ipv4\n# address will be used. If true the first ipv6 address will be used. Defaults to false preferring\n# ipv4. If there is only one address it will be selected regardless of ipv4/ipv6.\nrpc_address: 172.31.77.64\n# rpc_interface: eth1\n# rpc_interface_prefer_ipv6: false\n\n# port for Thrift to listen for clients on\nrpc_port: 9160\n\n# RPC address to broadcast to drivers and other Cassandra nodes. This cannot\n# be set to 0.0.0.0. If left blank, this will be set to the value of\n# rpc_address. If rpc_address is set to 0.0.0.0, broadcast_rpc_address must\n# be set.\n#broadcast_rpc_address: \n\n# enable or disable keepalive on rpc/native connections\nrpc_keepalive: true\n\n# Cassandra provides two out-of-the-box options for the RPC Server:\n#\n# sync  -\u003e One thread per thrift connection. For a very large number of clients, memory\n#          will be your limiting factor. On a 64 bit JVM, 180KB is the minimum stack size\n#          per thread, and that will correspond to your use of virtual memory (but physical memory\n#          may be limited depending on use of stack space).\n#\n# hsha  -\u003e Stands for \"half synchronous, half asynchronous.\" All thrift clients are handled\n#          asynchronously using a small number of threads that does not vary with the amount\n#          of thrift clients (and thus scales well to many clients). The rpc requests are still\n#          synchronous (one thread per active request). If hsha is selected then it is essential\n#          that rpc_max_threads is changed from the default value of unlimited.\n#\n# The default is sync because on Windows hsha is about 30% slower.  On Linux,\n# sync/hsha performance is about the same, with hsha of course using less memory.\n#\n# Alternatively,  can provide your own RPC server by providing the fully-qualified class name\n# of an o.a.c.t.TServerFactory that can create an instance of it.\nrpc_server_type: sync\n\n# Uncomment rpc_min|max_thread to set request pool size limits.\n#\n# Regardless of your choice of RPC server (see above), the number of maximum requests in the\n# RPC thread pool dictates how many concurrent requests are possible (but if you are using the sync\n# RPC server, it also dictates the number of clients that can be connected at all).\n#\n# The default is unlimited and thus provides no protection against clients overwhelming the server. You are\n# encouraged to set a maximum that makes sense for you in production, but do keep in mind that\n# rpc_max_threads represents the maximum number of client requests this server may execute concurrently.\n#\n# rpc_min_threads: 16\n# rpc_max_threads: 2048\n\n# uncomment to set socket buffer sizes on rpc connections\n# rpc_send_buff_size_in_bytes:\n# rpc_recv_buff_size_in_bytes:\n\n# Uncomment to set socket buffer size for internode communication\n# Note that when setting this, the buffer size is limited by net.core.wmem_max\n# and when not setting it it is defined by net.ipv4.tcp_wmem\n# See:\n# /proc/sys/net/core/wmem_max\n# /proc/sys/net/core/rmem_max\n# /proc/sys/net/ipv4/tcp_wmem\n# /proc/sys/net/ipv4/tcp_wmem\n# and: man tcp\n# internode_send_buff_size_in_bytes:\n# internode_recv_buff_size_in_bytes:\n\n# Frame size for thrift (maximum message length).\nthrift_framed_transport_size_in_mb: 15\n\n# Set to true to have Cassandra create a hard link to each sstable\n# flushed or streamed locally in a backups/ subdirectory of the\n# keyspace data.  Removing these links is the operator's\n# responsibility.\nincremental_backups: false\n\n# Whether or not to take a snapshot before each compaction.  Be\n# careful using this option, since Cassandra won't clean up the\n# snapshots for you.  Mostly useful if you're paranoid when there\n# is a data format change.\nsnapshot_before_compaction: false\n\n# Whether or not a snapshot is taken of the data before keyspace truncation\n# or dropping of column families. The STRONGLY advised default of true \n# should be used to provide data safety. If you set this flag to false, you will\n# lose data on truncation or drop.\nauto_snapshot: true\n\n# Granularity of the collation index of rows within a partition.\n# Increase if your rows are large, or if you have a very large\n# number of rows per partition.  The competing goals are these:\n#   1) a smaller granularity means more index entries are generated\n#      and looking up rows withing the partition by collation column\n#      is faster\n#   2) but, Cassandra will keep the collation index in memory for hot\n#      rows (as part of the key cache), so a larger granularity means\n#      you can cache more hot rows\ncolumn_index_size_in_kb: 64\n# Per sstable indexed key cache entries (the collation index in memory\n# mentioned above) exceeding this size will not be held on heap.\n# This means that only partition information is held on heap and the\n# index entries are read from disk.\n#\n# Note that this size refers to the size of the\n# serialized index information and not the size of the partition.\ncolumn_index_cache_size_in_kb: 2\n\n# Number of simultaneous compactions to allow, NOT including\n# validation \"compactions\" for anti-entropy repair.  Simultaneous\n# compactions can help preserve read performance in a mixed read/write\n# workload, by mitigating the tendency of small sstables to accumulate\n# during a single long running compactions. The default is usually\n# fine and if you experience problems with compaction running too\n# slowly or too fast, you should look at\n# compaction_throughput_mb_per_sec first.\n#\n# concurrent_compactors defaults to the smaller of (number of disks,\n# number of cores), with a minimum of 2 and a maximum of 8.\n# \n# If your data directories are backed by SSD, you should increase this\n# to the number of cores.\n#concurrent_compactors: 1\n\n# Throttles compaction to the given total throughput across the entire\n# system. The faster you insert data, the faster you need to compact in\n# order to keep the sstable count down, but in general, setting this to\n# 16 to 32 times the rate you are inserting data is more than sufficient.\n# Setting this to 0 disables throttling. Note that this account for all types\n# of compaction, including validation compaction.\ncompaction_throughput_mb_per_sec: 16\n\n# When compacting, the replacement sstable(s) can be opened before they\n# are completely written, and used in place of the prior sstables for\n# any range that has been written. This helps to smoothly transfer reads \n# between the sstables, reducing page cache churn and keeping hot rows hot\nsstable_preemptive_open_interval_in_mb: 50\n\n# Throttles all outbound streaming file transfers on this node to the\n# given total throughput in Mbps. This is necessary because Cassandra does\n# mostly sequential IO when streaming data during bootstrap or repair, which\n# can lead to saturating the network connection and degrading rpc performance.\n# When unset, the default is 200 Mbps or 25 MB/s.\n# stream_throughput_outbound_megabits_per_sec: 200\n\n# Throttles all streaming file transfer between the datacenters,\n# this setting allows users to throttle inter dc stream throughput in addition\n# to throttling all network stream traffic as configured with\n# stream_throughput_outbound_megabits_per_sec\n# When unset, the default is 200 Mbps or 25 MB/s\n# inter_dc_stream_throughput_outbound_megabits_per_sec: 200\n\n# How long the coordinator should wait for read operations to complete\nread_request_timeout_in_ms: 5000\n# How long the coordinator should wait for seq or index scans to complete\nrange_request_timeout_in_ms: 10000\n# How long the coordinator should wait for writes to complete\nwrite_request_timeout_in_ms: 2000\n# How long the coordinator should wait for counter writes to complete\ncounter_write_request_timeout_in_ms: 5000\n# How long a coordinator should continue to retry a CAS operation\n# that contends with other proposals for the same row\ncas_contention_timeout_in_ms: 1000\n# How long the coordinator should wait for truncates to complete\n# (This can be much longer, because unless auto_snapshot is disabled\n# we need to flush first so we can snapshot before removing the data.)\ntruncate_request_timeout_in_ms: 60000\n# The default timeout for other, miscellaneous operations\nrequest_timeout_in_ms: 10000\n\n# Enable operation timeout information exchange between nodes to accurately\n# measure request timeouts.  If disabled, replicas will assume that requests\n# were forwarded to them instantly by the coordinator, which means that\n# under overload conditions we will waste that much extra time processing \n# already-timed-out requests.\n#\n# Warning: before enabling this property make sure to ntp is installed\n# and the times are synchronized between the nodes.\ncross_node_timeout: false\n\n# Set socket timeout for streaming operation.\n# The stream session is failed if no data is received by any of the\n# participants within that period.\n# Default value is 3600000, which means streams timeout after an hour.\n# streaming_socket_timeout_in_ms: 3600000\n\n# phi value that must be reached for a host to be marked down.\n# most users should never need to adjust this.\n# phi_convict_threshold: 8\n\n# endpoint_snitch -- Set this to a class that implements\n# IEndpointSnitch.  The snitch has two functions:\n# - it teaches Cassandra enough about your network topology to route\n#   requests efficiently\n# - it allows Cassandra to spread replicas around your cluster to avoid\n#   correlated failures. It does this by grouping machines into\n#   \"datacenters\" and \"racks.\"  Cassandra will do its best not to have\n#   more than one replica on the same \"rack\" (which may not actually\n#   be a physical location)\n#\n# IF YOU CHANGE THE SNITCH AFTER DATA IS INSERTED INTO THE CLUSTER,\n# YOU MUST RUN A FULL REPAIR, SINCE THE SNITCH AFFECTS WHERE REPLICAS\n# ARE PLACED.\n#\n# IF THE RACK A REPLICA IS PLACED IN CHANGES AFTER THE REPLICA HAS BEEN\n# ADDED TO A RING, THE NODE MUST BE DECOMMISSIONED AND REBOOTSTRAPPED.\n#\n# Out of the box, Cassandra provides\n#  - SimpleSnitch:\n#    Treats Strategy order as proximity. This can improve cache\n#    locality when disabling read repair.  Only appropriate for\n#    single-datacenter deployments.\n#  - GossipingPropertyFileSnitch\n#    This should be your go-to snitch for production use.  The rack\n#    and datacenter for the local node are defined in\n#    cassandra-rackdc.properties and propagated to other nodes via\n#    gossip.  If cassandra-topology.properties exists, it is used as a\n#    fallback, allowing migration from the PropertyFileSnitch.\n#  - PropertyFileSnitch:\n#    Proximity is determined by rack and data center, which are\n#    explicitly configured in cassandra-topology.properties.\n#  - Ec2Snitch:\n#    Appropriate for EC2 deployments in a single Region. Loads Region\n#    and Availability Zone information from the EC2 API. The Region is\n#    treated as the datacenter, and the Availability Zone as the rack.\n#    Only private IPs are used, so this will not work across multiple\n#    Regions.\n#  - Ec2MultiRegionSnitch:\n#    Uses public IPs as broadcast_address to allow cross-region\n#    connectivity.  (Thus, you should set seed addresses to the public\n#    IP as well.) You will need to open the storage_port or\n#    ssl_storage_port on the public IP firewall.  (For intra-Region\n#    traffic, Cassandra will switch to the private IP after\n#    establishing a connection.)\n#  - RackInferringSnitch:\n#    Proximity is determined by rack and data center, which are\n#    assumed to correspond to the 3rd and 2nd octet of each node's IP\n#    address, respectively.  Unless this happens to match your\n#    deployment conventions, this is best used as an example of\n#    writing a custom Snitch class and is provided in that spirit.\n#\n# You can use a custom Snitch by setting this to the full class name\n# of the snitch, which will be assumed to be on your classpath.\nendpoint_snitch: GossipingPropertyFileSnitch\n\n# controls how often to perform the more expensive part of host score\n# calculation\ndynamic_snitch_update_interval_in_ms: 100 \n# controls how often to reset all host scores, allowing a bad host to\n# possibly recover\ndynamic_snitch_reset_interval_in_ms: 600000\n# if set greater than zero and read_repair_chance is \u003c 1.0, this will allow\n# 'pinning' of replicas to hosts in order to increase cache capacity.\n# The badness threshold will control how much worse the pinned host has to be\n# before the dynamic snitch will prefer other replicas over it.  This is\n# expressed as a double which represents a percentage.  Thus, a value of\n# 0.2 means Cassandra would continue to prefer the static snitch values\n# until the pinned host was 20% worse than the fastest.\ndynamic_snitch_badness_threshold: 0.1\n\n# request_scheduler -- Set this to a class that implements\n# RequestScheduler, which will schedule incoming client requests\n# according to the specific policy. This is useful for multi-tenancy\n# with a single Cassandra cluster.\n# NOTE: This is specifically for requests from the client and does\n# not affect inter node communication.\n# org.apache.cassandra.scheduler.NoScheduler - No scheduling takes place\n# org.apache.cassandra.scheduler.RoundRobinScheduler - Round robin of\n# client requests to a node with a separate queue for each\n# request_scheduler_id. The scheduler is further customized by\n# request_scheduler_options as described below.\nrequest_scheduler: org.apache.cassandra.scheduler.NoScheduler\n\n# Scheduler Options vary based on the type of scheduler\n# NoScheduler - Has no options\n# RoundRobin\n#  - throttle_limit -- The throttle_limit is the number of in-flight\n#                      requests per client.  Requests beyond \n#                      that limit are queued up until\n#                      running requests can complete.\n#                      The value of 80 here is twice the number of\n#                      concurrent_reads + concurrent_writes.\n#  - default_weight -- default_weight is optional and allows for\n#                      overriding the default which is 1.\n#  - weights -- Weights are optional and will default to 1 or the\n#               overridden default_weight. The weight translates into how\n#               many requests are handled during each turn of the\n#               RoundRobin, based on the scheduler id.\n#\n# request_scheduler_options:\n#    throttle_limit: 80\n#    default_weight: 5\n#    weights:\n#      Keyspace1: 1\n#      Keyspace2: 5\n\n# request_scheduler_id -- An identifier based on which to perform\n# the request scheduling. Currently the only valid option is keyspace.\n# request_scheduler_id: keyspace\n\n# Enable or disable inter-node encryption\n# JVM defaults for supported SSL socket protocols and cipher suites can\n# be replaced using custom encryption options. This is not recommended\n# unless you have policies in place that dictate certain settings, or\n# need to disable vulnerable ciphers or protocols in case the JVM cannot\n# be updated.\n# FIPS compliant settings can be configured at JVM level and should not\n# involve changing encryption settings here:\n# https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/FIPS.html\n# NOTE: No custom encryption options are enabled at the moment\n# The available internode options are : all, none, dc, rack\n#\n# If set to dc cassandra will encrypt the traffic between the DCs\n# If set to rack cassandra will encrypt the traffic between the racks\n#\n# The passwords used in these options must match the passwords used when generating\n# the keystore and truststore.  For instructions on generating these files, see:\n# http://download.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html#CreateKeystore\n#\nserver_encryption_options:\n    internode_encryption: none\n    keystore: conf/.keystore\n    keystore_password: cassandra\n    truststore: conf/.truststore\n    truststore_password: cassandra\n    # More advanced defaults below:\n    # protocol: TLS\n    # algorithm: SunX509\n    # store_type: JKS\n    # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n    # require_client_auth: false\n    # require_endpoint_verification: false\n\n# enable or disable client/server encryption.\nclient_encryption_options:\n    enabled: false\n    # If enabled and optional is set to true encrypted and unencrypted connections are handled.\n    optional: false\n    keystore: conf/.keystore\n    keystore_password: cassandra\n    # require_client_auth: false\n    # Set trustore and truststore_password if require_client_auth is true\n    # truststore: conf/.truststore\n    # truststore_password: cassandra\n    # More advanced defaults below:\n    # protocol: TLS\n    # algorithm: SunX509\n    # store_type: JKS\n    # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n\n# internode_compression controls whether traffic between nodes is\n# compressed.\n# can be:  all  - all traffic is compressed\n#          dc   - traffic between different datacenters is compressed\n#          none - nothing is compressed.\ninternode_compression: dc\n\n# Enable or disable tcp_nodelay for inter-dc communication.\n# Disabling it will result in larger (but fewer) network packets being sent,\n# reducing overhead from the TCP protocol itself, at the cost of increasing\n# latency if you block for cross-datacenter responses.\ninter_dc_tcp_nodelay: false\n\n# TTL for different trace types used during logging of the repair process.\ntracetype_query_ttl: 86400\ntracetype_repair_ttl: 604800\n\n# UDFs (user defined functions) are disabled by default.\n# As of Cassandra 3.0 there is a sandbox in place that should prevent execution of evil code.\nenable_user_defined_functions: false\n\n# Enables scripted UDFs (JavaScript UDFs).\n# Java UDFs are always enabled, if enable_user_defined_functions is true.\n# Enable this option to be able to use UDFs with \"language javascript\" or any custom JSR-223 provider.\n# This option has no effect, if enable_user_defined_functions is false.\nenable_scripted_user_defined_functions: false\n\n# The default Windows kernel timer and scheduling resolution is 15.6ms for power conservation.\n# Lowering this value on Windows can provide much tighter latency and better throughput, however\n# some virtualized environments may see a negative performance impact from changing this setting\n# below their system default. The sysinternals 'clockres' tool can confirm your system's default\n# setting.\nwindows_timer_interval: 1\n\n\n# Enables encrypting data at-rest (on disk). Different key providers can be plugged in, but the default reads from\n# a JCE-style keystore. A single keystore can hold multiple keys, but the one referenced by\n# the \"key_alias\" is the only key that will be used for encrypt opertaions; previously used keys\n# can still (and should!) be in the keystore and will be used on decrypt operations\n# (to handle the case of key rotation).\n#\n# It is strongly recommended to download and install Java Cryptography Extension (JCE)\n# Unlimited Strength Jurisdiction Policy Files for your version of the JDK.\n# (current link: http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html)\n#\n# Currently, only the following file types are supported for transparent data encryption, although\n# more are coming in future cassandra releases: commitlog, hints\ntransparent_data_encryption_options:\n    enabled: false\n    chunk_length_kb: 64\n    cipher: AES/CBC/PKCS5Padding\n    key_alias: testing:1\n    # CBC IV length for AES needs to be 16 bytes (which is also the default size)\n    # iv_length: 16\n    key_provider: \n      - class_name: org.apache.cassandra.security.JKSKeyProvider\n        parameters: \n          - keystore: conf/.keystore\n            keystore_password: cassandra\n            store_type: JCEKS\n            key_password: cassandra\n\n\n#####################\n# SAFETY THRESHOLDS #\n#####################\n\n# When executing a scan, within or across a partition, we need to keep the\n# tombstones seen in memory so we can return them to the coordinator, which\n# will use them to make sure other replicas also know about the deleted rows.\n# With workloads that generate a lot of tombstones, this can cause performance\n# problems and even exaust the server heap.\n# (http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets)\n# Adjust the thresholds here if you understand the dangers and want to\n# scan more tombstones anyway.  These thresholds may also be adjusted at runtime\n# using the StorageService mbean.\ntombstone_warn_threshold: 1000\ntombstone_failure_threshold: 100000\n\n# Log WARN on any batch size exceeding this value. 5kb per batch by default.\n# Caution should be taken on increasing the size of this threshold as it can lead to node instability.\nbatch_size_warn_threshold_in_kb: 5\n\n# Fail any batch exceeding this value. 50kb (10x warn threshold) by default.\nbatch_size_fail_threshold_in_kb: 50\n\n# Log WARN on any batches not of type LOGGED than span across more partitions than this limit\nunlogged_batch_across_partitions_warn_threshold: 10\n\n# Log a warning when compacting partitions larger than this value\ncompaction_large_partition_warning_threshold_mb: 100\n\n# GC Pauses greater than gc_warn_threshold_in_ms will be logged at WARN level\n# Adjust the threshold based on your application throughput requirement\n# By default, Cassandra logs GC Pauses greater than 200 ms at INFO level\ngc_warn_threshold_in_ms: 1000\n\nauto_bootstrap: false\n",
                            "template": "# Cassandra storage config YAML \n\n# NOTE:\n#   See http://wiki.apache.org/cassandra/StorageConfiguration for\n#   full explanations of configuration directives\n# /NOTE\n\n# The name of the cluster. This is mainly used to prevent machines in\n# one logical cluster from joining another.\ncluster_name: '${cluster_name}'\n\n# This defines the number of tokens randomly assigned to this node on the ring\n# The more tokens, relative to other nodes, the larger the proportion of data\n# that this node will store. You probably want all nodes to have the same number\n# of tokens assuming they have equal hardware capability.\n#\n# If you leave this unspecified, Cassandra will use the default of 1 token for legacy compatibility,\n# and will use the initial_token as described below.\n#\n# Specifying initial_token will override this setting on the node's initial start,\n# on subsequent starts, this setting will apply even if initial token is set.\n#\n# If you already have a cluster with 1 token per node, and wish to migrate to \n# multiple tokens per node, see http://wiki.apache.org/cassandra/Operations\nnum_tokens: 256\n\n# Triggers automatic allocation of num_tokens tokens for this node. The allocation\n# algorithm attempts to choose tokens in a way that optimizes replicated load over\n# the nodes in the datacenter for the replication strategy used by the specified\n# keyspace.\n#\n# The load assigned to each node will be close to proportional to its number of\n# vnodes.\n#\n# Only supported with the Murmur3Partitioner.\n# allocate_tokens_for_keyspace: KEYSPACE\n\n# initial_token allows you to specify tokens manually.  While you can use # it with\n# vnodes (num_tokens \u003e 1, above) -- in which case you should provide a \n# comma-separated list -- it's primarily used when adding nodes # to legacy clusters \n# that do not have vnodes enabled.\n# initial_token:\n\n# See http://wiki.apache.org/cassandra/HintedHandoff\n# May either be \"true\" or \"false\" to enable globally\nhinted_handoff_enabled: true\n# When hinted_handoff_enabled is true, a black list of data centers that will not\n# perform hinted handoff\n#hinted_handoff_disabled_datacenters:\n#    - DC1\n#    - DC2\n# this defines the maximum amount of time a dead host will have hints\n# generated.  After it has been dead this long, new hints for it will not be\n# created until it has been seen alive and gone down again.\nmax_hint_window_in_ms: 10800000 # 3 hours\n\n# Maximum throttle in KBs per second, per delivery thread.  This will be\n# reduced proportionally to the number of nodes in the cluster.  (If there\n# are two nodes in the cluster, each delivery thread will use the maximum\n# rate; if there are three, each will throttle to half of the maximum,\n# since we expect two nodes to be delivering hints simultaneously.)\nhinted_handoff_throttle_in_kb: 1024\n\n# Number of threads with which to deliver hints;\n# Consider increasing this number when you have multi-dc deployments, since\n# cross-dc handoff tends to be slower\nmax_hints_delivery_threads: 2\n\n# Directory where Cassandra should store hints.\n# If not set, the default directory is $CASSANDRA_HOME/data/hints.\n# hints_directory: /var/lib/cassandra/hints\n\n# How often hints should be flushed from the internal buffers to disk.\n# Will *not* trigger fsync.\nhints_flush_period_in_ms: 10000\n\n# Maximum size for a single hints file, in megabytes.\nmax_hints_file_size_in_mb: 128\n\n# Compression to apply to the hint files. If omitted, hints files\n# will be written uncompressed. LZ4, Snappy, and Deflate compressors\n# are supported.\n#hints_compression:\n#   - class_name: LZ4Compressor\n#     parameters:\n#         -\n\n# Maximum throttle in KBs per second, total. This will be\n# reduced proportionally to the number of nodes in the cluster.\nbatchlog_replay_throttle_in_kb: 1024\n\n# Authentication backend, implementing IAuthenticator; used to identify users\n# Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthenticator,\n# PasswordAuthenticator}.\n#\n# - AllowAllAuthenticator performs no checks - set it to disable authentication.\n# - PasswordAuthenticator relies on username/password pairs to authenticate\n#   users. It keeps usernames and hashed passwords in system_auth.credentials table.\n#   Please increase system_auth keyspace replication factor if you use this authenticator.\n#   If using PasswordAuthenticator, CassandraRoleManager must also be used (see below)\nauthenticator: AllowAllAuthenticator\n\n# Authorization backend, implementing IAuthorizer; used to limit access/provide permissions\n# Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthorizer,\n# CassandraAuthorizer}.\n#\n# - AllowAllAuthorizer allows any action to any user - set it to disable authorization.\n# - CassandraAuthorizer stores permissions in system_auth.permissions table. Please\n#   increase system_auth keyspace replication factor if you use this authorizer.\nauthorizer: AllowAllAuthorizer\n\n# Part of the Authentication \u0026 Authorization backend, implementing IRoleManager; used\n# to maintain grants and memberships between roles.\n# Out of the box, Cassandra provides org.apache.cassandra.auth.CassandraRoleManager,\n# which stores role information in the system_auth keyspace. Most functions of the\n# IRoleManager require an authenticated login, so unless the configured IAuthenticator\n# actually implements authentication, most of this functionality will be unavailable.\n#\n# - CassandraRoleManager stores role data in the system_auth keyspace. Please\n#   increase system_auth keyspace replication factor if you use this role manager.\nrole_manager: CassandraRoleManager\n\n# Validity period for roles cache (fetching granted roles can be an expensive\n# operation depending on the role manager, CassandraRoleManager is one example)\n# Granted roles are cached for authenticated sessions in AuthenticatedUser and\n# after the period specified here, become eligible for (async) reload.\n# Defaults to 2000, set to 0 to disable caching entirely.\n# Will be disabled automatically for AllowAllAuthenticator.\nroles_validity_in_ms: 2000\n\n# Refresh interval for roles cache (if enabled).\n# After this interval, cache entries become eligible for refresh. Upon next\n# access, an async reload is scheduled and the old value returned until it\n# completes. If roles_validity_in_ms is non-zero, then this must be\n# also.\n# Defaults to the same value as roles_validity_in_ms.\n# roles_update_interval_in_ms: 2000\n\n# Validity period for permissions cache (fetching permissions can be an\n# expensive operation depending on the authorizer, CassandraAuthorizer is\n# one example). Defaults to 2000, set to 0 to disable.\n# Will be disabled automatically for AllowAllAuthorizer.\npermissions_validity_in_ms: 2000\n\n# Refresh interval for permissions cache (if enabled).\n# After this interval, cache entries become eligible for refresh. Upon next\n# access, an async reload is scheduled and the old value returned until it\n# completes. If permissions_validity_in_ms is non-zero, then this must be\n# also.\n# Defaults to the same value as permissions_validity_in_ms.\n# permissions_update_interval_in_ms: 2000\n\n# Validity period for credentials cache. This cache is tightly coupled to\n# the provided PasswordAuthenticator implementation of IAuthenticator. If\n# another IAuthenticator implementation is configured, this cache will not\n# be automatically used and so the following settings will have no effect.\n# Please note, credentials are cached in their encrypted form, so while\n# activating this cache may reduce the number of queries made to the\n# underlying table, it may not  bring a significant reduction in the\n# latency of individual authentication attempts.\n# Defaults to 2000, set to 0 to disable credentials caching.\ncredentials_validity_in_ms: 2000\n\n# Refresh interval for credentials cache (if enabled).\n# After this interval, cache entries become eligible for refresh. Upon next\n# access, an async reload is scheduled and the old value returned until it\n# completes. If credentials_validity_in_ms is non-zero, then this must be\n# also.\n# Defaults to the same value as credentials_validity_in_ms.\n# credentials_update_interval_in_ms: 2000\n\n# The partitioner is responsible for distributing groups of rows (by\n# partition key) across nodes in the cluster.  You should leave this\n# alone for new clusters.  The partitioner can NOT be changed without\n# reloading all data, so when upgrading you should set this to the\n# same partitioner you were already using.\n#\n# Besides Murmur3Partitioner, partitioners included for backwards\n# compatibility include RandomPartitioner, ByteOrderedPartitioner, and\n# OrderPreservingPartitioner.\n#\npartitioner: org.apache.cassandra.dht.Murmur3Partitioner\n\n# Directories where Cassandra should store data on disk.  Cassandra\n# will spread data evenly across them, subject to the granularity of\n# the configured compaction strategy.\n# If not set, the default directory is $CASSANDRA_HOME/data/data.\ndata_file_directories:\n    - /var/lib/cassandra/data\n\n# commit log.  when running on magnetic HDD, this should be a\n# separate spindle than the data directories.\n# If not set, the default directory is $CASSANDRA_HOME/data/commitlog.\ncommitlog_directory: /var/lib/cassandra/commitlog\n\n# policy for data disk failures:\n# die: shut down gossip and client transports and kill the JVM for any fs errors or\n#      single-sstable errors, so the node can be replaced.\n# stop_paranoid: shut down gossip and client transports even for single-sstable errors,\n#                kill the JVM for errors during startup.\n# stop: shut down gossip and client transports, leaving the node effectively dead, but\n#       can still be inspected via JMX, kill the JVM for errors during startup.\n# best_effort: stop using the failed disk and respond to requests based on\n#              remaining available sstables.  This means you WILL see obsolete\n#              data at CL.ONE!\n# ignore: ignore fatal errors and let requests fail, as in pre-1.2 Cassandra\ndisk_failure_policy: stop\n\n# policy for commit disk failures:\n# die: shut down gossip and Thrift and kill the JVM, so the node can be replaced.\n# stop: shut down gossip and Thrift, leaving the node effectively dead, but\n#       can still be inspected via JMX.\n# stop_commit: shutdown the commit log, letting writes collect but\n#              continuing to service reads, as in pre-2.0.5 Cassandra\n# ignore: ignore fatal errors and let the batches fail\ncommit_failure_policy: stop\n\n# Maximum size of the native protocol prepared statement cache\n#\n# Valid values are either \"auto\" (omitting the value) or a value greater 0.\n#\n# Note that specifying a too large value will result in long running GCs and possbily\n# out-of-memory errors. Keep the value at a small fraction of the heap.\n#\n# If you constantly see \"prepared statements discarded in the last minute because\n# cache limit reached\" messages, the first step is to investigate the root cause\n# of these messages and check whether prepared statements are used correctly -\n# i.e. use bind markers for variable parts.\n#\n# Do only change the default value, if you really have more prepared statements than\n# fit in the cache. In most cases it is not neccessary to change this value.\n# Constantly re-preparing statements is a performance penalty.\n#\n# Default value (\"auto\") is 1/256th of the heap or 10MB, whichever is greater\nprepared_statements_cache_size_mb:\n\n# Maximum size of the Thrift prepared statement cache\n#\n# If you do not use Thrift at all, it is safe to leave this value at \"auto\".\n#\n# See description of 'prepared_statements_cache_size_mb' above for more information.\n#\n# Default value (\"auto\") is 1/256th of the heap or 10MB, whichever is greater\nthrift_prepared_statements_cache_size_mb:\n\n# Maximum size of the key cache in memory.\n#\n# Each key cache hit saves 1 seek and each row cache hit saves 2 seeks at the\n# minimum, sometimes more. The key cache is fairly tiny for the amount of\n# time it saves, so it's worthwhile to use it at large numbers.\n# The row cache saves even more time, but must contain the entire row,\n# so it is extremely space-intensive. It's best to only use the\n# row cache if you have hot rows or static rows.\n#\n# NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.\n#\n# Default value is empty to make it \"auto\" (min(5% of Heap (in MB), 100MB)). Set to 0 to disable key cache.\nkey_cache_size_in_mb:\n\n# Duration in seconds after which Cassandra should\n# save the key cache. Caches are saved to saved_caches_directory as\n# specified in this configuration file.\n#\n# Saved caches greatly improve cold-start speeds, and is relatively cheap in\n# terms of I/O for the key cache. Row cache saving is much more expensive and\n# has limited use.\n#\n# Default is 14400 or 4 hours.\nkey_cache_save_period: 14400\n\n# Number of keys from the key cache to save\n# Disabled by default, meaning all keys are going to be saved\n# key_cache_keys_to_save: 100\n\n# Row cache implementation class name.\n# Available implementations:\n#   org.apache.cassandra.cache.OHCProvider                Fully off-heap row cache implementation (default).\n#   org.apache.cassandra.cache.SerializingCacheProvider   This is the row cache implementation availabile\n#                                                         in previous releases of Cassandra.\n# row_cache_class_name: org.apache.cassandra.cache.OHCProvider\n\n# Maximum size of the row cache in memory.\n# Please note that OHC cache implementation requires some additional off-heap memory to manage\n# the map structures and some in-flight memory during operations before/after cache entries can be\n# accounted against the cache capacity. This overhead is usually small compared to the whole capacity.\n# Do not specify more memory that the system can afford in the worst usual situation and leave some\n# headroom for OS block level cache. Do never allow your system to swap.\n#\n# Default value is 0, to disable row caching.\nrow_cache_size_in_mb: 0\n\n# Duration in seconds after which Cassandra should save the row cache.\n# Caches are saved to saved_caches_directory as specified in this configuration file.\n#\n# Saved caches greatly improve cold-start speeds, and is relatively cheap in\n# terms of I/O for the key cache. Row cache saving is much more expensive and\n# has limited use.\n#\n# Default is 0 to disable saving the row cache.\nrow_cache_save_period: 0\n\n# Number of keys from the row cache to save.\n# Specify 0 (which is the default), meaning all keys are going to be saved\n# row_cache_keys_to_save: 100\n\n# Maximum size of the counter cache in memory.\n#\n# Counter cache helps to reduce counter locks' contention for hot counter cells.\n# In case of RF = 1 a counter cache hit will cause Cassandra to skip the read before\n# write entirely. With RF \u003e 1 a counter cache hit will still help to reduce the duration\n# of the lock hold, helping with hot counter cell updates, but will not allow skipping\n# the read entirely. Only the local (clock, count) tuple of a counter cell is kept\n# in memory, not the whole counter, so it's relatively cheap.\n#\n# NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.\n#\n# Default value is empty to make it \"auto\" (min(2.5% of Heap (in MB), 50MB)). Set to 0 to disable counter cache.\n# NOTE: if you perform counter deletes and rely on low gcgs, you should disable the counter cache.\ncounter_cache_size_in_mb:\n\n# Duration in seconds after which Cassandra should\n# save the counter cache (keys only). Caches are saved to saved_caches_directory as\n# specified in this configuration file.\n#\n# Default is 7200 or 2 hours.\ncounter_cache_save_period: 7200\n\n# Number of keys from the counter cache to save\n# Disabled by default, meaning all keys are going to be saved\n# counter_cache_keys_to_save: 100\n\n# saved caches\n# If not set, the default directory is $CASSANDRA_HOME/data/saved_caches.\nsaved_caches_directory: /var/lib/cassandra/saved_caches\n\n# commitlog_sync may be either \"periodic\" or \"batch.\" \n# \n# When in batch mode, Cassandra won't ack writes until the commit log\n# has been fsynced to disk.  It will wait\n# commitlog_sync_batch_window_in_ms milliseconds between fsyncs.\n# This window should be kept short because the writer threads will\n# be unable to do extra work while waiting.  (You may need to increase\n# concurrent_writes for the same reason.)\n#\n# commitlog_sync: batch\n# commitlog_sync_batch_window_in_ms: 2\n#\n# the other option is \"periodic\" where writes may be acked immediately\n# and the CommitLog is simply synced every commitlog_sync_period_in_ms\n# milliseconds. \ncommitlog_sync: periodic\ncommitlog_sync_period_in_ms: 10000\n\n# The size of the individual commitlog file segments.  A commitlog\n# segment may be archived, deleted, or recycled once all the data\n# in it (potentially from each columnfamily in the system) has been\n# flushed to sstables.\n#\n# The default size is 32, which is almost always fine, but if you are\n# archiving commitlog segments (see commitlog_archiving.properties),\n# then you probably want a finer granularity of archiving; 8 or 16 MB\n# is reasonable.\n# Max mutation size is also configurable via max_mutation_size_in_kb setting in\n# cassandra.yaml. The default is half the size commitlog_segment_size_in_mb * 1024.\n#\n# NOTE: If max_mutation_size_in_kb is set explicitly then commitlog_segment_size_in_mb must\n# be set to at least twice the size of max_mutation_size_in_kb / 1024\n#\ncommitlog_segment_size_in_mb: 32\n\n# Compression to apply to the commit log. If omitted, the commit log\n# will be written uncompressed.  LZ4, Snappy, and Deflate compressors\n# are supported.\n#commitlog_compression:\n#   - class_name: LZ4Compressor\n#     parameters:\n#         -\n\n# any class that implements the SeedProvider interface and has a\n# constructor that takes a Map\u003cString, String\u003e of parameters will do.\nseed_provider:\n    # Addresses of hosts that are deemed contact points. \n    # Cassandra nodes use this list of hosts to find each other and learn\n    # the topology of the ring.  You must change this if you are running\n    # multiple nodes!\n    - class_name: org.apache.cassandra.locator.SimpleSeedProvider\n      parameters:\n          # seeds is actually a comma-delimited list of addresses.\n          # Ex: \"\u003cip1\u003e,\u003cip2\u003e,\u003cip3\u003e\"\n          - seeds: \"${seeds}\"\n\n# For workloads with more data than can fit in memory, Cassandra's\n# bottleneck will be reads that need to fetch data from\n# disk. \"concurrent_reads\" should be set to (16 * number_of_drives) in\n# order to allow the operations to enqueue low enough in the stack\n# that the OS and drives can reorder them. Same applies to\n# \"concurrent_counter_writes\", since counter writes read the current\n# values before incrementing and writing them back.\n#\n# On the other hand, since writes are almost never IO bound, the ideal\n# number of \"concurrent_writes\" is dependent on the number of cores in\n# your system; (8 * number_of_cores) is a good rule of thumb.\nconcurrent_reads: 32\nconcurrent_writes: 32\nconcurrent_counter_writes: 32\n\n# For materialized view writes, as there is a read involved, so this should\n# be limited by the less of concurrent reads or concurrent writes.\nconcurrent_materialized_view_writes: 32\n\n# Maximum memory to use for sstable chunk cache and buffer pooling.\n# 32MB of this are reserved for pooling buffers, the rest is used as an\n# cache that holds uncompressed sstable chunks.\n# Defaults to the smaller of 1/4 of heap or 512MB. This pool is allocated off-heap,\n# so is in addition to the memory allocated for heap. The cache also has on-heap\n# overhead which is roughly 128 bytes per chunk (i.e. 0.2% of the reserved size\n# if the default 64k chunk size is used).\n# Memory is only allocated when needed.\n# file_cache_size_in_mb: 512\n\n# Flag indicating whether to allocate on or off heap when the sstable buffer\n# pool is exhausted, that is when it has exceeded the maximum memory\n# file_cache_size_in_mb, beyond which it will not cache buffers but allocate on request.\n\n# buffer_pool_use_heap_if_exhausted: true\n\n# The strategy for optimizing disk read\n# Possible values are:\n# ssd (for solid state disks, the default)\n# spinning (for spinning disks)\n# disk_optimization_strategy: ssd\n\n# Total permitted memory to use for memtables. Cassandra will stop\n# accepting writes when the limit is exceeded until a flush completes,\n# and will trigger a flush based on memtable_cleanup_threshold\n# If omitted, Cassandra will set both to 1/4 the size of the heap.\n# memtable_heap_space_in_mb: 2048\n# memtable_offheap_space_in_mb: 2048\n\n# Ratio of occupied non-flushing memtable size to total permitted size\n# that will trigger a flush of the largest memtable. Larger mct will\n# mean larger flushes and hence less compaction, but also less concurrent\n# flush activity which can make it difficult to keep your disks fed\n# under heavy write load.\n#\n# memtable_cleanup_threshold defaults to 1 / (memtable_flush_writers + 1)\n# memtable_cleanup_threshold: 0.11\n\n# Specify the way Cassandra allocates and manages memtable memory.\n# Options are:\n#   heap_buffers:    on heap nio buffers\n#   offheap_buffers: off heap (direct) nio buffers\n#   offheap_objects: off heap objects\nmemtable_allocation_type: heap_buffers\n\n# Total space to use for commit logs on disk.\n#\n# If space gets above this value, Cassandra will flush every dirty CF\n# in the oldest segment and remove it.  So a small total commitlog space\n# will tend to cause more flush activity on less-active columnfamilies.\n#\n# The default value is the smaller of 8192, and 1/4 of the total space\n# of the commitlog volume.\n#\n# commitlog_total_space_in_mb: 8192\n\n# This sets the amount of memtable flush writer threads.  These will\n# be blocked by disk io, and each one will hold a memtable in memory\n# while blocked.\n#\n# memtable_flush_writers defaults to one per data_file_directory.\n#\n# If your data directories are backed by SSD, you can increase this, but\n# avoid having memtable_flush_writers * data_file_directories \u003e number of cores\n#memtable_flush_writers: 1\n\n# A fixed memory pool size in MB for for SSTable index summaries. If left\n# empty, this will default to 5% of the heap size. If the memory usage of\n# all index summaries exceeds this limit, SSTables with low read rates will\n# shrink their index summaries in order to meet this limit.  However, this\n# is a best-effort process. In extreme conditions Cassandra may need to use\n# more than this amount of memory.\nindex_summary_capacity_in_mb:\n\n# How frequently index summaries should be resampled.  This is done\n# periodically to redistribute memory from the fixed-size pool to sstables\n# proportional their recent read rates.  Setting to -1 will disable this\n# process, leaving existing index summaries at their current sampling level.\nindex_summary_resize_interval_in_minutes: 60\n\n# Whether to, when doing sequential writing, fsync() at intervals in\n# order to force the operating system to flush the dirty\n# buffers. Enable this to avoid sudden dirty buffer flushing from\n# impacting read latencies. Almost always a good idea on SSDs; not\n# necessarily on platters.\ntrickle_fsync: false\ntrickle_fsync_interval_in_kb: 10240\n\n# TCP port, for commands and data\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\nstorage_port: 7000\n\n# SSL port, for encrypted communication.  Unused unless enabled in\n# encryption_options\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\nssl_storage_port: 7001\n\n# Address or interface to bind to and tell other Cassandra nodes to connect to.\n# You _must_ change this if you want multiple nodes to be able to communicate!\n#\n# Set listen_address OR listen_interface, not both. Interfaces must correspond\n# to a single address, IP aliasing is not supported.\n#\n# Leaving it blank leaves it up to InetAddress.getLocalHost(). This\n# will always do the Right Thing _if_ the node is properly configured\n# (hostname, name resolution, etc), and the Right Thing is to use the\n# address associated with the hostname (it might not be).\n#\n# Setting listen_address to 0.0.0.0 is always wrong.\n#\n# If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\n# you can specify which should be chosen using listen_interface_prefer_ipv6. If false the first ipv4\n# address will be used. If true the first ipv6 address will be used. Defaults to false preferring\n# ipv4. If there is only one address it will be selected regardless of ipv4/ipv6.\nlisten_address: ${server_ip}\n# listen_interface: eth0\n# listen_interface_prefer_ipv6: false\n\n# Address to broadcast to other Cassandra nodes\n# Leaving this blank will set it to the same value as listen_address\n# broadcast_address: 1.2.3.4\n\n# When using multiple physical network interfaces, set this\n# to true to listen on broadcast_address in addition to\n# the listen_address, allowing nodes to communicate in both\n# interfaces.\n# Ignore this property if the network configuration automatically\n# routes  between the public and private networks such as EC2.\n# listen_on_broadcast_address: false\n\n# Internode authentication backend, implementing IInternodeAuthenticator;\n# used to allow/disallow connections from peer nodes.\n# internode_authenticator: org.apache.cassandra.auth.AllowAllInternodeAuthenticator\n\n# Whether to start the native transport server.\n# Please note that the address on which the native transport is bound is the\n# same as the rpc_address. The port however is different and specified below.\nstart_native_transport: true\n# port for the CQL native transport to listen for clients on\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\nnative_transport_port: 9042\n# Enabling native transport encryption in client_encryption_options allows you to either use\n# encryption for the standard port or to use a dedicated, additional port along with the unencrypted\n# standard native_transport_port.\n# Enabling client encryption and keeping native_transport_port_ssl disabled will use encryption\n# for native_transport_port. Setting native_transport_port_ssl to a different value\n# from native_transport_port will use encryption for native_transport_port_ssl while\n# keeping native_transport_port unencrypted.\n# native_transport_port_ssl: 9142\n# The maximum threads for handling requests when the native transport is used.\n# This is similar to rpc_max_threads though the default differs slightly (and\n# there is no native_transport_min_threads, idle threads will always be stopped\n# after 30 seconds).\n# native_transport_max_threads: 128\n#\n# The maximum size of allowed frame. Frame (requests) larger than this will\n# be rejected as invalid. The default is 256MB.\n# native_transport_max_frame_size_in_mb: 256\n\n# The maximum number of concurrent client connections.\n# The default is -1, which means unlimited.\n# native_transport_max_concurrent_connections: -1\n\n# The maximum number of concurrent client connections per source ip.\n# The default is -1, which means unlimited.\n# native_transport_max_concurrent_connections_per_ip: -1\n\n# Whether to start the thrift rpc server.\nstart_rpc: true\n\n# The address or interface to bind the Thrift RPC service and native transport\n# server to.\n#\n# Set rpc_address OR rpc_interface, not both. Interfaces must correspond\n# to a single address, IP aliasing is not supported.\n#\n# Leaving rpc_address blank has the same effect as on listen_address\n# (i.e. it will be based on the configured hostname of the node).\n#\n# Note that unlike listen_address, you can specify 0.0.0.0, but you must also\n# set broadcast_rpc_address to a value other than 0.0.0.0.\n#\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\n#\n# If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\n# you can specify which should be chosen using rpc_interface_prefer_ipv6. If false the first ipv4\n# address will be used. If true the first ipv6 address will be used. Defaults to false preferring\n# ipv4. If there is only one address it will be selected regardless of ipv4/ipv6.\nrpc_address: ${server_ip}\n# rpc_interface: eth1\n# rpc_interface_prefer_ipv6: false\n\n# port for Thrift to listen for clients on\nrpc_port: 9160\n\n# RPC address to broadcast to drivers and other Cassandra nodes. This cannot\n# be set to 0.0.0.0. If left blank, this will be set to the value of\n# rpc_address. If rpc_address is set to 0.0.0.0, broadcast_rpc_address must\n# be set.\n#broadcast_rpc_address: \n\n# enable or disable keepalive on rpc/native connections\nrpc_keepalive: true\n\n# Cassandra provides two out-of-the-box options for the RPC Server:\n#\n# sync  -\u003e One thread per thrift connection. For a very large number of clients, memory\n#          will be your limiting factor. On a 64 bit JVM, 180KB is the minimum stack size\n#          per thread, and that will correspond to your use of virtual memory (but physical memory\n#          may be limited depending on use of stack space).\n#\n# hsha  -\u003e Stands for \"half synchronous, half asynchronous.\" All thrift clients are handled\n#          asynchronously using a small number of threads that does not vary with the amount\n#          of thrift clients (and thus scales well to many clients). The rpc requests are still\n#          synchronous (one thread per active request). If hsha is selected then it is essential\n#          that rpc_max_threads is changed from the default value of unlimited.\n#\n# The default is sync because on Windows hsha is about 30% slower.  On Linux,\n# sync/hsha performance is about the same, with hsha of course using less memory.\n#\n# Alternatively,  can provide your own RPC server by providing the fully-qualified class name\n# of an o.a.c.t.TServerFactory that can create an instance of it.\nrpc_server_type: sync\n\n# Uncomment rpc_min|max_thread to set request pool size limits.\n#\n# Regardless of your choice of RPC server (see above), the number of maximum requests in the\n# RPC thread pool dictates how many concurrent requests are possible (but if you are using the sync\n# RPC server, it also dictates the number of clients that can be connected at all).\n#\n# The default is unlimited and thus provides no protection against clients overwhelming the server. You are\n# encouraged to set a maximum that makes sense for you in production, but do keep in mind that\n# rpc_max_threads represents the maximum number of client requests this server may execute concurrently.\n#\n# rpc_min_threads: 16\n# rpc_max_threads: 2048\n\n# uncomment to set socket buffer sizes on rpc connections\n# rpc_send_buff_size_in_bytes:\n# rpc_recv_buff_size_in_bytes:\n\n# Uncomment to set socket buffer size for internode communication\n# Note that when setting this, the buffer size is limited by net.core.wmem_max\n# and when not setting it it is defined by net.ipv4.tcp_wmem\n# See:\n# /proc/sys/net/core/wmem_max\n# /proc/sys/net/core/rmem_max\n# /proc/sys/net/ipv4/tcp_wmem\n# /proc/sys/net/ipv4/tcp_wmem\n# and: man tcp\n# internode_send_buff_size_in_bytes:\n# internode_recv_buff_size_in_bytes:\n\n# Frame size for thrift (maximum message length).\nthrift_framed_transport_size_in_mb: 15\n\n# Set to true to have Cassandra create a hard link to each sstable\n# flushed or streamed locally in a backups/ subdirectory of the\n# keyspace data.  Removing these links is the operator's\n# responsibility.\nincremental_backups: false\n\n# Whether or not to take a snapshot before each compaction.  Be\n# careful using this option, since Cassandra won't clean up the\n# snapshots for you.  Mostly useful if you're paranoid when there\n# is a data format change.\nsnapshot_before_compaction: false\n\n# Whether or not a snapshot is taken of the data before keyspace truncation\n# or dropping of column families. The STRONGLY advised default of true \n# should be used to provide data safety. If you set this flag to false, you will\n# lose data on truncation or drop.\nauto_snapshot: true\n\n# Granularity of the collation index of rows within a partition.\n# Increase if your rows are large, or if you have a very large\n# number of rows per partition.  The competing goals are these:\n#   1) a smaller granularity means more index entries are generated\n#      and looking up rows withing the partition by collation column\n#      is faster\n#   2) but, Cassandra will keep the collation index in memory for hot\n#      rows (as part of the key cache), so a larger granularity means\n#      you can cache more hot rows\ncolumn_index_size_in_kb: 64\n# Per sstable indexed key cache entries (the collation index in memory\n# mentioned above) exceeding this size will not be held on heap.\n# This means that only partition information is held on heap and the\n# index entries are read from disk.\n#\n# Note that this size refers to the size of the\n# serialized index information and not the size of the partition.\ncolumn_index_cache_size_in_kb: 2\n\n# Number of simultaneous compactions to allow, NOT including\n# validation \"compactions\" for anti-entropy repair.  Simultaneous\n# compactions can help preserve read performance in a mixed read/write\n# workload, by mitigating the tendency of small sstables to accumulate\n# during a single long running compactions. The default is usually\n# fine and if you experience problems with compaction running too\n# slowly or too fast, you should look at\n# compaction_throughput_mb_per_sec first.\n#\n# concurrent_compactors defaults to the smaller of (number of disks,\n# number of cores), with a minimum of 2 and a maximum of 8.\n# \n# If your data directories are backed by SSD, you should increase this\n# to the number of cores.\n#concurrent_compactors: 1\n\n# Throttles compaction to the given total throughput across the entire\n# system. The faster you insert data, the faster you need to compact in\n# order to keep the sstable count down, but in general, setting this to\n# 16 to 32 times the rate you are inserting data is more than sufficient.\n# Setting this to 0 disables throttling. Note that this account for all types\n# of compaction, including validation compaction.\ncompaction_throughput_mb_per_sec: 16\n\n# When compacting, the replacement sstable(s) can be opened before they\n# are completely written, and used in place of the prior sstables for\n# any range that has been written. This helps to smoothly transfer reads \n# between the sstables, reducing page cache churn and keeping hot rows hot\nsstable_preemptive_open_interval_in_mb: 50\n\n# Throttles all outbound streaming file transfers on this node to the\n# given total throughput in Mbps. This is necessary because Cassandra does\n# mostly sequential IO when streaming data during bootstrap or repair, which\n# can lead to saturating the network connection and degrading rpc performance.\n# When unset, the default is 200 Mbps or 25 MB/s.\n# stream_throughput_outbound_megabits_per_sec: 200\n\n# Throttles all streaming file transfer between the datacenters,\n# this setting allows users to throttle inter dc stream throughput in addition\n# to throttling all network stream traffic as configured with\n# stream_throughput_outbound_megabits_per_sec\n# When unset, the default is 200 Mbps or 25 MB/s\n# inter_dc_stream_throughput_outbound_megabits_per_sec: 200\n\n# How long the coordinator should wait for read operations to complete\nread_request_timeout_in_ms: 5000\n# How long the coordinator should wait for seq or index scans to complete\nrange_request_timeout_in_ms: 10000\n# How long the coordinator should wait for writes to complete\nwrite_request_timeout_in_ms: 2000\n# How long the coordinator should wait for counter writes to complete\ncounter_write_request_timeout_in_ms: 5000\n# How long a coordinator should continue to retry a CAS operation\n# that contends with other proposals for the same row\ncas_contention_timeout_in_ms: 1000\n# How long the coordinator should wait for truncates to complete\n# (This can be much longer, because unless auto_snapshot is disabled\n# we need to flush first so we can snapshot before removing the data.)\ntruncate_request_timeout_in_ms: 60000\n# The default timeout for other, miscellaneous operations\nrequest_timeout_in_ms: 10000\n\n# Enable operation timeout information exchange between nodes to accurately\n# measure request timeouts.  If disabled, replicas will assume that requests\n# were forwarded to them instantly by the coordinator, which means that\n# under overload conditions we will waste that much extra time processing \n# already-timed-out requests.\n#\n# Warning: before enabling this property make sure to ntp is installed\n# and the times are synchronized between the nodes.\ncross_node_timeout: false\n\n# Set socket timeout for streaming operation.\n# The stream session is failed if no data is received by any of the\n# participants within that period.\n# Default value is 3600000, which means streams timeout after an hour.\n# streaming_socket_timeout_in_ms: 3600000\n\n# phi value that must be reached for a host to be marked down.\n# most users should never need to adjust this.\n# phi_convict_threshold: 8\n\n# endpoint_snitch -- Set this to a class that implements\n# IEndpointSnitch.  The snitch has two functions:\n# - it teaches Cassandra enough about your network topology to route\n#   requests efficiently\n# - it allows Cassandra to spread replicas around your cluster to avoid\n#   correlated failures. It does this by grouping machines into\n#   \"datacenters\" and \"racks.\"  Cassandra will do its best not to have\n#   more than one replica on the same \"rack\" (which may not actually\n#   be a physical location)\n#\n# IF YOU CHANGE THE SNITCH AFTER DATA IS INSERTED INTO THE CLUSTER,\n# YOU MUST RUN A FULL REPAIR, SINCE THE SNITCH AFFECTS WHERE REPLICAS\n# ARE PLACED.\n#\n# IF THE RACK A REPLICA IS PLACED IN CHANGES AFTER THE REPLICA HAS BEEN\n# ADDED TO A RING, THE NODE MUST BE DECOMMISSIONED AND REBOOTSTRAPPED.\n#\n# Out of the box, Cassandra provides\n#  - SimpleSnitch:\n#    Treats Strategy order as proximity. This can improve cache\n#    locality when disabling read repair.  Only appropriate for\n#    single-datacenter deployments.\n#  - GossipingPropertyFileSnitch\n#    This should be your go-to snitch for production use.  The rack\n#    and datacenter for the local node are defined in\n#    cassandra-rackdc.properties and propagated to other nodes via\n#    gossip.  If cassandra-topology.properties exists, it is used as a\n#    fallback, allowing migration from the PropertyFileSnitch.\n#  - PropertyFileSnitch:\n#    Proximity is determined by rack and data center, which are\n#    explicitly configured in cassandra-topology.properties.\n#  - Ec2Snitch:\n#    Appropriate for EC2 deployments in a single Region. Loads Region\n#    and Availability Zone information from the EC2 API. The Region is\n#    treated as the datacenter, and the Availability Zone as the rack.\n#    Only private IPs are used, so this will not work across multiple\n#    Regions.\n#  - Ec2MultiRegionSnitch:\n#    Uses public IPs as broadcast_address to allow cross-region\n#    connectivity.  (Thus, you should set seed addresses to the public\n#    IP as well.) You will need to open the storage_port or\n#    ssl_storage_port on the public IP firewall.  (For intra-Region\n#    traffic, Cassandra will switch to the private IP after\n#    establishing a connection.)\n#  - RackInferringSnitch:\n#    Proximity is determined by rack and data center, which are\n#    assumed to correspond to the 3rd and 2nd octet of each node's IP\n#    address, respectively.  Unless this happens to match your\n#    deployment conventions, this is best used as an example of\n#    writing a custom Snitch class and is provided in that spirit.\n#\n# You can use a custom Snitch by setting this to the full class name\n# of the snitch, which will be assumed to be on your classpath.\nendpoint_snitch: GossipingPropertyFileSnitch\n\n# controls how often to perform the more expensive part of host score\n# calculation\ndynamic_snitch_update_interval_in_ms: 100 \n# controls how often to reset all host scores, allowing a bad host to\n# possibly recover\ndynamic_snitch_reset_interval_in_ms: 600000\n# if set greater than zero and read_repair_chance is \u003c 1.0, this will allow\n# 'pinning' of replicas to hosts in order to increase cache capacity.\n# The badness threshold will control how much worse the pinned host has to be\n# before the dynamic snitch will prefer other replicas over it.  This is\n# expressed as a double which represents a percentage.  Thus, a value of\n# 0.2 means Cassandra would continue to prefer the static snitch values\n# until the pinned host was 20% worse than the fastest.\ndynamic_snitch_badness_threshold: 0.1\n\n# request_scheduler -- Set this to a class that implements\n# RequestScheduler, which will schedule incoming client requests\n# according to the specific policy. This is useful for multi-tenancy\n# with a single Cassandra cluster.\n# NOTE: This is specifically for requests from the client and does\n# not affect inter node communication.\n# org.apache.cassandra.scheduler.NoScheduler - No scheduling takes place\n# org.apache.cassandra.scheduler.RoundRobinScheduler - Round robin of\n# client requests to a node with a separate queue for each\n# request_scheduler_id. The scheduler is further customized by\n# request_scheduler_options as described below.\nrequest_scheduler: org.apache.cassandra.scheduler.NoScheduler\n\n# Scheduler Options vary based on the type of scheduler\n# NoScheduler - Has no options\n# RoundRobin\n#  - throttle_limit -- The throttle_limit is the number of in-flight\n#                      requests per client.  Requests beyond \n#                      that limit are queued up until\n#                      running requests can complete.\n#                      The value of 80 here is twice the number of\n#                      concurrent_reads + concurrent_writes.\n#  - default_weight -- default_weight is optional and allows for\n#                      overriding the default which is 1.\n#  - weights -- Weights are optional and will default to 1 or the\n#               overridden default_weight. The weight translates into how\n#               many requests are handled during each turn of the\n#               RoundRobin, based on the scheduler id.\n#\n# request_scheduler_options:\n#    throttle_limit: 80\n#    default_weight: 5\n#    weights:\n#      Keyspace1: 1\n#      Keyspace2: 5\n\n# request_scheduler_id -- An identifier based on which to perform\n# the request scheduling. Currently the only valid option is keyspace.\n# request_scheduler_id: keyspace\n\n# Enable or disable inter-node encryption\n# JVM defaults for supported SSL socket protocols and cipher suites can\n# be replaced using custom encryption options. This is not recommended\n# unless you have policies in place that dictate certain settings, or\n# need to disable vulnerable ciphers or protocols in case the JVM cannot\n# be updated.\n# FIPS compliant settings can be configured at JVM level and should not\n# involve changing encryption settings here:\n# https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/FIPS.html\n# NOTE: No custom encryption options are enabled at the moment\n# The available internode options are : all, none, dc, rack\n#\n# If set to dc cassandra will encrypt the traffic between the DCs\n# If set to rack cassandra will encrypt the traffic between the racks\n#\n# The passwords used in these options must match the passwords used when generating\n# the keystore and truststore.  For instructions on generating these files, see:\n# http://download.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html#CreateKeystore\n#\nserver_encryption_options:\n    internode_encryption: none\n    keystore: conf/.keystore\n    keystore_password: cassandra\n    truststore: conf/.truststore\n    truststore_password: cassandra\n    # More advanced defaults below:\n    # protocol: TLS\n    # algorithm: SunX509\n    # store_type: JKS\n    # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n    # require_client_auth: false\n    # require_endpoint_verification: false\n\n# enable or disable client/server encryption.\nclient_encryption_options:\n    enabled: false\n    # If enabled and optional is set to true encrypted and unencrypted connections are handled.\n    optional: false\n    keystore: conf/.keystore\n    keystore_password: cassandra\n    # require_client_auth: false\n    # Set trustore and truststore_password if require_client_auth is true\n    # truststore: conf/.truststore\n    # truststore_password: cassandra\n    # More advanced defaults below:\n    # protocol: TLS\n    # algorithm: SunX509\n    # store_type: JKS\n    # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n\n# internode_compression controls whether traffic between nodes is\n# compressed.\n# can be:  all  - all traffic is compressed\n#          dc   - traffic between different datacenters is compressed\n#          none - nothing is compressed.\ninternode_compression: dc\n\n# Enable or disable tcp_nodelay for inter-dc communication.\n# Disabling it will result in larger (but fewer) network packets being sent,\n# reducing overhead from the TCP protocol itself, at the cost of increasing\n# latency if you block for cross-datacenter responses.\ninter_dc_tcp_nodelay: false\n\n# TTL for different trace types used during logging of the repair process.\ntracetype_query_ttl: 86400\ntracetype_repair_ttl: 604800\n\n# UDFs (user defined functions) are disabled by default.\n# As of Cassandra 3.0 there is a sandbox in place that should prevent execution of evil code.\nenable_user_defined_functions: false\n\n# Enables scripted UDFs (JavaScript UDFs).\n# Java UDFs are always enabled, if enable_user_defined_functions is true.\n# Enable this option to be able to use UDFs with \"language javascript\" or any custom JSR-223 provider.\n# This option has no effect, if enable_user_defined_functions is false.\nenable_scripted_user_defined_functions: false\n\n# The default Windows kernel timer and scheduling resolution is 15.6ms for power conservation.\n# Lowering this value on Windows can provide much tighter latency and better throughput, however\n# some virtualized environments may see a negative performance impact from changing this setting\n# below their system default. The sysinternals 'clockres' tool can confirm your system's default\n# setting.\nwindows_timer_interval: 1\n\n\n# Enables encrypting data at-rest (on disk). Different key providers can be plugged in, but the default reads from\n# a JCE-style keystore. A single keystore can hold multiple keys, but the one referenced by\n# the \"key_alias\" is the only key that will be used for encrypt opertaions; previously used keys\n# can still (and should!) be in the keystore and will be used on decrypt operations\n# (to handle the case of key rotation).\n#\n# It is strongly recommended to download and install Java Cryptography Extension (JCE)\n# Unlimited Strength Jurisdiction Policy Files for your version of the JDK.\n# (current link: http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html)\n#\n# Currently, only the following file types are supported for transparent data encryption, although\n# more are coming in future cassandra releases: commitlog, hints\ntransparent_data_encryption_options:\n    enabled: false\n    chunk_length_kb: 64\n    cipher: AES/CBC/PKCS5Padding\n    key_alias: testing:1\n    # CBC IV length for AES needs to be 16 bytes (which is also the default size)\n    # iv_length: 16\n    key_provider: \n      - class_name: org.apache.cassandra.security.JKSKeyProvider\n        parameters: \n          - keystore: conf/.keystore\n            keystore_password: cassandra\n            store_type: JCEKS\n            key_password: cassandra\n\n\n#####################\n# SAFETY THRESHOLDS #\n#####################\n\n# When executing a scan, within or across a partition, we need to keep the\n# tombstones seen in memory so we can return them to the coordinator, which\n# will use them to make sure other replicas also know about the deleted rows.\n# With workloads that generate a lot of tombstones, this can cause performance\n# problems and even exaust the server heap.\n# (http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets)\n# Adjust the thresholds here if you understand the dangers and want to\n# scan more tombstones anyway.  These thresholds may also be adjusted at runtime\n# using the StorageService mbean.\ntombstone_warn_threshold: 1000\ntombstone_failure_threshold: 100000\n\n# Log WARN on any batch size exceeding this value. 5kb per batch by default.\n# Caution should be taken on increasing the size of this threshold as it can lead to node instability.\nbatch_size_warn_threshold_in_kb: 5\n\n# Fail any batch exceeding this value. 50kb (10x warn threshold) by default.\nbatch_size_fail_threshold_in_kb: 50\n\n# Log WARN on any batches not of type LOGGED than span across more partitions than this limit\nunlogged_batch_across_partitions_warn_threshold: 10\n\n# Log a warning when compacting partitions larger than this value\ncompaction_large_partition_warning_threshold_mb: 100\n\n# GC Pauses greater than gc_warn_threshold_in_ms will be logged at WARN level\n# Adjust the threshold based on your application throughput requirement\n# By default, Cassandra logs GC Pauses greater than 200 ms at INFO level\ngc_warn_threshold_in_ms: 1000\n\nauto_bootstrap: false\n",
                            "vars.%": "3",
                            "vars.cluster_name": "opsschool",
                            "vars.seeds": "172.31.48.41,172.31.77.64,172.31.6.131",
                            "vars.server_ip": "172.31.77.64"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.template"
                },
                "data.template_file.cassandra_yaml.2": {
                    "type": "template_file",
                    "depends_on": [
                        "aws_instance.cassandra.*"
                    ],
                    "primary": {
                        "id": "b6412a44a791cdec56f2ed490c452f4b721fd159839b83223e9c85c52dab408c",
                        "attributes": {
                            "id": "b6412a44a791cdec56f2ed490c452f4b721fd159839b83223e9c85c52dab408c",
                            "rendered": "# Cassandra storage config YAML \n\n# NOTE:\n#   See http://wiki.apache.org/cassandra/StorageConfiguration for\n#   full explanations of configuration directives\n# /NOTE\n\n# The name of the cluster. This is mainly used to prevent machines in\n# one logical cluster from joining another.\ncluster_name: 'opsschool'\n\n# This defines the number of tokens randomly assigned to this node on the ring\n# The more tokens, relative to other nodes, the larger the proportion of data\n# that this node will store. You probably want all nodes to have the same number\n# of tokens assuming they have equal hardware capability.\n#\n# If you leave this unspecified, Cassandra will use the default of 1 token for legacy compatibility,\n# and will use the initial_token as described below.\n#\n# Specifying initial_token will override this setting on the node's initial start,\n# on subsequent starts, this setting will apply even if initial token is set.\n#\n# If you already have a cluster with 1 token per node, and wish to migrate to \n# multiple tokens per node, see http://wiki.apache.org/cassandra/Operations\nnum_tokens: 256\n\n# Triggers automatic allocation of num_tokens tokens for this node. The allocation\n# algorithm attempts to choose tokens in a way that optimizes replicated load over\n# the nodes in the datacenter for the replication strategy used by the specified\n# keyspace.\n#\n# The load assigned to each node will be close to proportional to its number of\n# vnodes.\n#\n# Only supported with the Murmur3Partitioner.\n# allocate_tokens_for_keyspace: KEYSPACE\n\n# initial_token allows you to specify tokens manually.  While you can use # it with\n# vnodes (num_tokens \u003e 1, above) -- in which case you should provide a \n# comma-separated list -- it's primarily used when adding nodes # to legacy clusters \n# that do not have vnodes enabled.\n# initial_token:\n\n# See http://wiki.apache.org/cassandra/HintedHandoff\n# May either be \"true\" or \"false\" to enable globally\nhinted_handoff_enabled: true\n# When hinted_handoff_enabled is true, a black list of data centers that will not\n# perform hinted handoff\n#hinted_handoff_disabled_datacenters:\n#    - DC1\n#    - DC2\n# this defines the maximum amount of time a dead host will have hints\n# generated.  After it has been dead this long, new hints for it will not be\n# created until it has been seen alive and gone down again.\nmax_hint_window_in_ms: 10800000 # 3 hours\n\n# Maximum throttle in KBs per second, per delivery thread.  This will be\n# reduced proportionally to the number of nodes in the cluster.  (If there\n# are two nodes in the cluster, each delivery thread will use the maximum\n# rate; if there are three, each will throttle to half of the maximum,\n# since we expect two nodes to be delivering hints simultaneously.)\nhinted_handoff_throttle_in_kb: 1024\n\n# Number of threads with which to deliver hints;\n# Consider increasing this number when you have multi-dc deployments, since\n# cross-dc handoff tends to be slower\nmax_hints_delivery_threads: 2\n\n# Directory where Cassandra should store hints.\n# If not set, the default directory is $CASSANDRA_HOME/data/hints.\n# hints_directory: /var/lib/cassandra/hints\n\n# How often hints should be flushed from the internal buffers to disk.\n# Will *not* trigger fsync.\nhints_flush_period_in_ms: 10000\n\n# Maximum size for a single hints file, in megabytes.\nmax_hints_file_size_in_mb: 128\n\n# Compression to apply to the hint files. If omitted, hints files\n# will be written uncompressed. LZ4, Snappy, and Deflate compressors\n# are supported.\n#hints_compression:\n#   - class_name: LZ4Compressor\n#     parameters:\n#         -\n\n# Maximum throttle in KBs per second, total. This will be\n# reduced proportionally to the number of nodes in the cluster.\nbatchlog_replay_throttle_in_kb: 1024\n\n# Authentication backend, implementing IAuthenticator; used to identify users\n# Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthenticator,\n# PasswordAuthenticator}.\n#\n# - AllowAllAuthenticator performs no checks - set it to disable authentication.\n# - PasswordAuthenticator relies on username/password pairs to authenticate\n#   users. It keeps usernames and hashed passwords in system_auth.credentials table.\n#   Please increase system_auth keyspace replication factor if you use this authenticator.\n#   If using PasswordAuthenticator, CassandraRoleManager must also be used (see below)\nauthenticator: AllowAllAuthenticator\n\n# Authorization backend, implementing IAuthorizer; used to limit access/provide permissions\n# Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthorizer,\n# CassandraAuthorizer}.\n#\n# - AllowAllAuthorizer allows any action to any user - set it to disable authorization.\n# - CassandraAuthorizer stores permissions in system_auth.permissions table. Please\n#   increase system_auth keyspace replication factor if you use this authorizer.\nauthorizer: AllowAllAuthorizer\n\n# Part of the Authentication \u0026 Authorization backend, implementing IRoleManager; used\n# to maintain grants and memberships between roles.\n# Out of the box, Cassandra provides org.apache.cassandra.auth.CassandraRoleManager,\n# which stores role information in the system_auth keyspace. Most functions of the\n# IRoleManager require an authenticated login, so unless the configured IAuthenticator\n# actually implements authentication, most of this functionality will be unavailable.\n#\n# - CassandraRoleManager stores role data in the system_auth keyspace. Please\n#   increase system_auth keyspace replication factor if you use this role manager.\nrole_manager: CassandraRoleManager\n\n# Validity period for roles cache (fetching granted roles can be an expensive\n# operation depending on the role manager, CassandraRoleManager is one example)\n# Granted roles are cached for authenticated sessions in AuthenticatedUser and\n# after the period specified here, become eligible for (async) reload.\n# Defaults to 2000, set to 0 to disable caching entirely.\n# Will be disabled automatically for AllowAllAuthenticator.\nroles_validity_in_ms: 2000\n\n# Refresh interval for roles cache (if enabled).\n# After this interval, cache entries become eligible for refresh. Upon next\n# access, an async reload is scheduled and the old value returned until it\n# completes. If roles_validity_in_ms is non-zero, then this must be\n# also.\n# Defaults to the same value as roles_validity_in_ms.\n# roles_update_interval_in_ms: 2000\n\n# Validity period for permissions cache (fetching permissions can be an\n# expensive operation depending on the authorizer, CassandraAuthorizer is\n# one example). Defaults to 2000, set to 0 to disable.\n# Will be disabled automatically for AllowAllAuthorizer.\npermissions_validity_in_ms: 2000\n\n# Refresh interval for permissions cache (if enabled).\n# After this interval, cache entries become eligible for refresh. Upon next\n# access, an async reload is scheduled and the old value returned until it\n# completes. If permissions_validity_in_ms is non-zero, then this must be\n# also.\n# Defaults to the same value as permissions_validity_in_ms.\n# permissions_update_interval_in_ms: 2000\n\n# Validity period for credentials cache. This cache is tightly coupled to\n# the provided PasswordAuthenticator implementation of IAuthenticator. If\n# another IAuthenticator implementation is configured, this cache will not\n# be automatically used and so the following settings will have no effect.\n# Please note, credentials are cached in their encrypted form, so while\n# activating this cache may reduce the number of queries made to the\n# underlying table, it may not  bring a significant reduction in the\n# latency of individual authentication attempts.\n# Defaults to 2000, set to 0 to disable credentials caching.\ncredentials_validity_in_ms: 2000\n\n# Refresh interval for credentials cache (if enabled).\n# After this interval, cache entries become eligible for refresh. Upon next\n# access, an async reload is scheduled and the old value returned until it\n# completes. If credentials_validity_in_ms is non-zero, then this must be\n# also.\n# Defaults to the same value as credentials_validity_in_ms.\n# credentials_update_interval_in_ms: 2000\n\n# The partitioner is responsible for distributing groups of rows (by\n# partition key) across nodes in the cluster.  You should leave this\n# alone for new clusters.  The partitioner can NOT be changed without\n# reloading all data, so when upgrading you should set this to the\n# same partitioner you were already using.\n#\n# Besides Murmur3Partitioner, partitioners included for backwards\n# compatibility include RandomPartitioner, ByteOrderedPartitioner, and\n# OrderPreservingPartitioner.\n#\npartitioner: org.apache.cassandra.dht.Murmur3Partitioner\n\n# Directories where Cassandra should store data on disk.  Cassandra\n# will spread data evenly across them, subject to the granularity of\n# the configured compaction strategy.\n# If not set, the default directory is $CASSANDRA_HOME/data/data.\ndata_file_directories:\n    - /var/lib/cassandra/data\n\n# commit log.  when running on magnetic HDD, this should be a\n# separate spindle than the data directories.\n# If not set, the default directory is $CASSANDRA_HOME/data/commitlog.\ncommitlog_directory: /var/lib/cassandra/commitlog\n\n# policy for data disk failures:\n# die: shut down gossip and client transports and kill the JVM for any fs errors or\n#      single-sstable errors, so the node can be replaced.\n# stop_paranoid: shut down gossip and client transports even for single-sstable errors,\n#                kill the JVM for errors during startup.\n# stop: shut down gossip and client transports, leaving the node effectively dead, but\n#       can still be inspected via JMX, kill the JVM for errors during startup.\n# best_effort: stop using the failed disk and respond to requests based on\n#              remaining available sstables.  This means you WILL see obsolete\n#              data at CL.ONE!\n# ignore: ignore fatal errors and let requests fail, as in pre-1.2 Cassandra\ndisk_failure_policy: stop\n\n# policy for commit disk failures:\n# die: shut down gossip and Thrift and kill the JVM, so the node can be replaced.\n# stop: shut down gossip and Thrift, leaving the node effectively dead, but\n#       can still be inspected via JMX.\n# stop_commit: shutdown the commit log, letting writes collect but\n#              continuing to service reads, as in pre-2.0.5 Cassandra\n# ignore: ignore fatal errors and let the batches fail\ncommit_failure_policy: stop\n\n# Maximum size of the native protocol prepared statement cache\n#\n# Valid values are either \"auto\" (omitting the value) or a value greater 0.\n#\n# Note that specifying a too large value will result in long running GCs and possbily\n# out-of-memory errors. Keep the value at a small fraction of the heap.\n#\n# If you constantly see \"prepared statements discarded in the last minute because\n# cache limit reached\" messages, the first step is to investigate the root cause\n# of these messages and check whether prepared statements are used correctly -\n# i.e. use bind markers for variable parts.\n#\n# Do only change the default value, if you really have more prepared statements than\n# fit in the cache. In most cases it is not neccessary to change this value.\n# Constantly re-preparing statements is a performance penalty.\n#\n# Default value (\"auto\") is 1/256th of the heap or 10MB, whichever is greater\nprepared_statements_cache_size_mb:\n\n# Maximum size of the Thrift prepared statement cache\n#\n# If you do not use Thrift at all, it is safe to leave this value at \"auto\".\n#\n# See description of 'prepared_statements_cache_size_mb' above for more information.\n#\n# Default value (\"auto\") is 1/256th of the heap or 10MB, whichever is greater\nthrift_prepared_statements_cache_size_mb:\n\n# Maximum size of the key cache in memory.\n#\n# Each key cache hit saves 1 seek and each row cache hit saves 2 seeks at the\n# minimum, sometimes more. The key cache is fairly tiny for the amount of\n# time it saves, so it's worthwhile to use it at large numbers.\n# The row cache saves even more time, but must contain the entire row,\n# so it is extremely space-intensive. It's best to only use the\n# row cache if you have hot rows or static rows.\n#\n# NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.\n#\n# Default value is empty to make it \"auto\" (min(5% of Heap (in MB), 100MB)). Set to 0 to disable key cache.\nkey_cache_size_in_mb:\n\n# Duration in seconds after which Cassandra should\n# save the key cache. Caches are saved to saved_caches_directory as\n# specified in this configuration file.\n#\n# Saved caches greatly improve cold-start speeds, and is relatively cheap in\n# terms of I/O for the key cache. Row cache saving is much more expensive and\n# has limited use.\n#\n# Default is 14400 or 4 hours.\nkey_cache_save_period: 14400\n\n# Number of keys from the key cache to save\n# Disabled by default, meaning all keys are going to be saved\n# key_cache_keys_to_save: 100\n\n# Row cache implementation class name.\n# Available implementations:\n#   org.apache.cassandra.cache.OHCProvider                Fully off-heap row cache implementation (default).\n#   org.apache.cassandra.cache.SerializingCacheProvider   This is the row cache implementation availabile\n#                                                         in previous releases of Cassandra.\n# row_cache_class_name: org.apache.cassandra.cache.OHCProvider\n\n# Maximum size of the row cache in memory.\n# Please note that OHC cache implementation requires some additional off-heap memory to manage\n# the map structures and some in-flight memory during operations before/after cache entries can be\n# accounted against the cache capacity. This overhead is usually small compared to the whole capacity.\n# Do not specify more memory that the system can afford in the worst usual situation and leave some\n# headroom for OS block level cache. Do never allow your system to swap.\n#\n# Default value is 0, to disable row caching.\nrow_cache_size_in_mb: 0\n\n# Duration in seconds after which Cassandra should save the row cache.\n# Caches are saved to saved_caches_directory as specified in this configuration file.\n#\n# Saved caches greatly improve cold-start speeds, and is relatively cheap in\n# terms of I/O for the key cache. Row cache saving is much more expensive and\n# has limited use.\n#\n# Default is 0 to disable saving the row cache.\nrow_cache_save_period: 0\n\n# Number of keys from the row cache to save.\n# Specify 0 (which is the default), meaning all keys are going to be saved\n# row_cache_keys_to_save: 100\n\n# Maximum size of the counter cache in memory.\n#\n# Counter cache helps to reduce counter locks' contention for hot counter cells.\n# In case of RF = 1 a counter cache hit will cause Cassandra to skip the read before\n# write entirely. With RF \u003e 1 a counter cache hit will still help to reduce the duration\n# of the lock hold, helping with hot counter cell updates, but will not allow skipping\n# the read entirely. Only the local (clock, count) tuple of a counter cell is kept\n# in memory, not the whole counter, so it's relatively cheap.\n#\n# NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.\n#\n# Default value is empty to make it \"auto\" (min(2.5% of Heap (in MB), 50MB)). Set to 0 to disable counter cache.\n# NOTE: if you perform counter deletes and rely on low gcgs, you should disable the counter cache.\ncounter_cache_size_in_mb:\n\n# Duration in seconds after which Cassandra should\n# save the counter cache (keys only). Caches are saved to saved_caches_directory as\n# specified in this configuration file.\n#\n# Default is 7200 or 2 hours.\ncounter_cache_save_period: 7200\n\n# Number of keys from the counter cache to save\n# Disabled by default, meaning all keys are going to be saved\n# counter_cache_keys_to_save: 100\n\n# saved caches\n# If not set, the default directory is $CASSANDRA_HOME/data/saved_caches.\nsaved_caches_directory: /var/lib/cassandra/saved_caches\n\n# commitlog_sync may be either \"periodic\" or \"batch.\" \n# \n# When in batch mode, Cassandra won't ack writes until the commit log\n# has been fsynced to disk.  It will wait\n# commitlog_sync_batch_window_in_ms milliseconds between fsyncs.\n# This window should be kept short because the writer threads will\n# be unable to do extra work while waiting.  (You may need to increase\n# concurrent_writes for the same reason.)\n#\n# commitlog_sync: batch\n# commitlog_sync_batch_window_in_ms: 2\n#\n# the other option is \"periodic\" where writes may be acked immediately\n# and the CommitLog is simply synced every commitlog_sync_period_in_ms\n# milliseconds. \ncommitlog_sync: periodic\ncommitlog_sync_period_in_ms: 10000\n\n# The size of the individual commitlog file segments.  A commitlog\n# segment may be archived, deleted, or recycled once all the data\n# in it (potentially from each columnfamily in the system) has been\n# flushed to sstables.\n#\n# The default size is 32, which is almost always fine, but if you are\n# archiving commitlog segments (see commitlog_archiving.properties),\n# then you probably want a finer granularity of archiving; 8 or 16 MB\n# is reasonable.\n# Max mutation size is also configurable via max_mutation_size_in_kb setting in\n# cassandra.yaml. The default is half the size commitlog_segment_size_in_mb * 1024.\n#\n# NOTE: If max_mutation_size_in_kb is set explicitly then commitlog_segment_size_in_mb must\n# be set to at least twice the size of max_mutation_size_in_kb / 1024\n#\ncommitlog_segment_size_in_mb: 32\n\n# Compression to apply to the commit log. If omitted, the commit log\n# will be written uncompressed.  LZ4, Snappy, and Deflate compressors\n# are supported.\n#commitlog_compression:\n#   - class_name: LZ4Compressor\n#     parameters:\n#         -\n\n# any class that implements the SeedProvider interface and has a\n# constructor that takes a Map\u003cString, String\u003e of parameters will do.\nseed_provider:\n    # Addresses of hosts that are deemed contact points. \n    # Cassandra nodes use this list of hosts to find each other and learn\n    # the topology of the ring.  You must change this if you are running\n    # multiple nodes!\n    - class_name: org.apache.cassandra.locator.SimpleSeedProvider\n      parameters:\n          # seeds is actually a comma-delimited list of addresses.\n          # Ex: \"\u003cip1\u003e,\u003cip2\u003e,\u003cip3\u003e\"\n          - seeds: \"172.31.48.41,172.31.77.64,172.31.6.131\"\n\n# For workloads with more data than can fit in memory, Cassandra's\n# bottleneck will be reads that need to fetch data from\n# disk. \"concurrent_reads\" should be set to (16 * number_of_drives) in\n# order to allow the operations to enqueue low enough in the stack\n# that the OS and drives can reorder them. Same applies to\n# \"concurrent_counter_writes\", since counter writes read the current\n# values before incrementing and writing them back.\n#\n# On the other hand, since writes are almost never IO bound, the ideal\n# number of \"concurrent_writes\" is dependent on the number of cores in\n# your system; (8 * number_of_cores) is a good rule of thumb.\nconcurrent_reads: 32\nconcurrent_writes: 32\nconcurrent_counter_writes: 32\n\n# For materialized view writes, as there is a read involved, so this should\n# be limited by the less of concurrent reads or concurrent writes.\nconcurrent_materialized_view_writes: 32\n\n# Maximum memory to use for sstable chunk cache and buffer pooling.\n# 32MB of this are reserved for pooling buffers, the rest is used as an\n# cache that holds uncompressed sstable chunks.\n# Defaults to the smaller of 1/4 of heap or 512MB. This pool is allocated off-heap,\n# so is in addition to the memory allocated for heap. The cache also has on-heap\n# overhead which is roughly 128 bytes per chunk (i.e. 0.2% of the reserved size\n# if the default 64k chunk size is used).\n# Memory is only allocated when needed.\n# file_cache_size_in_mb: 512\n\n# Flag indicating whether to allocate on or off heap when the sstable buffer\n# pool is exhausted, that is when it has exceeded the maximum memory\n# file_cache_size_in_mb, beyond which it will not cache buffers but allocate on request.\n\n# buffer_pool_use_heap_if_exhausted: true\n\n# The strategy for optimizing disk read\n# Possible values are:\n# ssd (for solid state disks, the default)\n# spinning (for spinning disks)\n# disk_optimization_strategy: ssd\n\n# Total permitted memory to use for memtables. Cassandra will stop\n# accepting writes when the limit is exceeded until a flush completes,\n# and will trigger a flush based on memtable_cleanup_threshold\n# If omitted, Cassandra will set both to 1/4 the size of the heap.\n# memtable_heap_space_in_mb: 2048\n# memtable_offheap_space_in_mb: 2048\n\n# Ratio of occupied non-flushing memtable size to total permitted size\n# that will trigger a flush of the largest memtable. Larger mct will\n# mean larger flushes and hence less compaction, but also less concurrent\n# flush activity which can make it difficult to keep your disks fed\n# under heavy write load.\n#\n# memtable_cleanup_threshold defaults to 1 / (memtable_flush_writers + 1)\n# memtable_cleanup_threshold: 0.11\n\n# Specify the way Cassandra allocates and manages memtable memory.\n# Options are:\n#   heap_buffers:    on heap nio buffers\n#   offheap_buffers: off heap (direct) nio buffers\n#   offheap_objects: off heap objects\nmemtable_allocation_type: heap_buffers\n\n# Total space to use for commit logs on disk.\n#\n# If space gets above this value, Cassandra will flush every dirty CF\n# in the oldest segment and remove it.  So a small total commitlog space\n# will tend to cause more flush activity on less-active columnfamilies.\n#\n# The default value is the smaller of 8192, and 1/4 of the total space\n# of the commitlog volume.\n#\n# commitlog_total_space_in_mb: 8192\n\n# This sets the amount of memtable flush writer threads.  These will\n# be blocked by disk io, and each one will hold a memtable in memory\n# while blocked.\n#\n# memtable_flush_writers defaults to one per data_file_directory.\n#\n# If your data directories are backed by SSD, you can increase this, but\n# avoid having memtable_flush_writers * data_file_directories \u003e number of cores\n#memtable_flush_writers: 1\n\n# A fixed memory pool size in MB for for SSTable index summaries. If left\n# empty, this will default to 5% of the heap size. If the memory usage of\n# all index summaries exceeds this limit, SSTables with low read rates will\n# shrink their index summaries in order to meet this limit.  However, this\n# is a best-effort process. In extreme conditions Cassandra may need to use\n# more than this amount of memory.\nindex_summary_capacity_in_mb:\n\n# How frequently index summaries should be resampled.  This is done\n# periodically to redistribute memory from the fixed-size pool to sstables\n# proportional their recent read rates.  Setting to -1 will disable this\n# process, leaving existing index summaries at their current sampling level.\nindex_summary_resize_interval_in_minutes: 60\n\n# Whether to, when doing sequential writing, fsync() at intervals in\n# order to force the operating system to flush the dirty\n# buffers. Enable this to avoid sudden dirty buffer flushing from\n# impacting read latencies. Almost always a good idea on SSDs; not\n# necessarily on platters.\ntrickle_fsync: false\ntrickle_fsync_interval_in_kb: 10240\n\n# TCP port, for commands and data\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\nstorage_port: 7000\n\n# SSL port, for encrypted communication.  Unused unless enabled in\n# encryption_options\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\nssl_storage_port: 7001\n\n# Address or interface to bind to and tell other Cassandra nodes to connect to.\n# You _must_ change this if you want multiple nodes to be able to communicate!\n#\n# Set listen_address OR listen_interface, not both. Interfaces must correspond\n# to a single address, IP aliasing is not supported.\n#\n# Leaving it blank leaves it up to InetAddress.getLocalHost(). This\n# will always do the Right Thing _if_ the node is properly configured\n# (hostname, name resolution, etc), and the Right Thing is to use the\n# address associated with the hostname (it might not be).\n#\n# Setting listen_address to 0.0.0.0 is always wrong.\n#\n# If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\n# you can specify which should be chosen using listen_interface_prefer_ipv6. If false the first ipv4\n# address will be used. If true the first ipv6 address will be used. Defaults to false preferring\n# ipv4. If there is only one address it will be selected regardless of ipv4/ipv6.\nlisten_address: 172.31.6.131\n# listen_interface: eth0\n# listen_interface_prefer_ipv6: false\n\n# Address to broadcast to other Cassandra nodes\n# Leaving this blank will set it to the same value as listen_address\n# broadcast_address: 1.2.3.4\n\n# When using multiple physical network interfaces, set this\n# to true to listen on broadcast_address in addition to\n# the listen_address, allowing nodes to communicate in both\n# interfaces.\n# Ignore this property if the network configuration automatically\n# routes  between the public and private networks such as EC2.\n# listen_on_broadcast_address: false\n\n# Internode authentication backend, implementing IInternodeAuthenticator;\n# used to allow/disallow connections from peer nodes.\n# internode_authenticator: org.apache.cassandra.auth.AllowAllInternodeAuthenticator\n\n# Whether to start the native transport server.\n# Please note that the address on which the native transport is bound is the\n# same as the rpc_address. The port however is different and specified below.\nstart_native_transport: true\n# port for the CQL native transport to listen for clients on\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\nnative_transport_port: 9042\n# Enabling native transport encryption in client_encryption_options allows you to either use\n# encryption for the standard port or to use a dedicated, additional port along with the unencrypted\n# standard native_transport_port.\n# Enabling client encryption and keeping native_transport_port_ssl disabled will use encryption\n# for native_transport_port. Setting native_transport_port_ssl to a different value\n# from native_transport_port will use encryption for native_transport_port_ssl while\n# keeping native_transport_port unencrypted.\n# native_transport_port_ssl: 9142\n# The maximum threads for handling requests when the native transport is used.\n# This is similar to rpc_max_threads though the default differs slightly (and\n# there is no native_transport_min_threads, idle threads will always be stopped\n# after 30 seconds).\n# native_transport_max_threads: 128\n#\n# The maximum size of allowed frame. Frame (requests) larger than this will\n# be rejected as invalid. The default is 256MB.\n# native_transport_max_frame_size_in_mb: 256\n\n# The maximum number of concurrent client connections.\n# The default is -1, which means unlimited.\n# native_transport_max_concurrent_connections: -1\n\n# The maximum number of concurrent client connections per source ip.\n# The default is -1, which means unlimited.\n# native_transport_max_concurrent_connections_per_ip: -1\n\n# Whether to start the thrift rpc server.\nstart_rpc: true\n\n# The address or interface to bind the Thrift RPC service and native transport\n# server to.\n#\n# Set rpc_address OR rpc_interface, not both. Interfaces must correspond\n# to a single address, IP aliasing is not supported.\n#\n# Leaving rpc_address blank has the same effect as on listen_address\n# (i.e. it will be based on the configured hostname of the node).\n#\n# Note that unlike listen_address, you can specify 0.0.0.0, but you must also\n# set broadcast_rpc_address to a value other than 0.0.0.0.\n#\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\n#\n# If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\n# you can specify which should be chosen using rpc_interface_prefer_ipv6. If false the first ipv4\n# address will be used. If true the first ipv6 address will be used. Defaults to false preferring\n# ipv4. If there is only one address it will be selected regardless of ipv4/ipv6.\nrpc_address: 172.31.6.131\n# rpc_interface: eth1\n# rpc_interface_prefer_ipv6: false\n\n# port for Thrift to listen for clients on\nrpc_port: 9160\n\n# RPC address to broadcast to drivers and other Cassandra nodes. This cannot\n# be set to 0.0.0.0. If left blank, this will be set to the value of\n# rpc_address. If rpc_address is set to 0.0.0.0, broadcast_rpc_address must\n# be set.\n#broadcast_rpc_address: \n\n# enable or disable keepalive on rpc/native connections\nrpc_keepalive: true\n\n# Cassandra provides two out-of-the-box options for the RPC Server:\n#\n# sync  -\u003e One thread per thrift connection. For a very large number of clients, memory\n#          will be your limiting factor. On a 64 bit JVM, 180KB is the minimum stack size\n#          per thread, and that will correspond to your use of virtual memory (but physical memory\n#          may be limited depending on use of stack space).\n#\n# hsha  -\u003e Stands for \"half synchronous, half asynchronous.\" All thrift clients are handled\n#          asynchronously using a small number of threads that does not vary with the amount\n#          of thrift clients (and thus scales well to many clients). The rpc requests are still\n#          synchronous (one thread per active request). If hsha is selected then it is essential\n#          that rpc_max_threads is changed from the default value of unlimited.\n#\n# The default is sync because on Windows hsha is about 30% slower.  On Linux,\n# sync/hsha performance is about the same, with hsha of course using less memory.\n#\n# Alternatively,  can provide your own RPC server by providing the fully-qualified class name\n# of an o.a.c.t.TServerFactory that can create an instance of it.\nrpc_server_type: sync\n\n# Uncomment rpc_min|max_thread to set request pool size limits.\n#\n# Regardless of your choice of RPC server (see above), the number of maximum requests in the\n# RPC thread pool dictates how many concurrent requests are possible (but if you are using the sync\n# RPC server, it also dictates the number of clients that can be connected at all).\n#\n# The default is unlimited and thus provides no protection against clients overwhelming the server. You are\n# encouraged to set a maximum that makes sense for you in production, but do keep in mind that\n# rpc_max_threads represents the maximum number of client requests this server may execute concurrently.\n#\n# rpc_min_threads: 16\n# rpc_max_threads: 2048\n\n# uncomment to set socket buffer sizes on rpc connections\n# rpc_send_buff_size_in_bytes:\n# rpc_recv_buff_size_in_bytes:\n\n# Uncomment to set socket buffer size for internode communication\n# Note that when setting this, the buffer size is limited by net.core.wmem_max\n# and when not setting it it is defined by net.ipv4.tcp_wmem\n# See:\n# /proc/sys/net/core/wmem_max\n# /proc/sys/net/core/rmem_max\n# /proc/sys/net/ipv4/tcp_wmem\n# /proc/sys/net/ipv4/tcp_wmem\n# and: man tcp\n# internode_send_buff_size_in_bytes:\n# internode_recv_buff_size_in_bytes:\n\n# Frame size for thrift (maximum message length).\nthrift_framed_transport_size_in_mb: 15\n\n# Set to true to have Cassandra create a hard link to each sstable\n# flushed or streamed locally in a backups/ subdirectory of the\n# keyspace data.  Removing these links is the operator's\n# responsibility.\nincremental_backups: false\n\n# Whether or not to take a snapshot before each compaction.  Be\n# careful using this option, since Cassandra won't clean up the\n# snapshots for you.  Mostly useful if you're paranoid when there\n# is a data format change.\nsnapshot_before_compaction: false\n\n# Whether or not a snapshot is taken of the data before keyspace truncation\n# or dropping of column families. The STRONGLY advised default of true \n# should be used to provide data safety. If you set this flag to false, you will\n# lose data on truncation or drop.\nauto_snapshot: true\n\n# Granularity of the collation index of rows within a partition.\n# Increase if your rows are large, or if you have a very large\n# number of rows per partition.  The competing goals are these:\n#   1) a smaller granularity means more index entries are generated\n#      and looking up rows withing the partition by collation column\n#      is faster\n#   2) but, Cassandra will keep the collation index in memory for hot\n#      rows (as part of the key cache), so a larger granularity means\n#      you can cache more hot rows\ncolumn_index_size_in_kb: 64\n# Per sstable indexed key cache entries (the collation index in memory\n# mentioned above) exceeding this size will not be held on heap.\n# This means that only partition information is held on heap and the\n# index entries are read from disk.\n#\n# Note that this size refers to the size of the\n# serialized index information and not the size of the partition.\ncolumn_index_cache_size_in_kb: 2\n\n# Number of simultaneous compactions to allow, NOT including\n# validation \"compactions\" for anti-entropy repair.  Simultaneous\n# compactions can help preserve read performance in a mixed read/write\n# workload, by mitigating the tendency of small sstables to accumulate\n# during a single long running compactions. The default is usually\n# fine and if you experience problems with compaction running too\n# slowly or too fast, you should look at\n# compaction_throughput_mb_per_sec first.\n#\n# concurrent_compactors defaults to the smaller of (number of disks,\n# number of cores), with a minimum of 2 and a maximum of 8.\n# \n# If your data directories are backed by SSD, you should increase this\n# to the number of cores.\n#concurrent_compactors: 1\n\n# Throttles compaction to the given total throughput across the entire\n# system. The faster you insert data, the faster you need to compact in\n# order to keep the sstable count down, but in general, setting this to\n# 16 to 32 times the rate you are inserting data is more than sufficient.\n# Setting this to 0 disables throttling. Note that this account for all types\n# of compaction, including validation compaction.\ncompaction_throughput_mb_per_sec: 16\n\n# When compacting, the replacement sstable(s) can be opened before they\n# are completely written, and used in place of the prior sstables for\n# any range that has been written. This helps to smoothly transfer reads \n# between the sstables, reducing page cache churn and keeping hot rows hot\nsstable_preemptive_open_interval_in_mb: 50\n\n# Throttles all outbound streaming file transfers on this node to the\n# given total throughput in Mbps. This is necessary because Cassandra does\n# mostly sequential IO when streaming data during bootstrap or repair, which\n# can lead to saturating the network connection and degrading rpc performance.\n# When unset, the default is 200 Mbps or 25 MB/s.\n# stream_throughput_outbound_megabits_per_sec: 200\n\n# Throttles all streaming file transfer between the datacenters,\n# this setting allows users to throttle inter dc stream throughput in addition\n# to throttling all network stream traffic as configured with\n# stream_throughput_outbound_megabits_per_sec\n# When unset, the default is 200 Mbps or 25 MB/s\n# inter_dc_stream_throughput_outbound_megabits_per_sec: 200\n\n# How long the coordinator should wait for read operations to complete\nread_request_timeout_in_ms: 5000\n# How long the coordinator should wait for seq or index scans to complete\nrange_request_timeout_in_ms: 10000\n# How long the coordinator should wait for writes to complete\nwrite_request_timeout_in_ms: 2000\n# How long the coordinator should wait for counter writes to complete\ncounter_write_request_timeout_in_ms: 5000\n# How long a coordinator should continue to retry a CAS operation\n# that contends with other proposals for the same row\ncas_contention_timeout_in_ms: 1000\n# How long the coordinator should wait for truncates to complete\n# (This can be much longer, because unless auto_snapshot is disabled\n# we need to flush first so we can snapshot before removing the data.)\ntruncate_request_timeout_in_ms: 60000\n# The default timeout for other, miscellaneous operations\nrequest_timeout_in_ms: 10000\n\n# Enable operation timeout information exchange between nodes to accurately\n# measure request timeouts.  If disabled, replicas will assume that requests\n# were forwarded to them instantly by the coordinator, which means that\n# under overload conditions we will waste that much extra time processing \n# already-timed-out requests.\n#\n# Warning: before enabling this property make sure to ntp is installed\n# and the times are synchronized between the nodes.\ncross_node_timeout: false\n\n# Set socket timeout for streaming operation.\n# The stream session is failed if no data is received by any of the\n# participants within that period.\n# Default value is 3600000, which means streams timeout after an hour.\n# streaming_socket_timeout_in_ms: 3600000\n\n# phi value that must be reached for a host to be marked down.\n# most users should never need to adjust this.\n# phi_convict_threshold: 8\n\n# endpoint_snitch -- Set this to a class that implements\n# IEndpointSnitch.  The snitch has two functions:\n# - it teaches Cassandra enough about your network topology to route\n#   requests efficiently\n# - it allows Cassandra to spread replicas around your cluster to avoid\n#   correlated failures. It does this by grouping machines into\n#   \"datacenters\" and \"racks.\"  Cassandra will do its best not to have\n#   more than one replica on the same \"rack\" (which may not actually\n#   be a physical location)\n#\n# IF YOU CHANGE THE SNITCH AFTER DATA IS INSERTED INTO THE CLUSTER,\n# YOU MUST RUN A FULL REPAIR, SINCE THE SNITCH AFFECTS WHERE REPLICAS\n# ARE PLACED.\n#\n# IF THE RACK A REPLICA IS PLACED IN CHANGES AFTER THE REPLICA HAS BEEN\n# ADDED TO A RING, THE NODE MUST BE DECOMMISSIONED AND REBOOTSTRAPPED.\n#\n# Out of the box, Cassandra provides\n#  - SimpleSnitch:\n#    Treats Strategy order as proximity. This can improve cache\n#    locality when disabling read repair.  Only appropriate for\n#    single-datacenter deployments.\n#  - GossipingPropertyFileSnitch\n#    This should be your go-to snitch for production use.  The rack\n#    and datacenter for the local node are defined in\n#    cassandra-rackdc.properties and propagated to other nodes via\n#    gossip.  If cassandra-topology.properties exists, it is used as a\n#    fallback, allowing migration from the PropertyFileSnitch.\n#  - PropertyFileSnitch:\n#    Proximity is determined by rack and data center, which are\n#    explicitly configured in cassandra-topology.properties.\n#  - Ec2Snitch:\n#    Appropriate for EC2 deployments in a single Region. Loads Region\n#    and Availability Zone information from the EC2 API. The Region is\n#    treated as the datacenter, and the Availability Zone as the rack.\n#    Only private IPs are used, so this will not work across multiple\n#    Regions.\n#  - Ec2MultiRegionSnitch:\n#    Uses public IPs as broadcast_address to allow cross-region\n#    connectivity.  (Thus, you should set seed addresses to the public\n#    IP as well.) You will need to open the storage_port or\n#    ssl_storage_port on the public IP firewall.  (For intra-Region\n#    traffic, Cassandra will switch to the private IP after\n#    establishing a connection.)\n#  - RackInferringSnitch:\n#    Proximity is determined by rack and data center, which are\n#    assumed to correspond to the 3rd and 2nd octet of each node's IP\n#    address, respectively.  Unless this happens to match your\n#    deployment conventions, this is best used as an example of\n#    writing a custom Snitch class and is provided in that spirit.\n#\n# You can use a custom Snitch by setting this to the full class name\n# of the snitch, which will be assumed to be on your classpath.\nendpoint_snitch: GossipingPropertyFileSnitch\n\n# controls how often to perform the more expensive part of host score\n# calculation\ndynamic_snitch_update_interval_in_ms: 100 \n# controls how often to reset all host scores, allowing a bad host to\n# possibly recover\ndynamic_snitch_reset_interval_in_ms: 600000\n# if set greater than zero and read_repair_chance is \u003c 1.0, this will allow\n# 'pinning' of replicas to hosts in order to increase cache capacity.\n# The badness threshold will control how much worse the pinned host has to be\n# before the dynamic snitch will prefer other replicas over it.  This is\n# expressed as a double which represents a percentage.  Thus, a value of\n# 0.2 means Cassandra would continue to prefer the static snitch values\n# until the pinned host was 20% worse than the fastest.\ndynamic_snitch_badness_threshold: 0.1\n\n# request_scheduler -- Set this to a class that implements\n# RequestScheduler, which will schedule incoming client requests\n# according to the specific policy. This is useful for multi-tenancy\n# with a single Cassandra cluster.\n# NOTE: This is specifically for requests from the client and does\n# not affect inter node communication.\n# org.apache.cassandra.scheduler.NoScheduler - No scheduling takes place\n# org.apache.cassandra.scheduler.RoundRobinScheduler - Round robin of\n# client requests to a node with a separate queue for each\n# request_scheduler_id. The scheduler is further customized by\n# request_scheduler_options as described below.\nrequest_scheduler: org.apache.cassandra.scheduler.NoScheduler\n\n# Scheduler Options vary based on the type of scheduler\n# NoScheduler - Has no options\n# RoundRobin\n#  - throttle_limit -- The throttle_limit is the number of in-flight\n#                      requests per client.  Requests beyond \n#                      that limit are queued up until\n#                      running requests can complete.\n#                      The value of 80 here is twice the number of\n#                      concurrent_reads + concurrent_writes.\n#  - default_weight -- default_weight is optional and allows for\n#                      overriding the default which is 1.\n#  - weights -- Weights are optional and will default to 1 or the\n#               overridden default_weight. The weight translates into how\n#               many requests are handled during each turn of the\n#               RoundRobin, based on the scheduler id.\n#\n# request_scheduler_options:\n#    throttle_limit: 80\n#    default_weight: 5\n#    weights:\n#      Keyspace1: 1\n#      Keyspace2: 5\n\n# request_scheduler_id -- An identifier based on which to perform\n# the request scheduling. Currently the only valid option is keyspace.\n# request_scheduler_id: keyspace\n\n# Enable or disable inter-node encryption\n# JVM defaults for supported SSL socket protocols and cipher suites can\n# be replaced using custom encryption options. This is not recommended\n# unless you have policies in place that dictate certain settings, or\n# need to disable vulnerable ciphers or protocols in case the JVM cannot\n# be updated.\n# FIPS compliant settings can be configured at JVM level and should not\n# involve changing encryption settings here:\n# https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/FIPS.html\n# NOTE: No custom encryption options are enabled at the moment\n# The available internode options are : all, none, dc, rack\n#\n# If set to dc cassandra will encrypt the traffic between the DCs\n# If set to rack cassandra will encrypt the traffic between the racks\n#\n# The passwords used in these options must match the passwords used when generating\n# the keystore and truststore.  For instructions on generating these files, see:\n# http://download.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html#CreateKeystore\n#\nserver_encryption_options:\n    internode_encryption: none\n    keystore: conf/.keystore\n    keystore_password: cassandra\n    truststore: conf/.truststore\n    truststore_password: cassandra\n    # More advanced defaults below:\n    # protocol: TLS\n    # algorithm: SunX509\n    # store_type: JKS\n    # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n    # require_client_auth: false\n    # require_endpoint_verification: false\n\n# enable or disable client/server encryption.\nclient_encryption_options:\n    enabled: false\n    # If enabled and optional is set to true encrypted and unencrypted connections are handled.\n    optional: false\n    keystore: conf/.keystore\n    keystore_password: cassandra\n    # require_client_auth: false\n    # Set trustore and truststore_password if require_client_auth is true\n    # truststore: conf/.truststore\n    # truststore_password: cassandra\n    # More advanced defaults below:\n    # protocol: TLS\n    # algorithm: SunX509\n    # store_type: JKS\n    # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n\n# internode_compression controls whether traffic between nodes is\n# compressed.\n# can be:  all  - all traffic is compressed\n#          dc   - traffic between different datacenters is compressed\n#          none - nothing is compressed.\ninternode_compression: dc\n\n# Enable or disable tcp_nodelay for inter-dc communication.\n# Disabling it will result in larger (but fewer) network packets being sent,\n# reducing overhead from the TCP protocol itself, at the cost of increasing\n# latency if you block for cross-datacenter responses.\ninter_dc_tcp_nodelay: false\n\n# TTL for different trace types used during logging of the repair process.\ntracetype_query_ttl: 86400\ntracetype_repair_ttl: 604800\n\n# UDFs (user defined functions) are disabled by default.\n# As of Cassandra 3.0 there is a sandbox in place that should prevent execution of evil code.\nenable_user_defined_functions: false\n\n# Enables scripted UDFs (JavaScript UDFs).\n# Java UDFs are always enabled, if enable_user_defined_functions is true.\n# Enable this option to be able to use UDFs with \"language javascript\" or any custom JSR-223 provider.\n# This option has no effect, if enable_user_defined_functions is false.\nenable_scripted_user_defined_functions: false\n\n# The default Windows kernel timer and scheduling resolution is 15.6ms for power conservation.\n# Lowering this value on Windows can provide much tighter latency and better throughput, however\n# some virtualized environments may see a negative performance impact from changing this setting\n# below their system default. The sysinternals 'clockres' tool can confirm your system's default\n# setting.\nwindows_timer_interval: 1\n\n\n# Enables encrypting data at-rest (on disk). Different key providers can be plugged in, but the default reads from\n# a JCE-style keystore. A single keystore can hold multiple keys, but the one referenced by\n# the \"key_alias\" is the only key that will be used for encrypt opertaions; previously used keys\n# can still (and should!) be in the keystore and will be used on decrypt operations\n# (to handle the case of key rotation).\n#\n# It is strongly recommended to download and install Java Cryptography Extension (JCE)\n# Unlimited Strength Jurisdiction Policy Files for your version of the JDK.\n# (current link: http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html)\n#\n# Currently, only the following file types are supported for transparent data encryption, although\n# more are coming in future cassandra releases: commitlog, hints\ntransparent_data_encryption_options:\n    enabled: false\n    chunk_length_kb: 64\n    cipher: AES/CBC/PKCS5Padding\n    key_alias: testing:1\n    # CBC IV length for AES needs to be 16 bytes (which is also the default size)\n    # iv_length: 16\n    key_provider: \n      - class_name: org.apache.cassandra.security.JKSKeyProvider\n        parameters: \n          - keystore: conf/.keystore\n            keystore_password: cassandra\n            store_type: JCEKS\n            key_password: cassandra\n\n\n#####################\n# SAFETY THRESHOLDS #\n#####################\n\n# When executing a scan, within or across a partition, we need to keep the\n# tombstones seen in memory so we can return them to the coordinator, which\n# will use them to make sure other replicas also know about the deleted rows.\n# With workloads that generate a lot of tombstones, this can cause performance\n# problems and even exaust the server heap.\n# (http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets)\n# Adjust the thresholds here if you understand the dangers and want to\n# scan more tombstones anyway.  These thresholds may also be adjusted at runtime\n# using the StorageService mbean.\ntombstone_warn_threshold: 1000\ntombstone_failure_threshold: 100000\n\n# Log WARN on any batch size exceeding this value. 5kb per batch by default.\n# Caution should be taken on increasing the size of this threshold as it can lead to node instability.\nbatch_size_warn_threshold_in_kb: 5\n\n# Fail any batch exceeding this value. 50kb (10x warn threshold) by default.\nbatch_size_fail_threshold_in_kb: 50\n\n# Log WARN on any batches not of type LOGGED than span across more partitions than this limit\nunlogged_batch_across_partitions_warn_threshold: 10\n\n# Log a warning when compacting partitions larger than this value\ncompaction_large_partition_warning_threshold_mb: 100\n\n# GC Pauses greater than gc_warn_threshold_in_ms will be logged at WARN level\n# Adjust the threshold based on your application throughput requirement\n# By default, Cassandra logs GC Pauses greater than 200 ms at INFO level\ngc_warn_threshold_in_ms: 1000\n\nauto_bootstrap: false\n",
                            "template": "# Cassandra storage config YAML \n\n# NOTE:\n#   See http://wiki.apache.org/cassandra/StorageConfiguration for\n#   full explanations of configuration directives\n# /NOTE\n\n# The name of the cluster. This is mainly used to prevent machines in\n# one logical cluster from joining another.\ncluster_name: '${cluster_name}'\n\n# This defines the number of tokens randomly assigned to this node on the ring\n# The more tokens, relative to other nodes, the larger the proportion of data\n# that this node will store. You probably want all nodes to have the same number\n# of tokens assuming they have equal hardware capability.\n#\n# If you leave this unspecified, Cassandra will use the default of 1 token for legacy compatibility,\n# and will use the initial_token as described below.\n#\n# Specifying initial_token will override this setting on the node's initial start,\n# on subsequent starts, this setting will apply even if initial token is set.\n#\n# If you already have a cluster with 1 token per node, and wish to migrate to \n# multiple tokens per node, see http://wiki.apache.org/cassandra/Operations\nnum_tokens: 256\n\n# Triggers automatic allocation of num_tokens tokens for this node. The allocation\n# algorithm attempts to choose tokens in a way that optimizes replicated load over\n# the nodes in the datacenter for the replication strategy used by the specified\n# keyspace.\n#\n# The load assigned to each node will be close to proportional to its number of\n# vnodes.\n#\n# Only supported with the Murmur3Partitioner.\n# allocate_tokens_for_keyspace: KEYSPACE\n\n# initial_token allows you to specify tokens manually.  While you can use # it with\n# vnodes (num_tokens \u003e 1, above) -- in which case you should provide a \n# comma-separated list -- it's primarily used when adding nodes # to legacy clusters \n# that do not have vnodes enabled.\n# initial_token:\n\n# See http://wiki.apache.org/cassandra/HintedHandoff\n# May either be \"true\" or \"false\" to enable globally\nhinted_handoff_enabled: true\n# When hinted_handoff_enabled is true, a black list of data centers that will not\n# perform hinted handoff\n#hinted_handoff_disabled_datacenters:\n#    - DC1\n#    - DC2\n# this defines the maximum amount of time a dead host will have hints\n# generated.  After it has been dead this long, new hints for it will not be\n# created until it has been seen alive and gone down again.\nmax_hint_window_in_ms: 10800000 # 3 hours\n\n# Maximum throttle in KBs per second, per delivery thread.  This will be\n# reduced proportionally to the number of nodes in the cluster.  (If there\n# are two nodes in the cluster, each delivery thread will use the maximum\n# rate; if there are three, each will throttle to half of the maximum,\n# since we expect two nodes to be delivering hints simultaneously.)\nhinted_handoff_throttle_in_kb: 1024\n\n# Number of threads with which to deliver hints;\n# Consider increasing this number when you have multi-dc deployments, since\n# cross-dc handoff tends to be slower\nmax_hints_delivery_threads: 2\n\n# Directory where Cassandra should store hints.\n# If not set, the default directory is $CASSANDRA_HOME/data/hints.\n# hints_directory: /var/lib/cassandra/hints\n\n# How often hints should be flushed from the internal buffers to disk.\n# Will *not* trigger fsync.\nhints_flush_period_in_ms: 10000\n\n# Maximum size for a single hints file, in megabytes.\nmax_hints_file_size_in_mb: 128\n\n# Compression to apply to the hint files. If omitted, hints files\n# will be written uncompressed. LZ4, Snappy, and Deflate compressors\n# are supported.\n#hints_compression:\n#   - class_name: LZ4Compressor\n#     parameters:\n#         -\n\n# Maximum throttle in KBs per second, total. This will be\n# reduced proportionally to the number of nodes in the cluster.\nbatchlog_replay_throttle_in_kb: 1024\n\n# Authentication backend, implementing IAuthenticator; used to identify users\n# Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthenticator,\n# PasswordAuthenticator}.\n#\n# - AllowAllAuthenticator performs no checks - set it to disable authentication.\n# - PasswordAuthenticator relies on username/password pairs to authenticate\n#   users. It keeps usernames and hashed passwords in system_auth.credentials table.\n#   Please increase system_auth keyspace replication factor if you use this authenticator.\n#   If using PasswordAuthenticator, CassandraRoleManager must also be used (see below)\nauthenticator: AllowAllAuthenticator\n\n# Authorization backend, implementing IAuthorizer; used to limit access/provide permissions\n# Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthorizer,\n# CassandraAuthorizer}.\n#\n# - AllowAllAuthorizer allows any action to any user - set it to disable authorization.\n# - CassandraAuthorizer stores permissions in system_auth.permissions table. Please\n#   increase system_auth keyspace replication factor if you use this authorizer.\nauthorizer: AllowAllAuthorizer\n\n# Part of the Authentication \u0026 Authorization backend, implementing IRoleManager; used\n# to maintain grants and memberships between roles.\n# Out of the box, Cassandra provides org.apache.cassandra.auth.CassandraRoleManager,\n# which stores role information in the system_auth keyspace. Most functions of the\n# IRoleManager require an authenticated login, so unless the configured IAuthenticator\n# actually implements authentication, most of this functionality will be unavailable.\n#\n# - CassandraRoleManager stores role data in the system_auth keyspace. Please\n#   increase system_auth keyspace replication factor if you use this role manager.\nrole_manager: CassandraRoleManager\n\n# Validity period for roles cache (fetching granted roles can be an expensive\n# operation depending on the role manager, CassandraRoleManager is one example)\n# Granted roles are cached for authenticated sessions in AuthenticatedUser and\n# after the period specified here, become eligible for (async) reload.\n# Defaults to 2000, set to 0 to disable caching entirely.\n# Will be disabled automatically for AllowAllAuthenticator.\nroles_validity_in_ms: 2000\n\n# Refresh interval for roles cache (if enabled).\n# After this interval, cache entries become eligible for refresh. Upon next\n# access, an async reload is scheduled and the old value returned until it\n# completes. If roles_validity_in_ms is non-zero, then this must be\n# also.\n# Defaults to the same value as roles_validity_in_ms.\n# roles_update_interval_in_ms: 2000\n\n# Validity period for permissions cache (fetching permissions can be an\n# expensive operation depending on the authorizer, CassandraAuthorizer is\n# one example). Defaults to 2000, set to 0 to disable.\n# Will be disabled automatically for AllowAllAuthorizer.\npermissions_validity_in_ms: 2000\n\n# Refresh interval for permissions cache (if enabled).\n# After this interval, cache entries become eligible for refresh. Upon next\n# access, an async reload is scheduled and the old value returned until it\n# completes. If permissions_validity_in_ms is non-zero, then this must be\n# also.\n# Defaults to the same value as permissions_validity_in_ms.\n# permissions_update_interval_in_ms: 2000\n\n# Validity period for credentials cache. This cache is tightly coupled to\n# the provided PasswordAuthenticator implementation of IAuthenticator. If\n# another IAuthenticator implementation is configured, this cache will not\n# be automatically used and so the following settings will have no effect.\n# Please note, credentials are cached in their encrypted form, so while\n# activating this cache may reduce the number of queries made to the\n# underlying table, it may not  bring a significant reduction in the\n# latency of individual authentication attempts.\n# Defaults to 2000, set to 0 to disable credentials caching.\ncredentials_validity_in_ms: 2000\n\n# Refresh interval for credentials cache (if enabled).\n# After this interval, cache entries become eligible for refresh. Upon next\n# access, an async reload is scheduled and the old value returned until it\n# completes. If credentials_validity_in_ms is non-zero, then this must be\n# also.\n# Defaults to the same value as credentials_validity_in_ms.\n# credentials_update_interval_in_ms: 2000\n\n# The partitioner is responsible for distributing groups of rows (by\n# partition key) across nodes in the cluster.  You should leave this\n# alone for new clusters.  The partitioner can NOT be changed without\n# reloading all data, so when upgrading you should set this to the\n# same partitioner you were already using.\n#\n# Besides Murmur3Partitioner, partitioners included for backwards\n# compatibility include RandomPartitioner, ByteOrderedPartitioner, and\n# OrderPreservingPartitioner.\n#\npartitioner: org.apache.cassandra.dht.Murmur3Partitioner\n\n# Directories where Cassandra should store data on disk.  Cassandra\n# will spread data evenly across them, subject to the granularity of\n# the configured compaction strategy.\n# If not set, the default directory is $CASSANDRA_HOME/data/data.\ndata_file_directories:\n    - /var/lib/cassandra/data\n\n# commit log.  when running on magnetic HDD, this should be a\n# separate spindle than the data directories.\n# If not set, the default directory is $CASSANDRA_HOME/data/commitlog.\ncommitlog_directory: /var/lib/cassandra/commitlog\n\n# policy for data disk failures:\n# die: shut down gossip and client transports and kill the JVM for any fs errors or\n#      single-sstable errors, so the node can be replaced.\n# stop_paranoid: shut down gossip and client transports even for single-sstable errors,\n#                kill the JVM for errors during startup.\n# stop: shut down gossip and client transports, leaving the node effectively dead, but\n#       can still be inspected via JMX, kill the JVM for errors during startup.\n# best_effort: stop using the failed disk and respond to requests based on\n#              remaining available sstables.  This means you WILL see obsolete\n#              data at CL.ONE!\n# ignore: ignore fatal errors and let requests fail, as in pre-1.2 Cassandra\ndisk_failure_policy: stop\n\n# policy for commit disk failures:\n# die: shut down gossip and Thrift and kill the JVM, so the node can be replaced.\n# stop: shut down gossip and Thrift, leaving the node effectively dead, but\n#       can still be inspected via JMX.\n# stop_commit: shutdown the commit log, letting writes collect but\n#              continuing to service reads, as in pre-2.0.5 Cassandra\n# ignore: ignore fatal errors and let the batches fail\ncommit_failure_policy: stop\n\n# Maximum size of the native protocol prepared statement cache\n#\n# Valid values are either \"auto\" (omitting the value) or a value greater 0.\n#\n# Note that specifying a too large value will result in long running GCs and possbily\n# out-of-memory errors. Keep the value at a small fraction of the heap.\n#\n# If you constantly see \"prepared statements discarded in the last minute because\n# cache limit reached\" messages, the first step is to investigate the root cause\n# of these messages and check whether prepared statements are used correctly -\n# i.e. use bind markers for variable parts.\n#\n# Do only change the default value, if you really have more prepared statements than\n# fit in the cache. In most cases it is not neccessary to change this value.\n# Constantly re-preparing statements is a performance penalty.\n#\n# Default value (\"auto\") is 1/256th of the heap or 10MB, whichever is greater\nprepared_statements_cache_size_mb:\n\n# Maximum size of the Thrift prepared statement cache\n#\n# If you do not use Thrift at all, it is safe to leave this value at \"auto\".\n#\n# See description of 'prepared_statements_cache_size_mb' above for more information.\n#\n# Default value (\"auto\") is 1/256th of the heap or 10MB, whichever is greater\nthrift_prepared_statements_cache_size_mb:\n\n# Maximum size of the key cache in memory.\n#\n# Each key cache hit saves 1 seek and each row cache hit saves 2 seeks at the\n# minimum, sometimes more. The key cache is fairly tiny for the amount of\n# time it saves, so it's worthwhile to use it at large numbers.\n# The row cache saves even more time, but must contain the entire row,\n# so it is extremely space-intensive. It's best to only use the\n# row cache if you have hot rows or static rows.\n#\n# NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.\n#\n# Default value is empty to make it \"auto\" (min(5% of Heap (in MB), 100MB)). Set to 0 to disable key cache.\nkey_cache_size_in_mb:\n\n# Duration in seconds after which Cassandra should\n# save the key cache. Caches are saved to saved_caches_directory as\n# specified in this configuration file.\n#\n# Saved caches greatly improve cold-start speeds, and is relatively cheap in\n# terms of I/O for the key cache. Row cache saving is much more expensive and\n# has limited use.\n#\n# Default is 14400 or 4 hours.\nkey_cache_save_period: 14400\n\n# Number of keys from the key cache to save\n# Disabled by default, meaning all keys are going to be saved\n# key_cache_keys_to_save: 100\n\n# Row cache implementation class name.\n# Available implementations:\n#   org.apache.cassandra.cache.OHCProvider                Fully off-heap row cache implementation (default).\n#   org.apache.cassandra.cache.SerializingCacheProvider   This is the row cache implementation availabile\n#                                                         in previous releases of Cassandra.\n# row_cache_class_name: org.apache.cassandra.cache.OHCProvider\n\n# Maximum size of the row cache in memory.\n# Please note that OHC cache implementation requires some additional off-heap memory to manage\n# the map structures and some in-flight memory during operations before/after cache entries can be\n# accounted against the cache capacity. This overhead is usually small compared to the whole capacity.\n# Do not specify more memory that the system can afford in the worst usual situation and leave some\n# headroom for OS block level cache. Do never allow your system to swap.\n#\n# Default value is 0, to disable row caching.\nrow_cache_size_in_mb: 0\n\n# Duration in seconds after which Cassandra should save the row cache.\n# Caches are saved to saved_caches_directory as specified in this configuration file.\n#\n# Saved caches greatly improve cold-start speeds, and is relatively cheap in\n# terms of I/O for the key cache. Row cache saving is much more expensive and\n# has limited use.\n#\n# Default is 0 to disable saving the row cache.\nrow_cache_save_period: 0\n\n# Number of keys from the row cache to save.\n# Specify 0 (which is the default), meaning all keys are going to be saved\n# row_cache_keys_to_save: 100\n\n# Maximum size of the counter cache in memory.\n#\n# Counter cache helps to reduce counter locks' contention for hot counter cells.\n# In case of RF = 1 a counter cache hit will cause Cassandra to skip the read before\n# write entirely. With RF \u003e 1 a counter cache hit will still help to reduce the duration\n# of the lock hold, helping with hot counter cell updates, but will not allow skipping\n# the read entirely. Only the local (clock, count) tuple of a counter cell is kept\n# in memory, not the whole counter, so it's relatively cheap.\n#\n# NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.\n#\n# Default value is empty to make it \"auto\" (min(2.5% of Heap (in MB), 50MB)). Set to 0 to disable counter cache.\n# NOTE: if you perform counter deletes and rely on low gcgs, you should disable the counter cache.\ncounter_cache_size_in_mb:\n\n# Duration in seconds after which Cassandra should\n# save the counter cache (keys only). Caches are saved to saved_caches_directory as\n# specified in this configuration file.\n#\n# Default is 7200 or 2 hours.\ncounter_cache_save_period: 7200\n\n# Number of keys from the counter cache to save\n# Disabled by default, meaning all keys are going to be saved\n# counter_cache_keys_to_save: 100\n\n# saved caches\n# If not set, the default directory is $CASSANDRA_HOME/data/saved_caches.\nsaved_caches_directory: /var/lib/cassandra/saved_caches\n\n# commitlog_sync may be either \"periodic\" or \"batch.\" \n# \n# When in batch mode, Cassandra won't ack writes until the commit log\n# has been fsynced to disk.  It will wait\n# commitlog_sync_batch_window_in_ms milliseconds between fsyncs.\n# This window should be kept short because the writer threads will\n# be unable to do extra work while waiting.  (You may need to increase\n# concurrent_writes for the same reason.)\n#\n# commitlog_sync: batch\n# commitlog_sync_batch_window_in_ms: 2\n#\n# the other option is \"periodic\" where writes may be acked immediately\n# and the CommitLog is simply synced every commitlog_sync_period_in_ms\n# milliseconds. \ncommitlog_sync: periodic\ncommitlog_sync_period_in_ms: 10000\n\n# The size of the individual commitlog file segments.  A commitlog\n# segment may be archived, deleted, or recycled once all the data\n# in it (potentially from each columnfamily in the system) has been\n# flushed to sstables.\n#\n# The default size is 32, which is almost always fine, but if you are\n# archiving commitlog segments (see commitlog_archiving.properties),\n# then you probably want a finer granularity of archiving; 8 or 16 MB\n# is reasonable.\n# Max mutation size is also configurable via max_mutation_size_in_kb setting in\n# cassandra.yaml. The default is half the size commitlog_segment_size_in_mb * 1024.\n#\n# NOTE: If max_mutation_size_in_kb is set explicitly then commitlog_segment_size_in_mb must\n# be set to at least twice the size of max_mutation_size_in_kb / 1024\n#\ncommitlog_segment_size_in_mb: 32\n\n# Compression to apply to the commit log. If omitted, the commit log\n# will be written uncompressed.  LZ4, Snappy, and Deflate compressors\n# are supported.\n#commitlog_compression:\n#   - class_name: LZ4Compressor\n#     parameters:\n#         -\n\n# any class that implements the SeedProvider interface and has a\n# constructor that takes a Map\u003cString, String\u003e of parameters will do.\nseed_provider:\n    # Addresses of hosts that are deemed contact points. \n    # Cassandra nodes use this list of hosts to find each other and learn\n    # the topology of the ring.  You must change this if you are running\n    # multiple nodes!\n    - class_name: org.apache.cassandra.locator.SimpleSeedProvider\n      parameters:\n          # seeds is actually a comma-delimited list of addresses.\n          # Ex: \"\u003cip1\u003e,\u003cip2\u003e,\u003cip3\u003e\"\n          - seeds: \"${seeds}\"\n\n# For workloads with more data than can fit in memory, Cassandra's\n# bottleneck will be reads that need to fetch data from\n# disk. \"concurrent_reads\" should be set to (16 * number_of_drives) in\n# order to allow the operations to enqueue low enough in the stack\n# that the OS and drives can reorder them. Same applies to\n# \"concurrent_counter_writes\", since counter writes read the current\n# values before incrementing and writing them back.\n#\n# On the other hand, since writes are almost never IO bound, the ideal\n# number of \"concurrent_writes\" is dependent on the number of cores in\n# your system; (8 * number_of_cores) is a good rule of thumb.\nconcurrent_reads: 32\nconcurrent_writes: 32\nconcurrent_counter_writes: 32\n\n# For materialized view writes, as there is a read involved, so this should\n# be limited by the less of concurrent reads or concurrent writes.\nconcurrent_materialized_view_writes: 32\n\n# Maximum memory to use for sstable chunk cache and buffer pooling.\n# 32MB of this are reserved for pooling buffers, the rest is used as an\n# cache that holds uncompressed sstable chunks.\n# Defaults to the smaller of 1/4 of heap or 512MB. This pool is allocated off-heap,\n# so is in addition to the memory allocated for heap. The cache also has on-heap\n# overhead which is roughly 128 bytes per chunk (i.e. 0.2% of the reserved size\n# if the default 64k chunk size is used).\n# Memory is only allocated when needed.\n# file_cache_size_in_mb: 512\n\n# Flag indicating whether to allocate on or off heap when the sstable buffer\n# pool is exhausted, that is when it has exceeded the maximum memory\n# file_cache_size_in_mb, beyond which it will not cache buffers but allocate on request.\n\n# buffer_pool_use_heap_if_exhausted: true\n\n# The strategy for optimizing disk read\n# Possible values are:\n# ssd (for solid state disks, the default)\n# spinning (for spinning disks)\n# disk_optimization_strategy: ssd\n\n# Total permitted memory to use for memtables. Cassandra will stop\n# accepting writes when the limit is exceeded until a flush completes,\n# and will trigger a flush based on memtable_cleanup_threshold\n# If omitted, Cassandra will set both to 1/4 the size of the heap.\n# memtable_heap_space_in_mb: 2048\n# memtable_offheap_space_in_mb: 2048\n\n# Ratio of occupied non-flushing memtable size to total permitted size\n# that will trigger a flush of the largest memtable. Larger mct will\n# mean larger flushes and hence less compaction, but also less concurrent\n# flush activity which can make it difficult to keep your disks fed\n# under heavy write load.\n#\n# memtable_cleanup_threshold defaults to 1 / (memtable_flush_writers + 1)\n# memtable_cleanup_threshold: 0.11\n\n# Specify the way Cassandra allocates and manages memtable memory.\n# Options are:\n#   heap_buffers:    on heap nio buffers\n#   offheap_buffers: off heap (direct) nio buffers\n#   offheap_objects: off heap objects\nmemtable_allocation_type: heap_buffers\n\n# Total space to use for commit logs on disk.\n#\n# If space gets above this value, Cassandra will flush every dirty CF\n# in the oldest segment and remove it.  So a small total commitlog space\n# will tend to cause more flush activity on less-active columnfamilies.\n#\n# The default value is the smaller of 8192, and 1/4 of the total space\n# of the commitlog volume.\n#\n# commitlog_total_space_in_mb: 8192\n\n# This sets the amount of memtable flush writer threads.  These will\n# be blocked by disk io, and each one will hold a memtable in memory\n# while blocked.\n#\n# memtable_flush_writers defaults to one per data_file_directory.\n#\n# If your data directories are backed by SSD, you can increase this, but\n# avoid having memtable_flush_writers * data_file_directories \u003e number of cores\n#memtable_flush_writers: 1\n\n# A fixed memory pool size in MB for for SSTable index summaries. If left\n# empty, this will default to 5% of the heap size. If the memory usage of\n# all index summaries exceeds this limit, SSTables with low read rates will\n# shrink their index summaries in order to meet this limit.  However, this\n# is a best-effort process. In extreme conditions Cassandra may need to use\n# more than this amount of memory.\nindex_summary_capacity_in_mb:\n\n# How frequently index summaries should be resampled.  This is done\n# periodically to redistribute memory from the fixed-size pool to sstables\n# proportional their recent read rates.  Setting to -1 will disable this\n# process, leaving existing index summaries at their current sampling level.\nindex_summary_resize_interval_in_minutes: 60\n\n# Whether to, when doing sequential writing, fsync() at intervals in\n# order to force the operating system to flush the dirty\n# buffers. Enable this to avoid sudden dirty buffer flushing from\n# impacting read latencies. Almost always a good idea on SSDs; not\n# necessarily on platters.\ntrickle_fsync: false\ntrickle_fsync_interval_in_kb: 10240\n\n# TCP port, for commands and data\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\nstorage_port: 7000\n\n# SSL port, for encrypted communication.  Unused unless enabled in\n# encryption_options\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\nssl_storage_port: 7001\n\n# Address or interface to bind to and tell other Cassandra nodes to connect to.\n# You _must_ change this if you want multiple nodes to be able to communicate!\n#\n# Set listen_address OR listen_interface, not both. Interfaces must correspond\n# to a single address, IP aliasing is not supported.\n#\n# Leaving it blank leaves it up to InetAddress.getLocalHost(). This\n# will always do the Right Thing _if_ the node is properly configured\n# (hostname, name resolution, etc), and the Right Thing is to use the\n# address associated with the hostname (it might not be).\n#\n# Setting listen_address to 0.0.0.0 is always wrong.\n#\n# If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\n# you can specify which should be chosen using listen_interface_prefer_ipv6. If false the first ipv4\n# address will be used. If true the first ipv6 address will be used. Defaults to false preferring\n# ipv4. If there is only one address it will be selected regardless of ipv4/ipv6.\nlisten_address: ${server_ip}\n# listen_interface: eth0\n# listen_interface_prefer_ipv6: false\n\n# Address to broadcast to other Cassandra nodes\n# Leaving this blank will set it to the same value as listen_address\n# broadcast_address: 1.2.3.4\n\n# When using multiple physical network interfaces, set this\n# to true to listen on broadcast_address in addition to\n# the listen_address, allowing nodes to communicate in both\n# interfaces.\n# Ignore this property if the network configuration automatically\n# routes  between the public and private networks such as EC2.\n# listen_on_broadcast_address: false\n\n# Internode authentication backend, implementing IInternodeAuthenticator;\n# used to allow/disallow connections from peer nodes.\n# internode_authenticator: org.apache.cassandra.auth.AllowAllInternodeAuthenticator\n\n# Whether to start the native transport server.\n# Please note that the address on which the native transport is bound is the\n# same as the rpc_address. The port however is different and specified below.\nstart_native_transport: true\n# port for the CQL native transport to listen for clients on\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\nnative_transport_port: 9042\n# Enabling native transport encryption in client_encryption_options allows you to either use\n# encryption for the standard port or to use a dedicated, additional port along with the unencrypted\n# standard native_transport_port.\n# Enabling client encryption and keeping native_transport_port_ssl disabled will use encryption\n# for native_transport_port. Setting native_transport_port_ssl to a different value\n# from native_transport_port will use encryption for native_transport_port_ssl while\n# keeping native_transport_port unencrypted.\n# native_transport_port_ssl: 9142\n# The maximum threads for handling requests when the native transport is used.\n# This is similar to rpc_max_threads though the default differs slightly (and\n# there is no native_transport_min_threads, idle threads will always be stopped\n# after 30 seconds).\n# native_transport_max_threads: 128\n#\n# The maximum size of allowed frame. Frame (requests) larger than this will\n# be rejected as invalid. The default is 256MB.\n# native_transport_max_frame_size_in_mb: 256\n\n# The maximum number of concurrent client connections.\n# The default is -1, which means unlimited.\n# native_transport_max_concurrent_connections: -1\n\n# The maximum number of concurrent client connections per source ip.\n# The default is -1, which means unlimited.\n# native_transport_max_concurrent_connections_per_ip: -1\n\n# Whether to start the thrift rpc server.\nstart_rpc: true\n\n# The address or interface to bind the Thrift RPC service and native transport\n# server to.\n#\n# Set rpc_address OR rpc_interface, not both. Interfaces must correspond\n# to a single address, IP aliasing is not supported.\n#\n# Leaving rpc_address blank has the same effect as on listen_address\n# (i.e. it will be based on the configured hostname of the node).\n#\n# Note that unlike listen_address, you can specify 0.0.0.0, but you must also\n# set broadcast_rpc_address to a value other than 0.0.0.0.\n#\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\n#\n# If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\n# you can specify which should be chosen using rpc_interface_prefer_ipv6. If false the first ipv4\n# address will be used. If true the first ipv6 address will be used. Defaults to false preferring\n# ipv4. If there is only one address it will be selected regardless of ipv4/ipv6.\nrpc_address: ${server_ip}\n# rpc_interface: eth1\n# rpc_interface_prefer_ipv6: false\n\n# port for Thrift to listen for clients on\nrpc_port: 9160\n\n# RPC address to broadcast to drivers and other Cassandra nodes. This cannot\n# be set to 0.0.0.0. If left blank, this will be set to the value of\n# rpc_address. If rpc_address is set to 0.0.0.0, broadcast_rpc_address must\n# be set.\n#broadcast_rpc_address: \n\n# enable or disable keepalive on rpc/native connections\nrpc_keepalive: true\n\n# Cassandra provides two out-of-the-box options for the RPC Server:\n#\n# sync  -\u003e One thread per thrift connection. For a very large number of clients, memory\n#          will be your limiting factor. On a 64 bit JVM, 180KB is the minimum stack size\n#          per thread, and that will correspond to your use of virtual memory (but physical memory\n#          may be limited depending on use of stack space).\n#\n# hsha  -\u003e Stands for \"half synchronous, half asynchronous.\" All thrift clients are handled\n#          asynchronously using a small number of threads that does not vary with the amount\n#          of thrift clients (and thus scales well to many clients). The rpc requests are still\n#          synchronous (one thread per active request). If hsha is selected then it is essential\n#          that rpc_max_threads is changed from the default value of unlimited.\n#\n# The default is sync because on Windows hsha is about 30% slower.  On Linux,\n# sync/hsha performance is about the same, with hsha of course using less memory.\n#\n# Alternatively,  can provide your own RPC server by providing the fully-qualified class name\n# of an o.a.c.t.TServerFactory that can create an instance of it.\nrpc_server_type: sync\n\n# Uncomment rpc_min|max_thread to set request pool size limits.\n#\n# Regardless of your choice of RPC server (see above), the number of maximum requests in the\n# RPC thread pool dictates how many concurrent requests are possible (but if you are using the sync\n# RPC server, it also dictates the number of clients that can be connected at all).\n#\n# The default is unlimited and thus provides no protection against clients overwhelming the server. You are\n# encouraged to set a maximum that makes sense for you in production, but do keep in mind that\n# rpc_max_threads represents the maximum number of client requests this server may execute concurrently.\n#\n# rpc_min_threads: 16\n# rpc_max_threads: 2048\n\n# uncomment to set socket buffer sizes on rpc connections\n# rpc_send_buff_size_in_bytes:\n# rpc_recv_buff_size_in_bytes:\n\n# Uncomment to set socket buffer size for internode communication\n# Note that when setting this, the buffer size is limited by net.core.wmem_max\n# and when not setting it it is defined by net.ipv4.tcp_wmem\n# See:\n# /proc/sys/net/core/wmem_max\n# /proc/sys/net/core/rmem_max\n# /proc/sys/net/ipv4/tcp_wmem\n# /proc/sys/net/ipv4/tcp_wmem\n# and: man tcp\n# internode_send_buff_size_in_bytes:\n# internode_recv_buff_size_in_bytes:\n\n# Frame size for thrift (maximum message length).\nthrift_framed_transport_size_in_mb: 15\n\n# Set to true to have Cassandra create a hard link to each sstable\n# flushed or streamed locally in a backups/ subdirectory of the\n# keyspace data.  Removing these links is the operator's\n# responsibility.\nincremental_backups: false\n\n# Whether or not to take a snapshot before each compaction.  Be\n# careful using this option, since Cassandra won't clean up the\n# snapshots for you.  Mostly useful if you're paranoid when there\n# is a data format change.\nsnapshot_before_compaction: false\n\n# Whether or not a snapshot is taken of the data before keyspace truncation\n# or dropping of column families. The STRONGLY advised default of true \n# should be used to provide data safety. If you set this flag to false, you will\n# lose data on truncation or drop.\nauto_snapshot: true\n\n# Granularity of the collation index of rows within a partition.\n# Increase if your rows are large, or if you have a very large\n# number of rows per partition.  The competing goals are these:\n#   1) a smaller granularity means more index entries are generated\n#      and looking up rows withing the partition by collation column\n#      is faster\n#   2) but, Cassandra will keep the collation index in memory for hot\n#      rows (as part of the key cache), so a larger granularity means\n#      you can cache more hot rows\ncolumn_index_size_in_kb: 64\n# Per sstable indexed key cache entries (the collation index in memory\n# mentioned above) exceeding this size will not be held on heap.\n# This means that only partition information is held on heap and the\n# index entries are read from disk.\n#\n# Note that this size refers to the size of the\n# serialized index information and not the size of the partition.\ncolumn_index_cache_size_in_kb: 2\n\n# Number of simultaneous compactions to allow, NOT including\n# validation \"compactions\" for anti-entropy repair.  Simultaneous\n# compactions can help preserve read performance in a mixed read/write\n# workload, by mitigating the tendency of small sstables to accumulate\n# during a single long running compactions. The default is usually\n# fine and if you experience problems with compaction running too\n# slowly or too fast, you should look at\n# compaction_throughput_mb_per_sec first.\n#\n# concurrent_compactors defaults to the smaller of (number of disks,\n# number of cores), with a minimum of 2 and a maximum of 8.\n# \n# If your data directories are backed by SSD, you should increase this\n# to the number of cores.\n#concurrent_compactors: 1\n\n# Throttles compaction to the given total throughput across the entire\n# system. The faster you insert data, the faster you need to compact in\n# order to keep the sstable count down, but in general, setting this to\n# 16 to 32 times the rate you are inserting data is more than sufficient.\n# Setting this to 0 disables throttling. Note that this account for all types\n# of compaction, including validation compaction.\ncompaction_throughput_mb_per_sec: 16\n\n# When compacting, the replacement sstable(s) can be opened before they\n# are completely written, and used in place of the prior sstables for\n# any range that has been written. This helps to smoothly transfer reads \n# between the sstables, reducing page cache churn and keeping hot rows hot\nsstable_preemptive_open_interval_in_mb: 50\n\n# Throttles all outbound streaming file transfers on this node to the\n# given total throughput in Mbps. This is necessary because Cassandra does\n# mostly sequential IO when streaming data during bootstrap or repair, which\n# can lead to saturating the network connection and degrading rpc performance.\n# When unset, the default is 200 Mbps or 25 MB/s.\n# stream_throughput_outbound_megabits_per_sec: 200\n\n# Throttles all streaming file transfer between the datacenters,\n# this setting allows users to throttle inter dc stream throughput in addition\n# to throttling all network stream traffic as configured with\n# stream_throughput_outbound_megabits_per_sec\n# When unset, the default is 200 Mbps or 25 MB/s\n# inter_dc_stream_throughput_outbound_megabits_per_sec: 200\n\n# How long the coordinator should wait for read operations to complete\nread_request_timeout_in_ms: 5000\n# How long the coordinator should wait for seq or index scans to complete\nrange_request_timeout_in_ms: 10000\n# How long the coordinator should wait for writes to complete\nwrite_request_timeout_in_ms: 2000\n# How long the coordinator should wait for counter writes to complete\ncounter_write_request_timeout_in_ms: 5000\n# How long a coordinator should continue to retry a CAS operation\n# that contends with other proposals for the same row\ncas_contention_timeout_in_ms: 1000\n# How long the coordinator should wait for truncates to complete\n# (This can be much longer, because unless auto_snapshot is disabled\n# we need to flush first so we can snapshot before removing the data.)\ntruncate_request_timeout_in_ms: 60000\n# The default timeout for other, miscellaneous operations\nrequest_timeout_in_ms: 10000\n\n# Enable operation timeout information exchange between nodes to accurately\n# measure request timeouts.  If disabled, replicas will assume that requests\n# were forwarded to them instantly by the coordinator, which means that\n# under overload conditions we will waste that much extra time processing \n# already-timed-out requests.\n#\n# Warning: before enabling this property make sure to ntp is installed\n# and the times are synchronized between the nodes.\ncross_node_timeout: false\n\n# Set socket timeout for streaming operation.\n# The stream session is failed if no data is received by any of the\n# participants within that period.\n# Default value is 3600000, which means streams timeout after an hour.\n# streaming_socket_timeout_in_ms: 3600000\n\n# phi value that must be reached for a host to be marked down.\n# most users should never need to adjust this.\n# phi_convict_threshold: 8\n\n# endpoint_snitch -- Set this to a class that implements\n# IEndpointSnitch.  The snitch has two functions:\n# - it teaches Cassandra enough about your network topology to route\n#   requests efficiently\n# - it allows Cassandra to spread replicas around your cluster to avoid\n#   correlated failures. It does this by grouping machines into\n#   \"datacenters\" and \"racks.\"  Cassandra will do its best not to have\n#   more than one replica on the same \"rack\" (which may not actually\n#   be a physical location)\n#\n# IF YOU CHANGE THE SNITCH AFTER DATA IS INSERTED INTO THE CLUSTER,\n# YOU MUST RUN A FULL REPAIR, SINCE THE SNITCH AFFECTS WHERE REPLICAS\n# ARE PLACED.\n#\n# IF THE RACK A REPLICA IS PLACED IN CHANGES AFTER THE REPLICA HAS BEEN\n# ADDED TO A RING, THE NODE MUST BE DECOMMISSIONED AND REBOOTSTRAPPED.\n#\n# Out of the box, Cassandra provides\n#  - SimpleSnitch:\n#    Treats Strategy order as proximity. This can improve cache\n#    locality when disabling read repair.  Only appropriate for\n#    single-datacenter deployments.\n#  - GossipingPropertyFileSnitch\n#    This should be your go-to snitch for production use.  The rack\n#    and datacenter for the local node are defined in\n#    cassandra-rackdc.properties and propagated to other nodes via\n#    gossip.  If cassandra-topology.properties exists, it is used as a\n#    fallback, allowing migration from the PropertyFileSnitch.\n#  - PropertyFileSnitch:\n#    Proximity is determined by rack and data center, which are\n#    explicitly configured in cassandra-topology.properties.\n#  - Ec2Snitch:\n#    Appropriate for EC2 deployments in a single Region. Loads Region\n#    and Availability Zone information from the EC2 API. The Region is\n#    treated as the datacenter, and the Availability Zone as the rack.\n#    Only private IPs are used, so this will not work across multiple\n#    Regions.\n#  - Ec2MultiRegionSnitch:\n#    Uses public IPs as broadcast_address to allow cross-region\n#    connectivity.  (Thus, you should set seed addresses to the public\n#    IP as well.) You will need to open the storage_port or\n#    ssl_storage_port on the public IP firewall.  (For intra-Region\n#    traffic, Cassandra will switch to the private IP after\n#    establishing a connection.)\n#  - RackInferringSnitch:\n#    Proximity is determined by rack and data center, which are\n#    assumed to correspond to the 3rd and 2nd octet of each node's IP\n#    address, respectively.  Unless this happens to match your\n#    deployment conventions, this is best used as an example of\n#    writing a custom Snitch class and is provided in that spirit.\n#\n# You can use a custom Snitch by setting this to the full class name\n# of the snitch, which will be assumed to be on your classpath.\nendpoint_snitch: GossipingPropertyFileSnitch\n\n# controls how often to perform the more expensive part of host score\n# calculation\ndynamic_snitch_update_interval_in_ms: 100 \n# controls how often to reset all host scores, allowing a bad host to\n# possibly recover\ndynamic_snitch_reset_interval_in_ms: 600000\n# if set greater than zero and read_repair_chance is \u003c 1.0, this will allow\n# 'pinning' of replicas to hosts in order to increase cache capacity.\n# The badness threshold will control how much worse the pinned host has to be\n# before the dynamic snitch will prefer other replicas over it.  This is\n# expressed as a double which represents a percentage.  Thus, a value of\n# 0.2 means Cassandra would continue to prefer the static snitch values\n# until the pinned host was 20% worse than the fastest.\ndynamic_snitch_badness_threshold: 0.1\n\n# request_scheduler -- Set this to a class that implements\n# RequestScheduler, which will schedule incoming client requests\n# according to the specific policy. This is useful for multi-tenancy\n# with a single Cassandra cluster.\n# NOTE: This is specifically for requests from the client and does\n# not affect inter node communication.\n# org.apache.cassandra.scheduler.NoScheduler - No scheduling takes place\n# org.apache.cassandra.scheduler.RoundRobinScheduler - Round robin of\n# client requests to a node with a separate queue for each\n# request_scheduler_id. The scheduler is further customized by\n# request_scheduler_options as described below.\nrequest_scheduler: org.apache.cassandra.scheduler.NoScheduler\n\n# Scheduler Options vary based on the type of scheduler\n# NoScheduler - Has no options\n# RoundRobin\n#  - throttle_limit -- The throttle_limit is the number of in-flight\n#                      requests per client.  Requests beyond \n#                      that limit are queued up until\n#                      running requests can complete.\n#                      The value of 80 here is twice the number of\n#                      concurrent_reads + concurrent_writes.\n#  - default_weight -- default_weight is optional and allows for\n#                      overriding the default which is 1.\n#  - weights -- Weights are optional and will default to 1 or the\n#               overridden default_weight. The weight translates into how\n#               many requests are handled during each turn of the\n#               RoundRobin, based on the scheduler id.\n#\n# request_scheduler_options:\n#    throttle_limit: 80\n#    default_weight: 5\n#    weights:\n#      Keyspace1: 1\n#      Keyspace2: 5\n\n# request_scheduler_id -- An identifier based on which to perform\n# the request scheduling. Currently the only valid option is keyspace.\n# request_scheduler_id: keyspace\n\n# Enable or disable inter-node encryption\n# JVM defaults for supported SSL socket protocols and cipher suites can\n# be replaced using custom encryption options. This is not recommended\n# unless you have policies in place that dictate certain settings, or\n# need to disable vulnerable ciphers or protocols in case the JVM cannot\n# be updated.\n# FIPS compliant settings can be configured at JVM level and should not\n# involve changing encryption settings here:\n# https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/FIPS.html\n# NOTE: No custom encryption options are enabled at the moment\n# The available internode options are : all, none, dc, rack\n#\n# If set to dc cassandra will encrypt the traffic between the DCs\n# If set to rack cassandra will encrypt the traffic between the racks\n#\n# The passwords used in these options must match the passwords used when generating\n# the keystore and truststore.  For instructions on generating these files, see:\n# http://download.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html#CreateKeystore\n#\nserver_encryption_options:\n    internode_encryption: none\n    keystore: conf/.keystore\n    keystore_password: cassandra\n    truststore: conf/.truststore\n    truststore_password: cassandra\n    # More advanced defaults below:\n    # protocol: TLS\n    # algorithm: SunX509\n    # store_type: JKS\n    # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n    # require_client_auth: false\n    # require_endpoint_verification: false\n\n# enable or disable client/server encryption.\nclient_encryption_options:\n    enabled: false\n    # If enabled and optional is set to true encrypted and unencrypted connections are handled.\n    optional: false\n    keystore: conf/.keystore\n    keystore_password: cassandra\n    # require_client_auth: false\n    # Set trustore and truststore_password if require_client_auth is true\n    # truststore: conf/.truststore\n    # truststore_password: cassandra\n    # More advanced defaults below:\n    # protocol: TLS\n    # algorithm: SunX509\n    # store_type: JKS\n    # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n\n# internode_compression controls whether traffic between nodes is\n# compressed.\n# can be:  all  - all traffic is compressed\n#          dc   - traffic between different datacenters is compressed\n#          none - nothing is compressed.\ninternode_compression: dc\n\n# Enable or disable tcp_nodelay for inter-dc communication.\n# Disabling it will result in larger (but fewer) network packets being sent,\n# reducing overhead from the TCP protocol itself, at the cost of increasing\n# latency if you block for cross-datacenter responses.\ninter_dc_tcp_nodelay: false\n\n# TTL for different trace types used during logging of the repair process.\ntracetype_query_ttl: 86400\ntracetype_repair_ttl: 604800\n\n# UDFs (user defined functions) are disabled by default.\n# As of Cassandra 3.0 there is a sandbox in place that should prevent execution of evil code.\nenable_user_defined_functions: false\n\n# Enables scripted UDFs (JavaScript UDFs).\n# Java UDFs are always enabled, if enable_user_defined_functions is true.\n# Enable this option to be able to use UDFs with \"language javascript\" or any custom JSR-223 provider.\n# This option has no effect, if enable_user_defined_functions is false.\nenable_scripted_user_defined_functions: false\n\n# The default Windows kernel timer and scheduling resolution is 15.6ms for power conservation.\n# Lowering this value on Windows can provide much tighter latency and better throughput, however\n# some virtualized environments may see a negative performance impact from changing this setting\n# below their system default. The sysinternals 'clockres' tool can confirm your system's default\n# setting.\nwindows_timer_interval: 1\n\n\n# Enables encrypting data at-rest (on disk). Different key providers can be plugged in, but the default reads from\n# a JCE-style keystore. A single keystore can hold multiple keys, but the one referenced by\n# the \"key_alias\" is the only key that will be used for encrypt opertaions; previously used keys\n# can still (and should!) be in the keystore and will be used on decrypt operations\n# (to handle the case of key rotation).\n#\n# It is strongly recommended to download and install Java Cryptography Extension (JCE)\n# Unlimited Strength Jurisdiction Policy Files for your version of the JDK.\n# (current link: http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html)\n#\n# Currently, only the following file types are supported for transparent data encryption, although\n# more are coming in future cassandra releases: commitlog, hints\ntransparent_data_encryption_options:\n    enabled: false\n    chunk_length_kb: 64\n    cipher: AES/CBC/PKCS5Padding\n    key_alias: testing:1\n    # CBC IV length for AES needs to be 16 bytes (which is also the default size)\n    # iv_length: 16\n    key_provider: \n      - class_name: org.apache.cassandra.security.JKSKeyProvider\n        parameters: \n          - keystore: conf/.keystore\n            keystore_password: cassandra\n            store_type: JCEKS\n            key_password: cassandra\n\n\n#####################\n# SAFETY THRESHOLDS #\n#####################\n\n# When executing a scan, within or across a partition, we need to keep the\n# tombstones seen in memory so we can return them to the coordinator, which\n# will use them to make sure other replicas also know about the deleted rows.\n# With workloads that generate a lot of tombstones, this can cause performance\n# problems and even exaust the server heap.\n# (http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets)\n# Adjust the thresholds here if you understand the dangers and want to\n# scan more tombstones anyway.  These thresholds may also be adjusted at runtime\n# using the StorageService mbean.\ntombstone_warn_threshold: 1000\ntombstone_failure_threshold: 100000\n\n# Log WARN on any batch size exceeding this value. 5kb per batch by default.\n# Caution should be taken on increasing the size of this threshold as it can lead to node instability.\nbatch_size_warn_threshold_in_kb: 5\n\n# Fail any batch exceeding this value. 50kb (10x warn threshold) by default.\nbatch_size_fail_threshold_in_kb: 50\n\n# Log WARN on any batches not of type LOGGED than span across more partitions than this limit\nunlogged_batch_across_partitions_warn_threshold: 10\n\n# Log a warning when compacting partitions larger than this value\ncompaction_large_partition_warning_threshold_mb: 100\n\n# GC Pauses greater than gc_warn_threshold_in_ms will be logged at WARN level\n# Adjust the threshold based on your application throughput requirement\n# By default, Cassandra logs GC Pauses greater than 200 ms at INFO level\ngc_warn_threshold_in_ms: 1000\n\nauto_bootstrap: false\n",
                            "vars.%": "3",
                            "vars.cluster_name": "opsschool",
                            "vars.seeds": "172.31.48.41,172.31.77.64,172.31.6.131",
                            "vars.server_ip": "172.31.6.131"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.template"
                },
                "null_resource.bootstrap.0": {
                    "type": "null_resource",
                    "depends_on": [
                        "aws_instance.cassandra.*",
                        "null_resource.copy_yaml"
                    ],
                    "primary": {
                        "id": "2117891712050423873",
                        "attributes": {
                            "id": "2117891712050423873"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.null"
                },
                "null_resource.bootstrap.1": {
                    "type": "null_resource",
                    "depends_on": [
                        "aws_instance.cassandra.*",
                        "null_resource.copy_yaml"
                    ],
                    "primary": {
                        "id": "7383885463573107115",
                        "attributes": {
                            "id": "7383885463573107115"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.null"
                },
                "null_resource.bootstrap.2": {
                    "type": "null_resource",
                    "depends_on": [
                        "aws_instance.cassandra.*",
                        "null_resource.copy_yaml"
                    ],
                    "primary": {
                        "id": "509544390448009514",
                        "attributes": {
                            "id": "509544390448009514"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.null"
                },
                "null_resource.copy_yaml.0": {
                    "type": "null_resource",
                    "depends_on": [
                        "aws_instance.cassandra.*",
                        "data.template_file.cassandra_yaml.*"
                    ],
                    "primary": {
                        "id": "494531876765833317",
                        "attributes": {
                            "id": "494531876765833317"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.null"
                },
                "null_resource.copy_yaml.1": {
                    "type": "null_resource",
                    "depends_on": [
                        "aws_instance.cassandra.*",
                        "data.template_file.cassandra_yaml.*"
                    ],
                    "primary": {
                        "id": "9088295822998767419",
                        "attributes": {
                            "id": "9088295822998767419"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.null"
                },
                "null_resource.copy_yaml.2": {
                    "type": "null_resource",
                    "depends_on": [
                        "aws_instance.cassandra.*",
                        "data.template_file.cassandra_yaml.*"
                    ],
                    "primary": {
                        "id": "7017589205003350355",
                        "attributes": {
                            "id": "7017589205003350355"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": "provider.null"
                }
            },
            "depends_on": []
        }
    ]
}
